[[["f6163dfe-27fc-4de4-9423-e1e1f1aef6ef",{"pageContent":"{/* Resposta */}\n      {response && (\n        <Card>\n          <CardBody>\n            <div className=\"flex justify-between items-start mb-4\">\n              <Typography variant=\"h5\">Resposta</Typography>\n              <div className=\"flex items-center gap-2\">\n                {response.cached && (\n                  <Chip\n                    value=\"Cache\"\n                    color=\"green\"\n                    size=\"sm\"\n                    icon={<CheckCircleIcon className=\"h-4 w-4\" />}\n                  />\n                )}\n                {response.responseTime && (\n                  <Chip\n                    value={`${response.responseTime.toFixed(1)}s`}\n                    color=\"blue\"\n                    size=\"sm\"\n                    icon={<ClockIcon className=\"h-4 w-4\" />}\n                  />\n                )}\n                {response.isStreaming && (\n                  <div className=\"flex items-center gap-2\">\n                    <div className=\"h-2 w-2 bg-blue-500 rounded-full animate-pulse\" />\n                    <Typography variant=\"small\" color=\"blue\">\n                      Processando...\n                    </Typography>\n                  </div>\n                )}\n              </div>\n            </div>\n            \n            <div className=\"prose prose-lg max-w-none\">\n              <Typography className=\"whitespace-pre-wrap leading-relaxed\">\n                {response.answer}\n              </Typography>\n            </div>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":39,"totalChunks":43,"timestamp":1749751085616}}],["2f8be819-32d0-449d-a6fc-f3e7bb6b6277",{"pageContent":"export const useOpenAI = (apiKey) => {\n  const [isLoading, setIsLoading] = useState(false);\n  const [isTranscribing, setIsTranscribing] = useState(false);\n  const [error, setError] = useState(null);\n\n  // Memoize service instance\n  const openAIService = useMemo(() => {\n    return apiKey ? createOpenAIService(apiKey) : null;\n  }, [apiKey]);\n\n  /**\n   * Send chat message with streaming\n   * @param {string} prompt - User prompt\n   * @param {string} context - PDF context\n   * @param {function} onChunk - Callback for response chunks\n   * @returns {Promise<string>} Complete response\n   */\n  const sendMessage = useCallback(async (prompt, context = '', onChunk) => {\n    if (!openAIService) {\n      throw new Error('OpenAI service not initialized');\n    }\n\n    setIsLoading(true);\n    setError(null);\n\n    try {\n      const response = await openAIService.sendChatMessage(prompt, context, onChunk);\n      return response;\n    } catch (err) {\n      setError(err.message);\n      throw err;\n    } finally {\n      setIsLoading(false);\n    }\n  }, [openAIService]);\n\n  /**\n   * Transcribe audio to text\n   * @param {Blob} audioBlob - Audio data\n   * @param {string} language - Language code\n   * @returns {Promise<string>} Transcribed text\n   */\n  const transcribeAudio = useCallback(async (audioBlob, language = 'pt') => {\n    if (!openAIService) {\n      throw new Error('OpenAI service not initialized');\n    }\n\n    setIsTranscribing(true);\n    setError(null);\n\n    try {\n      const transcription = await openAIService.transcribeAudio(audioBlob, language);\n      return transcription;\n    } catch (err) {\n      setError(err.message);\n      throw err;\n    } finally {\n      setIsTranscribing(false);\n    }\n  }, [openAIService]);\n\n  /**\n   * Clear error state\n   */\n  const clearError = useCallback(() => {\n    setError(null);\n  }, []);\n\n  return {\n    sendMessage,\n    transcribeAudio,\n    clearError,\n    isLoading,\n    isTranscribing,\n    error,\n    isReady: !!openAIService,\n  };\n};","metadata":{"source":"src/hooks/useOpenAI.js","section":"","chunkIndex":1,"totalChunks":2,"timestamp":1749751085616}}],["e7eec84b-b180-4984-9e13-07270375d616",{"pageContent":"/**\n * @fileoverview Application constants\n */\n\n/**\n * OpenAI API configuration\n */\nexport const OPENAI_CONFIG = {\n  CHAT_MODEL: 'gpt-4-turbo-preview',\n  WHISPER_MODEL: 'whisper-1',\n  TEMPERATURE: 0,\n  MAX_TOKENS: 4000,\n};\n\n/**\n * API endpoints\n */\nexport const API_ENDPOINTS = {\n  OPENAI_CHAT: 'https://api.openai.com/v1/chat/completions',\n  OPENAI_TRANSCRIPTIONS: 'https://api.openai.com/v1/audio/transcriptions',\n};\n\n/**\n * PDF processing configuration\n */\nexport const PDF_CONFIG = {\n  MAX_FILE_SIZE: 10 * 1024 * 1024, // 10MB\n  ACCEPTED_TYPES: ['application/pdf'],\n  MAX_TEXT_PREVIEW: 1000,\n  MAX_CONTEXT_LENGTH: 3000,\n};\n\n/**\n * Audio recording configuration\n */\nexport const AUDIO_CONFIG = {\n  AUDIO_TYPE: 'audio/webm',\n  CONSTRAINTS: {\n    audio: {\n      channelCount: 1,\n      sampleRate: 16000,\n    },\n  },\n  MAX_RECORDING_TIME: 5 * 60 * 1000, // 5 minutes\n};\n\n/**\n * UI constants\n */\nexport const UI_CONFIG = {\n  MAX_RESPONSE_HEIGHT: 'max-h-80',\n  SPINNER_SIZE: 'h-8 w-8',\n  ANIMATION_DURATION: 300,\n};\n\n/**\n * Error messages\n */\nexport const ERROR_MESSAGES = {\n  PDF_PROCESSING: 'Erro ao processar PDF. Verifique se o arquivo é válido.',\n  MICROPHONE_ACCESS: 'Erro ao acessar o microfone. Verifique as permissões.',\n  TRANSCRIPTION_FAILED: 'Erro ao transcrever o áudio. Tente novamente.',\n  CHAT_REQUEST_FAILED: 'Erro ao processar sua solicitação. Tente novamente.',\n  FILE_TOO_LARGE: 'Arquivo muito grande. Tamanho máximo permitido: 10MB.',\n  INVALID_FILE_TYPE: 'Tipo de arquivo não suportado. Apenas PDFs são aceitos.',\n};\n\n/**\n * Success messages\n */\nexport const SUCCESS_MESSAGES = {\n  PDF_UPLOADED: 'PDF carregado com sucesso!',\n  TRANSCRIPTION_COMPLETED: 'Transcrição concluída com sucesso!',\n};","metadata":{"source":"src/constants/index.js","section":"","chunkIndex":0,"totalChunks":1,"timestamp":1749751085616}}],["99f49edc-dfaa-42c2-9e0e-1c11bcb8b4b5",{"pageContent":"### Abordagem mobile-first\n```javascript\nimport { Typography, Card } from \"@material-tailwind/react\";\n\nexport default function ResponsiveCard() {\n  return (\n    <Card className=\"w-full max-w-sm mx-auto sm:max-w-md md:max-w-lg lg:max-w-xl\">\n      <Typography \n        variant=\"h4\" \n        className=\"text-center text-base sm:text-lg md:text-xl lg:text-2xl\"\n      >\n        Responsive Typography\n      </Typography>\n    </Card>\n  );\n}\n```\n\n## 7. Diferenças entre versões gratuitas e Pro\n\n| Recurso | Versão Gratuita | Versão Pro |\n|---------|-----------------|------------|\n| **Componentes** | 40+ componentes básicos | 300+ componentes premium |\n| **Blocks e Seções** | Seções pré-construídas limitadas | 100+ blocks prontos para uso |\n| **Templates** | Sem templates completos | 20+ templates de websites completos |\n| **Arquivos Figma** | Não incluídos | Sistema de design Figma completo |\n| **Componentes Avançados** | Exibição básica de dados | Tabelas avançadas, gráficos, calendários |\n| **Elementos Premium** | Apenas componentes padrão | Formulários complexos, dashboards, e-commerce |\n| **Suporte** | Apenas suporte da comunidade | Suporte prioritário por email |\n| **Atualizações** | Ciclo de lançamento padrão | Acesso antecipado a novos recursos |\n| **Licença Comercial** | Licença MIT (uso comercial gratuito) | Licença comercial estendida |\n| **Preço** | Gratuito (Licença MIT) | Pagamento único: $299 individual, $799 equipe |\n| **Ferramentas IA** | Não disponível | Geração de componentes com IA (V3 Pro) |\n| **Exportação de Código** | Copiar e colar manual | Ferramentas de exportação direta de código |","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":8,"totalChunks":15,"timestamp":1749751085616}}],["7cea2645-9f4d-4a1e-a0ab-3c9f5b170996",{"pageContent":"{/* Progresso */}\n      {progress && (\n        <Card>\n          <CardBody>\n            <div className=\"space-y-3\">\n              <div className=\"flex justify-between items-center\">\n                <Typography className=\"font-medium\">\n                  {progress.message}\n                </Typography>\n                <Typography className=\"font-bold text-blue-600\">\n                  {Math.round(progress.percentage)}%\n                </Typography>\n              </div>\n              <Progress \n                value={progress.percentage} \n                color=\"blue\" \n                className=\"h-3\"\n              />\n              {progress.current && (\n                <Typography variant=\"small\" color=\"gray\" className=\"text-center\">\n                  {progress.current} de {progress.total} processados\n                </Typography>\n              )}\n            </div>\n          </CardBody>\n        </Card>\n      )}","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":37,"totalChunks":43,"timestamp":1749751085616}}],["ffe097f5-8d92-475d-9a10-89c22c7a80d4",{"pageContent":"{/* Dialogs */}\n      <StatsDialog />\n      <HistoryDialog />\n    </div>\n  );\n}\n```\n\n## 6. App Principal com Roteamento\n\n```javascript\n// src/App.jsx\nimport React, { useState } from 'react';\nimport { ThemeProvider } from '@material-tailwind/react';\nimport { ApiKeySetup } from './components/ApiKeySetup';\nimport { HighQualityRAG } from './components/HighQualityRAG';\nimport { ConfigService } from './services/config.service';\n\nfunction App() {\n  const [hasApiKey, setHasApiKey] = useState(ConfigService.hasApiKey());\n\n  const handleApiKeySetup = () => {\n    setHasApiKey(true);\n  };\n\n  return (\n    <ThemeProvider>\n      <div className=\"min-h-screen bg-gray-50\">\n        {!hasApiKey ? (\n          <ApiKeySetup onComplete={handleApiKeySetup} />\n        ) : (\n          <HighQualityRAG />\n        )}\n      </div>\n    </ThemeProvider>\n  );\n}\n\nexport default App;\n```\n\n## 7. Configuração Vite Otimizada\n\n```javascript\n// vite.config.js\nimport { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\n\nexport default defineConfig({\n  plugins: [react()],\n  optimizeDeps: {\n    include: ['openai', '@orama/orama', 'pdfjs-dist', 'langchain']\n  },\n  build: {\n    rollupOptions: {\n      output: {\n        manualChunks: {\n          'openai': ['openai'],\n          'pdf': ['pdfjs-dist'],\n          'search': ['@orama/orama'],\n          'ui': ['@material-tailwind/react'],\n          'langchain': ['langchain']\n        }\n      }\n    },\n    chunkSizeWarningLimit: 1000\n  },\n  worker: {\n    format: 'es'\n  }\n})\n```\n\n## 8. Package.json Atualizado","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":41,"totalChunks":43,"timestamp":1749751085616}}],["e01f80b3-6ea4-4553-9ac5-e644d0dfe86a",{"pageContent":"{/* Exportar */}\n            {uploadedFile && (\n              <Button\n                onClick={handleExport}\n                variant=\"outlined\"\n                disabled={isProcessing}\n                className=\"flex items-center justify-center gap-2\"\n              >\n                <ArrowDownTrayIcon className=\"h-5 w-5\" />\n                Exportar\n              </Button>\n            )}\n\n            {/* Estatísticas */}\n            {stats && (\n              <Button\n                onClick={() => setShowStats(true)}\n                variant=\"outlined\"\n                color=\"blue-gray\"\n                className=\"flex items-center justify-center gap-2\"\n              >\n                <ChartBarIcon className=\"h-5 w-5\" />\n                Estatísticas\n              </Button>\n            )}\n          </div>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":35,"totalChunks":43,"timestamp":1749751085616}}],["d46a1cbe-e87b-47e3-9437-2bc5e53b1f87",{"pageContent":"/**\n * @fileoverview Audio recording service\n */\n\nimport { AUDIO_CONFIG, ERROR_MESSAGES } from '../constants';\n\n/**\n * Audio recording service class\n */","metadata":{"source":"src/services/audio.service.js","section":"","chunkIndex":0,"totalChunks":5,"timestamp":1749751085616}}],["db263f02-57b7-4be4-8daa-6c20bbe6a13a",{"pageContent":"const endTime = Date.now();\n      const responseTime = (endTime - startTime) / 1000;\n\n      setResponse(prev => ({\n        ...prev,\n        isStreaming: false,\n        responseTime: responseTime,\n        cached: result.cached || false\n      }));\n\n      // Adicionar ao histórico\n      const historyItem = {\n        query: query,\n        answer: fullResponse.substring(0, 100) + '...',\n        timestamp: new Date().toISOString(),\n        sources: result.sources.length\n      };\n      \n      const newHistory = [historyItem, ...queryHistory].slice(0, 20);\n      setQueryHistory(newHistory);\n      localStorage.setItem('query_history', JSON.stringify(newHistory));\n\n    } catch (err) {\n      setError(err.message);\n    } finally {\n      setIsProcessing(false);\n      setIsStreaming(false);\n    }\n  }, [query, ragService, queryHistory]);\n\n  // Exportar índice\n  const handleExport = useCallback(async () => {\n    try {\n      const blob = await ragService.exportIndex();\n      const url = URL.createObjectURL(blob);\n      const a = document.createElement('a');\n      a.href = url;\n      a.download = `rag-index-${uploadedFile?.name || 'export'}-${new Date().toISOString().split('T')[0]}.json`;\n      a.click();\n      URL.revokeObjectURL(url);\n      \n      // Notificar sucesso\n      alert('✅ Índice exportado com sucesso!');\n    } catch (err) {\n      setError(err.message);\n    }\n  }, [ragService, uploadedFile]);\n\n  // Importar índice\n  const handleImport = useCallback(async (event) => {\n    const file = event.target.files[0];\n    if (!file) return;\n\n    setIsProcessing(true);\n    setError(null);","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":23,"totalChunks":43,"timestamp":1749751085616}}],["b1272569-bfb1-4e6d-bc90-92fb2c4bcbe8",{"pageContent":"## 6. Melhores práticas de uso\n\n### Otimização de performance\n\n**Tree Shaking e Purging**\n```javascript\nmodule.exports = withMT({\n  content: [\n    \"./src/**/*.{js,jsx,ts,tsx}\",\n    \"./node_modules/@material-tailwind/react/components/**/*.{js,ts,jsx,tsx}\",\n    \"./node_modules/@material-tailwind/react/theme/components/**/*.{js,ts,jsx,tsx}\",\n  ],\n});\n```\n\n**Importação otimizada de componentes**\n```javascript\n// ✅ Bom - Importações individuais\nimport { Button, Card, Typography } from \"@material-tailwind/react\";\n\n// ❌ Evitar - Importar toda a biblioteca\nimport * as MaterialTailwind from \"@material-tailwind/react\";\n```\n\n### Padrões de composição de componentes\n```javascript\nimport { Card, CardHeader, CardBody, CardFooter, Typography, Button } from \"@material-tailwind/react\";\n\nexport default function ProductCard({ product }) {\n  return (\n    <Card className=\"mt-6 w-96\">\n      <CardHeader color=\"blue-gray\" className=\"relative h-56\">\n        <img src={product.image} alt={product.name} />\n      </CardHeader>\n      <CardBody>\n        <Typography variant=\"h5\" color=\"blue-gray\" className=\"mb-2\">\n          {product.name}\n        </Typography>\n        <Typography>\n          {product.description}\n        </Typography>\n      </CardBody>\n      <CardFooter className=\"pt-0\">\n        <Button>Add to Cart</Button>\n      </CardFooter>\n    </Card>\n  );\n}\n```\n\n### Considerações de acessibilidade\n```javascript\nimport { Button, IconButton } from \"@material-tailwind/react\";\n\nexport default function AccessibleButton() {\n  return (\n    <>\n      <Button \n        aria-label=\"Submit form\"\n        className=\"w-full\"\n      >\n        Submit\n      </Button>\n      \n      <IconButton \n        aria-label=\"Close dialog\"\n        variant=\"text\"\n      >\n        <XMarkIcon className=\"h-5 w-5\" />\n      </IconButton>\n    </>\n  );\n}\n```\n\n### Abordagem mobile-first\n```javascript\nimport { Typography, Card } from \"@material-tailwind/react\";","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":7,"totalChunks":15,"timestamp":1749751085616}}],["bca7231e-5af3-4baf-bf51-5de083f6edf0",{"pageContent":"// Verificar cache de respostas\n    const cacheKey = `${query}_${relevantDocs.map(d => d.metadata.hash).join('_')}`;\n    const cachedResponse = this.getResponseCache(cacheKey);\n    if (cachedResponse) {\n      return { ...cachedResponse, cached: true };\n    }\n\n    // Construir contexto otimizado\n    const context = this.buildOptimizedContext(relevantDocs, maxContextTokens);\n\n    // Preparar mensagens\n    const messages = [\n      {\n        role: 'system',\n        content: systemPrompt\n      },\n      {\n        role: 'user',\n        content: `Contexto do documento:\\n\\n${context}\\n\\nPergunta: ${query}\\n\\nPor favor, forneça uma resposta detalhada e precisa baseada no contexto acima.`\n      }\n    ];\n\n    try {\n      if (streamResponse) {\n        const stream = await this.openai.chat.completions.create({\n          model: this.config.chatModel,\n          messages: messages,\n          temperature: this.config.temperature,\n          max_tokens: this.config.maxTokens,\n          stream: true\n        });\n\n        return {\n          stream: stream,\n          sources: relevantDocs.map(doc => doc.metadata),\n          cached: false\n        };\n      } else {\n        const response = await this.openai.chat.completions.create({\n          model: this.config.chatModel,\n          messages: messages,\n          temperature: this.config.temperature,\n          max_tokens: this.config.maxTokens\n        });\n\n        const result = {\n          answer: response.choices[0].message.content,\n          sources: relevantDocs.map(doc => doc.metadata),\n          usage: response.usage,\n          cached: false\n        };\n\n        // Cachear resposta\n        this.setResponseCache(cacheKey, result);\n\n        return result;\n      }\n    } catch (error) {\n      console.error('Erro ao gerar resposta:', error);\n      throw new Error('Falha ao gerar resposta: ' + error.message);\n    }\n  }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":11,"totalChunks":43,"timestamp":1749751085616}}],["2055625f-1256-4efd-bcca-02755156e83e",{"pageContent":"{/* Interface de Busca */}\n      {uploadedFile && !progress && (\n        <Card>\n          <CardBody>\n            <Typography variant=\"h5\" className=\"mb-4\">\n              Fazer Pergunta\n            </Typography>\n            \n            <div className=\"space-y-4\">\n              <div className=\"relative\">\n                <Textarea\n                  value={query}\n                  onChange={(e) => setQuery(e.target.value)}\n                  placeholder=\"Digite sua pergunta sobre o documento...\"\n                  rows={3}\n                  disabled={isProcessing}\n                  onKeyDown={(e) => {\n                    if (e.key === 'Enter' && e.ctrlKey) {\n                      handleQuery();\n                    }\n                  }}\n                />\n                <Typography variant=\"small\" color=\"gray\" className=\"absolute bottom-2 right-2\">\n                  Ctrl+Enter para enviar\n                </Typography>\n              </div>\n              \n              <Button\n                onClick={handleQuery}\n                disabled={isProcessing || !query.trim() || isStreaming}\n                color=\"green\"\n                size=\"lg\"\n                className=\"w-full flex items-center justify-center gap-2\"\n              >\n                <MagnifyingGlassIcon className=\"h-5 w-5\" />\n                {isStreaming ? 'Gerando resposta...' : 'Buscar e Responder'}\n              </Button>\n            </div>\n          </CardBody>\n        </Card>\n      )}","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":38,"totalChunks":43,"timestamp":1749751085616}}],["e70b1ebf-a796-4a28-a92e-3beff6574c1e",{"pageContent":"<Button\n          variant=\"text\"\n          color=\"red\"\n          onClick={() => {\n            if (confirm('Deseja limpar todo o histórico?')) {\n              setQueryHistory([]);\n              localStorage.removeItem('query_history');\n            }\n          }}\n          className=\"mr-auto\"\n        >\n          Limpar Histórico\n        </Button>\n        <Button variant=\"text\" onClick={() => setShowSavedIndexes(false)}>\n          Fechar\n        </Button>\n      </DialogFooter>\n    </Dialog>\n  );","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":32,"totalChunks":43,"timestamp":1749751085616}}],["62907203-2a8c-4aef-b2de-6c47816173cd",{"pageContent":"// Reranking com GPT\n    if (useReranking && relevantDocs.length > 0) {\n      relevantDocs = await this.rerankDocuments(query, relevantDocs, limit);\n    }\n\n    // Incluir contexto adjacente se solicitado\n    if (includeContext) {\n      relevantDocs = await this.expandWithContext(relevantDocs);\n    }\n\n    return relevantDocs.slice(0, limit);\n  }\n\n  // Expandir com chunks adjacentes para mais contexto\n  async expandWithContext(documents) {\n    const expanded = [];\n    \n    for (const doc of documents) {\n      expanded.push(doc);\n      \n      // Buscar chunks adjacentes\n      const prevChunkId = `${doc.metadata.source}_p${doc.metadata.pageNumber}_c${doc.metadata.chunkIndex - 1}`;\n      const nextChunkId = `${doc.metadata.source}_p${doc.metadata.pageNumber}_c${doc.metadata.chunkIndex + 1}`;\n      \n      // Adicionar contexto anterior/posterior se existir\n      // (implementação simplificada - em produção, fazer query no DB)\n    }\n    \n    return expanded;\n  }\n\n  // Gerar resposta com streaming\n  async generateResponse(query, options = {}) {\n    const {\n      systemPrompt = `Você é um assistente especializado que fornece respostas precisas e detalhadas baseadas no contexto fornecido. \n      Sempre cite as páginas relevantes quando possível e seja específico nas suas respostas.\n      Se não encontrar informação suficiente no contexto, diga claramente.`,\n      includePageNumbers = true,\n      streamResponse = true,\n      maxContextTokens = 12000\n    } = options;\n\n    // Buscar documentos relevantes\n    const relevantDocs = await this.searchSemantic(query, {\n      useReranking: true,\n      includeContext: true\n    });\n    \n    if (relevantDocs.length === 0) {\n      return {\n        answer: 'Desculpe, não encontrei informações relevantes no documento para responder sua pergunta.',\n        sources: [],\n        cached: false\n      };\n    }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":10,"totalChunks":43,"timestamp":1749751085616}}],["6d6ce899-daa0-4d1a-b038-4acfcabd79b5",{"pageContent":"return relevantDocs.slice(0, limit);\n    } catch (error) {\n      console.error('Search failed:', error);\n      throw new Error(`Erro na busca: ${error.message}`);\n    }\n  }\n\n  // Expandir com chunks adjacentes para mais contexto\n  async expandWithContext(documents) {\n    const expanded = [];\n    \n    for (const doc of documents) {\n      expanded.push(doc);\n      \n      // Buscar chunks adjacentes\n      const prevChunkId = `${doc.metadata.source}_p${doc.metadata.pageNumber}_c${doc.metadata.chunkIndex - 1}`;\n      const nextChunkId = `${doc.metadata.source}_p${doc.metadata.pageNumber}_c${doc.metadata.chunkIndex + 1}`;\n      \n      // Adicionar contexto anterior/posterior se existir\n      // (implementação simplificada - em produção, fazer query no DB)\n    }\n    \n    return expanded;\n  }\n\n  // Gerar resposta com streaming\n  async generateResponse(query, options = {}) {\n    const {\n      systemPrompt = `Você é um assistente especializado que fornece respostas precisas e detalhadas baseadas no contexto fornecido. \n      Sempre cite as páginas relevantes quando possível e seja específico nas suas respostas.\n      Se não encontrar informação suficiente no contexto, diga claramente.`,\n      includePageNumbers = true,\n      streamResponse = true,\n      maxContextTokens = 12000\n    } = options;\n\n    // Buscar documentos relevantes\n    const relevantDocs = await this.searchSemantic(query, {\n      useReranking: true,\n      includeContext: true\n    });\n    \n    if (relevantDocs.length === 0) {\n      return {\n        answer: 'Desculpe, não encontrei informações relevantes no documento para responder sua pergunta.',\n        sources: [],\n        cached: false\n      };\n    }\n\n    // Verificar cache de respostas\n    const cacheKey = `${query}_${relevantDocs.map(d => d.metadata.hash).join('_')}`;\n    const cachedResponse = this.getResponseCache(cacheKey);\n    if (cachedResponse) {\n      return { ...cachedResponse, cached: true };\n    }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":8,"totalChunks":18,"timestamp":1749751085616}}],["9441a1ef-6651-4c29-acd2-fa538753c5f6",{"pageContent":"{response.sources && response.sources.length > 0 && (\n              <div className=\"mt-6 pt-4 border-t\">\n                <Typography variant=\"h6\" className=\"mb-3\">\n                  Fontes Utilizadas ({response.sources.length})\n                </Typography>\n                <div className=\"grid grid-cols-1 md:grid-cols-2 gap-3\">\n                  {response.sources.map((source, idx) => (\n                    <Card key={idx} className=\"bg-gray-50\">\n                      <CardBody className=\"p-3\">\n                        <div className=\"flex items-center justify-between mb-1\">\n                          <Chip\n                            value={`Página ${source.pageNumber}`}\n                            size=\"sm\"\n                            color=\"blue\"\n                          />\n                          <Typography variant=\"small\" color=\"gray\">\n                            Relevância: {((source.importance || 1) * 100).toFixed(0)}%\n                          </Typography>\n                        </div>\n                        <Typography variant=\"small\" color=\"gray\">\n                          Chunk #{source.chunkIndex + 1} • {source.totalTokens} tokens\n                        </Typography>\n                      </CardBody>\n                    </Card>\n                  ))}\n                </div>\n              </div>\n            )}\n          </CardBody>\n        </Card>\n      )}\n\n      {/* Erros */}\n      {error && (\n        <Alert\n          color=\"red\"\n          onClose={() => setError(null)}\n          className=\"flex items-center\"\n        >\n          <ExclamationCircleIcon className=\"h-5 w-5 mr-2\" />\n          <div className=\"flex-1\">\n            <Typography className=\"font-medium\">Erro</Typography>\n            <Typography variant=\"small\">{error}</Typography>\n          </div>\n        </Alert>\n      )}\n\n      {/* Dialogs */}\n      <StatsDialog />\n      <HistoryDialog />\n    </div>\n  );\n}\n```\n\n## 6. App Principal com Roteamento","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":40,"totalChunks":43,"timestamp":1749751085616}}],["e5d30ecc-601d-4b93-9809-c30bd88522a2",{"pageContent":"{/* Estimativa de custos detalhada */}\n            <Card className=\"bg-blue-gray-50\">\n              <CardBody>\n                <Typography variant=\"h6\" className=\"mb-3 flex items-center gap-2\">\n                  <CurrencyDollarIcon className=\"h-5 w-5\" />\n                  Estimativa de Custos\n                </Typography>\n                {(() => {\n                  const costs = ragService.estimateCost(stats.totalChunks);\n                  return (\n                    <div className=\"space-y-3\">\n                      <div className=\"p-3 bg-white rounded\">\n                        <Typography variant=\"small\" className=\"font-medium\">\n                          Embeddings ({ragService.config.embeddingModel})\n                        </Typography>\n                        <div className=\"flex justify-between mt-1\">\n                          <Typography variant=\"small\" color=\"gray\">\n                            {costs.embedding.tokens.toLocaleString()} tokens\n                          </Typography>\n                          <Typography variant=\"small\" className=\"font-medium text-green-600\">\n                            ${costs.embedding.cost.toFixed(4)}\n                          </Typography>\n                        </div>\n                      </div>\n                      \n                      <div className=\"p-3 bg-white rounded\">\n                        <Typography variant=\"small\" className=\"font-medium\">\n                          Uso Estimado de Chat (10 queries)\n                        </Typography>\n                        <div className=\"flex justify-between mt-1\">\n                          <Typography variant=\"small\" color=\"gray\">\n                            ~20,000 tokens\n                          </Typography>\n                          <Typography variant=\"small\" className=\"font-medium text-green-600\">\n                            $0.60\n                          </Typography>\n                        </div>\n                      </div>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":28,"totalChunks":43,"timestamp":1749751085616}}],["fc944953-5f6e-4cc4-bea9-9f679e5a3e04",{"pageContent":"return result;\n      }\n    } catch (error) {\n      console.error('Erro ao gerar resposta:', error);\n      throw new Error('Falha ao gerar resposta: ' + error.message);\n    }\n  }\n\n  // Sistema de cache de respostas\n  responseCache = new Map();\n  \n  getResponseCache(key) {\n    const cached = this.responseCache.get(key);\n    if (cached && Date.now() - cached.timestamp < 3600000) { // 1 hora\n      return cached.data;\n    }\n    return null;\n  }\n  \n  setResponseCache(key, data) {\n    this.responseCache.set(key, {\n      data: data,\n      timestamp: Date.now()\n    });\n    \n    // Limpar cache antigo\n    if (this.responseCache.size > 100) {\n      const oldestKey = this.responseCache.keys().next().value;\n      this.responseCache.delete(oldestKey);\n    }\n  }\n\n  // Reranking melhorado\n  async rerankDocuments(query, documents, topK) {\n    const prompt = `Task: Rank these text passages by relevance to the query.\n    \nQuery: \"${query}\"\n    \nPassages:\n${documents.map((doc, i) => `[${i + 1}] ${doc.text.substring(0, 300)}...`).join('\\n\\n')}\n    \nReturn ONLY the passage numbers in order of relevance (most to least relevant), separated by commas.\nExample: 3,1,5,2,4`;\n\n    try {\n      const response = await this.openai.chat.completions.create({\n        model: 'gpt-3.5-turbo',\n        messages: [{ role: 'user', content: prompt }],\n        temperature: 0,\n        max_tokens: 50\n      });\n\n      const content = response.choices[0].message.content.trim();\n      const ranking = content.split(',')\n        .map(n => parseInt(n.trim()) - 1)\n        .filter(n => !isNaN(n) && n >= 0 && n < documents.length);\n\n      if (ranking.length === 0) {\n        return documents;\n      }\n\n      return ranking.map(index => documents[index]);\n    } catch (error) {\n      console.warn('Reranking falhou:', error);\n      return documents;\n    }\n  }\n\n  // Construir contexto otimizado com metadados\n  buildOptimizedContext(documents, maxTokens) {\n    let context = '';\n    let currentTokens = 0;","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":12,"totalChunks":43,"timestamp":1749751085616}}],["6e0db5f7-d8d8-49f5-9ac5-12d10f89ed7d",{"pageContent":"return result;\n      }\n    } catch (error) {\n      console.error('Erro ao gerar resposta:', error);\n      throw new Error('Falha ao gerar resposta: ' + error.message);\n    }\n  }\n\n  // Sistema de cache de respostas\n  responseCache = new Map();\n  \n  getResponseCache(key) {\n    const cached = this.responseCache.get(key);\n    if (cached && Date.now() - cached.timestamp < 3600000) { // 1 hora\n      return cached.data;\n    }\n    return null;\n  }\n  \n  setResponseCache(key, data) {\n    this.responseCache.set(key, {\n      data: data,\n      timestamp: Date.now()\n    });\n    \n    // Limpar cache antigo\n    if (this.responseCache.size > 100) {\n      const oldestKey = this.responseCache.keys().next().value;\n      this.responseCache.delete(oldestKey);\n    }\n  }\n\n  // Reranking melhorado\n  async rerankDocuments(query, documents, topK) {\n    const prompt = `Task: Rank these text passages by relevance to the query.\n    \nQuery: \"${query}\"\n    \nPassages:\n${documents.map((doc, i) => `[${i + 1}] ${doc.text.substring(0, 300)}...`).join('\\n\\n')}\n    \nReturn ONLY the passage numbers in order of relevance (most to least relevant), separated by commas.\nExample: 3,1,5,2,4`;\n\n    try {\n      const response = await this.openai.chat.completions.create({\n        model: 'gpt-3.5-turbo',\n        messages: [{ role: 'user', content: prompt }],\n        temperature: 0,\n        max_tokens: 50\n      });\n\n      const content = response.choices[0].message.content.trim();\n      const ranking = content.split(',')\n        .map(n => parseInt(n.trim()) - 1)\n        .filter(n => !isNaN(n) && n >= 0 && n < documents.length);\n\n      if (ranking.length === 0) {\n        return documents;\n      }\n\n      return ranking.map(index => documents[index]);\n    } catch (error) {\n      console.warn('Reranking falhou:', error);\n      return documents;\n    }\n  }\n\n  // Construir contexto otimizado com metadados\n  buildOptimizedContext(documents, maxTokens) {\n    let context = '';\n    let currentTokens = 0;","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":10,"totalChunks":18,"timestamp":1749751085616}}],["fdea8ca4-2e30-4465-b669-6827e84ef8d2",{"pageContent":"/**\n   * Clear error state\n   */\n  const clearError = useCallback(() => {\n    setError(null);\n  }, []);\n\n  return {\n    extractText,\n    getMetadata,\n    validateFile,\n    clearText,\n    clearError,\n    extractedText,\n    isProcessing,\n    progress,\n    error,\n  };\n};","metadata":{"source":"src/hooks/usePDF.js","section":"","chunkIndex":2,"totalChunks":3,"timestamp":1749751085616}}],["6648d688-bc9d-4614-9417-897fc9f2c58e",{"pageContent":"if (!ConfigService.hasApiKey()) {\n    return null;\n  }\n\n  return (\n    <div className=\"max-w-7xl mx-auto p-4 space-y-4\">\n      {/* Header */}\n      <Card>\n        <CardBody className=\"flex justify-between items-center\">\n          <div>\n            <Typography variant=\"h3\" color=\"blue-gray\" className=\"flex items-center gap-2\">\n              <SparklesIcon className=\"h-8 w-8 text-blue-500\" />\n              Sistema RAG de Alta Qualidade\n            </Typography>\n            <Typography color=\"gray\" className=\"mt-1\">\n              {ragService.config.embeddingModel} • {ragService.config.chatModel}\n            </Typography>\n          </div>\n          <div className=\"flex gap-2\">\n            <Tooltip content=\"Histórico\">\n              <IconButton\n                variant=\"text\"\n                onClick={() => setShowSavedIndexes(true)}\n              >\n                <ClockIcon className=\"h-5 w-5\" />\n              </IconButton>\n            </Tooltip>\n            <Tooltip content=\"Configurações\">\n              <IconButton\n                variant=\"text\"\n                onClick={handleReset}\n              >\n                <Cog6ToothIcon className=\"h-5 w-5\" />\n              </IconButton>\n            </Tooltip>\n          </div>\n        </CardBody>\n      </Card>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":33,"totalChunks":43,"timestamp":1749751085616}}],["d8fac1e3-7212-4f86-afab-db01f1d5e4f5",{"pageContent":"/**\n * @fileoverview Enhanced text splitter with intelligent chunking\n */","metadata":{"source":"src/utils/textSplitter.js","section":"","chunkIndex":0,"totalChunks":4,"timestamp":1749751085616}}],["6299ba7e-b4e8-45f7-86a7-e5e57baac349",{"pageContent":"// Construir contexto otimizado\n    const context = this.buildOptimizedContext(relevantDocs, maxContextTokens);\n\n    // Preparar mensagens\n    const messages = [\n      {\n        role: 'system',\n        content: systemPrompt\n      },\n      {\n        role: 'user',\n        content: `Contexto do documento:\\n\\n${context}\\n\\nPergunta: ${query}\\n\\nPor favor, forneça uma resposta detalhada e precisa baseada no contexto acima.`\n      }\n    ];\n\n    try {\n      if (streamResponse) {\n        const stream = await this.openai.chat.completions.create({\n          model: this.config.chatModel,\n          messages: messages,\n          temperature: this.config.temperature,\n          max_tokens: this.config.maxTokens,\n          stream: true\n        });\n\n        return {\n          stream: stream,\n          sources: relevantDocs.map(doc => doc.metadata),\n          cached: false\n        };\n      } else {\n        const response = await this.openai.chat.completions.create({\n          model: this.config.chatModel,\n          messages: messages,\n          temperature: this.config.temperature,\n          max_tokens: this.config.maxTokens\n        });\n\n        const result = {\n          answer: response.choices[0].message.content,\n          sources: relevantDocs.map(doc => doc.metadata),\n          usage: response.usage,\n          cached: false\n        };\n\n        // Cachear resposta\n        this.setResponseCache(cacheKey, result);\n\n        return result;\n      }\n    } catch (error) {\n      console.error('Erro ao gerar resposta:', error);\n      throw new Error('Falha ao gerar resposta: ' + error.message);\n    }\n  }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":9,"totalChunks":18,"timestamp":1749751085616}}],["172f6ed5-e45a-4ffe-b93d-499e43ea71b2",{"pageContent":"calculateImportance(text, pageNum, totalPages) {\n    let score = 1.0;\n    \n    // Primeiras páginas (resumo, introdução)\n    if (pageNum <= 3) score *= 1.3;\n    \n    // Últimas páginas (conclusão)\n    if (pageNum >= totalPages - 2) score *= 1.2;\n    \n    // Títulos e subtítulos (heurística)\n    if (text.match(/^[A-Z\\s\\d\\.]{3,50}$/m)) score *= 1.4;\n    \n    // Parágrafos com números e dados\n    const numbers = text.match(/\\d+\\.?\\d*/g) || [];\n    if (numbers.length > 5) score *= 1.2;\n    \n    // Listas e enumerações\n    if (text.match(/^\\s*[\\d\\-\\*•]\\s+/m)) score *= 1.1;\n    \n    // Chunks mais longos (mais contexto)\n    if (text.length > 600) score *= 1.1;\n    \n    return Math.min(score, 2.0);\n  }\n\n  calculatePercentiles(values) {\n    if (!values.length) return {};\n    \n    const sorted = values.sort((a, b) => a - b);\n    const percentiles = {};\n    \n    [25, 50, 75, 90, 95].forEach(p => {\n      const index = Math.floor((p / 100) * sorted.length);\n      percentiles[`p${p}`] = sorted[index];\n    });\n    \n    return percentiles;\n  }\n\n  estimateCost(totalChunks) {\n    const tokensPerChunk = this.config.chunkSize;\n    const totalTokens = totalChunks * tokensPerChunk;\n    \n    return {\n      embedding: {\n        model: this.config.embeddingModel,\n        tokens: totalTokens,\n        cost: (totalTokens / 1000) * 0.00013 // $0.00013 per 1K tokens\n      },\n      total: (totalTokens / 1000) * 0.00013\n    };\n  }\n\n  // Limpar recursos\n  cleanup() {\n    if (this.pdfWorker) {\n      this.pdfWorker.terminate();\n      this.pdfWorker = null;\n    }\n    this.embeddingCache.clear();\n    this.responseCache.clear();\n  }\n}\n```\n\n## 4. Componente de Setup Inicial Melhorado","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":15,"totalChunks":43,"timestamp":1749751085616}}],["475bba3f-97d5-4b9c-8045-54c1285dc7f7",{"pageContent":"npm install\n```\n\n**Resolução de módulo em tsconfig.json**\n```json\n{\n  \"compilerOptions\": {\n    \"moduleResolution\": \"node\"\n  }\n}\n```\n\n### Problema 2: Conflitos com Tailwind CSS\n\n**Solução para cores que param de funcionar após usar withMT()**\n```javascript\nconst withMT = require('@material-tailwind/react/utils/withMT');\n\nmodule.exports = withMT({\n  content: [\n    './index.html',\n    './src/**/*.{vue,js,ts,jsx,tsx}',\n    'path-to-your-node_modules/@material-tailwind/react/components/**/*.{js,ts,jsx,tsx}',\n    'path-to-your-node_modules/@material-tailwind/react/theme/components/**/*.{js,ts,jsx,tsx}',\n  ],\n  theme: {\n    extend: {}, // Não sobrescrever o tema padrão completamente\n  },\n  plugins: [],\n});\n```\n\n### Problema 3: Erros de build com Vite\n\n**Soluções comuns**\n\n**Limpar cache npm e reinstalar**\n```bash\nnpm cache clean --force\nrm -rf node_modules package-lock.json\nnpm install\n```\n\n**Configuração Vite com PostCSS explícito**\n```javascript\n// vite.config.js\nimport { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\nimport tailwindcss from 'tailwindcss'\n\nexport default defineConfig({\n  plugins: [react()],\n  css: {\n    postcss: {\n      plugins: [tailwindcss()],\n    },\n  }\n})\n```\n\n### Problema 4: Problemas de resolução de módulo\n\n**Downgrade de versão do pacote**\n```bash\nnpm install @material-tailwind/react@2.0.5  # Versão estável conhecida\n```\n\n**Verificar versões corretas no package.json**\n```json\n{\n  \"dependencies\": {\n    \"@material-tailwind/react\": \"^2.1.10\"\n  },\n  \"devDependencies\": {\n    \"@types/react\": \"18.2.42\",\n    \"tailwindcss\": \"^3.3.0\"\n  }\n}\n```\n\n### Problema 5: Componentes interativos não funcionando\n\n**Adicionar script ripple para versão HTML**\n```html\n<!-- Para versão HTML -->\n<script async src=\"node_modules/@material-tailwind/html/scripts/ripple.js\"></script>\n<!-- ou CDN -->\n<script async src=\"https://unpkg.com/@material-tailwind/html@latest/scripts/ripple.js\"></script>\n```\n\n### Problema 6: Ícones Material não exibindo","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":13,"totalChunks":15,"timestamp":1749751085616}}],["dccd13b6-366e-46ae-9d96-b27436464c72",{"pageContent":"{/* Informações */}\n        <Card className=\"bg-blue-50\">\n          <CardBody>\n            <Typography variant=\"h6\" className=\"mb-2\">\n              ℹ️ Informações Importantes\n            </Typography>\n            <ul className=\"space-y-1\">\n              <li>\n                <Typography variant=\"small\">\n                  • Este sistema usa os modelos mais avançados da OpenAI\n                </Typography>\n              </li>\n              <li>\n                <Typography variant=\"small\">\n                  • Custo estimado: ~$0.13 por 1M tokens de embedding\n                </Typography>\n              </li>\n              <li>\n                <Typography variant=\"small\">\n                  • Seus documentos são processados localmente no navegador\n                </Typography>\n              </li>\n              <li>\n                <Typography variant=\"small\">\n                  • Índices podem ser salvos para uso futuro sem API key\n                </Typography>\n              </li>\n            </ul>\n          </CardBody>\n        </Card>\n      </div>\n    </div>\n  );\n}\n```\n\n## 5. Interface Principal Completa com Todas as Funcionalidades\n\n```javascript\n// src/components/HighQualityRAG.jsx\nimport React, { useState, useEffect, useCallback, useRef } from 'react';\nimport {\n  Card,\n  CardBody,\n  Typography,\n  Button,\n  Textarea,\n  Progress,\n  Alert,\n  Chip,\n  IconButton,\n  Dialog,\n  DialogHeader,\n  DialogBody,\n  DialogFooter,\n  List,\n  ListItem,\n  Menu,\n  MenuHandler,\n  MenuList,\n  MenuItem,\n  Tooltip,\n} from '@material-tailwind/react';\nimport {\n  DocumentIcon,\n  MagnifyingGlassIcon,\n  ArrowDownTrayIcon,\n  ArrowUpTrayIcon,\n  ChartBarIcon,\n  CurrencyDollarIcon,\n  Cog6ToothIcon,\n  XMarkIcon,\n  ClockIcon,\n  CheckCircleIcon,\n  ExclamationCircleIcon,\n  SparklesIcon,\n  FolderIcon,\n  TrashIcon,\n} from '@heroicons/react/24/outline';\nimport { HighQualityRAGService } from '../services/highQualityRAG.service';\nimport { ConfigService } from '../services/config.service';","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":20,"totalChunks":43,"timestamp":1749751085616}}],["b081e4e5-aa7d-439b-9030-1fa00e9817ea",{"pageContent":"{\n  \"name\": \"podcast-poc\",\n  \"private\": true,\n  \"version\": \"0.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"vite\",\n    \"build\": \"vite build\",\n    \"lint\": \"eslint .\",\n    \"preview\": \"vite preview\"\n  },\n  \"dependencies\": {\n    \"@heroicons/react\": \"^2.2.0\",\n    \"@material-tailwind/react\": \"^2.1.10\",\n    \"@orama/orama\": \"^3.1.7\",\n    \"audiobuffer-to-wav\": \"^1.0.0\",\n    \"buffer\": \"^6.0.3\",\n    \"file-saver\": \"^2.0.5\",\n    \"hnswlib-node\": \"^3.0.0\",\n    \"openai\": \"^5.3.0\",\n    \"pdf-parse\": \"^1.1.1\",\n    \"pdfjs-dist\": \"^5.3.31\",\n    \"react\": \"^19.1.0\",\n    \"react-dom\": \"^19.1.0\",\n    \"stream-browserify\": \"^3.0.0\",\n    \"uuid\": \"^11.1.0\",\n    \"wavesurfer.js\": \"^7.9.5\"\n  },\n  \"devDependencies\": {\n    \"@eslint/js\": \"^9.25.0\",\n    \"@radix-ui/react-slot\": \"^1.2.3\",\n    \"@types/react\": \"^19.1.2\",\n    \"@types/react-dom\": \"^19.1.2\",\n    \"@vitejs/plugin-react\": \"^4.4.1\",\n    \"autoprefixer\": \"^10.4.21\",\n    \"class-variance-authority\": \"^0.7.1\",\n    \"clsx\": \"^2.1.1\",\n    \"eslint\": \"^9.25.0\",\n    \"eslint-plugin-react-hooks\": \"^5.2.0\",\n    \"eslint-plugin-react-refresh\": \"^0.4.19\",\n    \"globals\": \"^16.0.0\",\n    \"postcss\": \"^8.5.4\",\n    \"tailwind-merge\": \"^3.3.1\",\n    \"tailwindcss\": \"^3.4.17\",\n    \"vite\": \"^6.3.5\"\n  }\n}","metadata":{"source":"package.json","section":"","chunkIndex":0,"totalChunks":1,"timestamp":1749751085616}}],["e8115602-7c95-4a30-ab19-e721d02756e7",{"pageContent":"{/* Informações do documento */}\n          {stats && !progress && (\n            <div className=\"mt-4 p-3 bg-gray-50 rounded-lg\">\n              <div className=\"grid grid-cols-2 md:grid-cols-4 gap-4 text-center\">\n                <div>\n                  <Typography variant=\"small\" color=\"gray\">Páginas</Typography>\n                  <Typography className=\"font-bold\">{stats.pagesProcessed}</Typography>\n                </div>\n                <div>\n                  <Typography variant=\"small\" color=\"gray\">Chunks</Typography>\n                  <Typography className=\"font-bold\">{stats.totalChunks}</Typography>\n                </div>\n                <div>\n                  <Typography variant=\"small\" color=\"gray\">Tokens</Typography>\n                  <Typography className=\"font-bold\">{(stats.totalTokens / 1000).toFixed(1)}k</Typography>\n                </div>\n                <div>\n                  <Typography variant=\"small\" color=\"gray\">Custo</Typography>\n                  <Typography className=\"font-bold text-green-600\">\n                    ${ragService.estimateCost(stats.totalChunks).total.toFixed(3)}\n                  </Typography>\n                </div>\n              </div>\n            </div>\n          )}\n        </CardBody>\n      </Card>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":36,"totalChunks":43,"timestamp":1749751085616}}],["85687305-0288-4749-8ff3-57f810c74355",{"pageContent":"$0.60\n                          </Typography>\n                        </div>\n                      </div>\n                      \n                      <div className=\"border-t pt-3 flex justify-between\">\n                        <Typography className=\"font-bold\">Total Estimado:</Typography>\n                        <Typography className=\"font-bold text-green-600 text-lg\">\n                          ${(costs.total + 0.60).toFixed(2)}\n                        </Typography>\n                      </div>\n                    </div>\n                  );\n                })()}\n              </CardBody>\n            </Card>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":29,"totalChunks":43,"timestamp":1749751085616}}],["a38decfb-a2e3-4029-a0da-26ec022c3cf3",{"pageContent":"{/* Informações adicionais */}\n            {stats.duplicateChunks > 0 && (\n              <Alert color=\"amber\" className=\"flex items-center\">\n                <ExclamationCircleIcon className=\"h-5 w-5 mr-2\" />\n                {stats.duplicateChunks} chunks duplicados foram detectados e otimizados\n              </Alert>\n            )}\n          </div>\n        )}\n      </DialogBody>\n      <DialogFooter>\n        <Button variant=\"text\" onClick={() => setShowStats(false)}>\n          Fechar\n        </Button>\n      </DialogFooter>\n    </Dialog>\n  );","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":30,"totalChunks":43,"timestamp":1749751085616}}],["77c98b8f-2bf3-4e64-9805-e1b80825004d",{"pageContent":"const root = ReactDOM.createRoot(document.getElementById(\"root\"));\nroot.render(\n  <React.StrictMode>\n    <ThemeProvider>\n      <App />\n    </ThemeProvider>\n  </React.StrictMode>\n);\n```\n\n## 2. Lista completa de componentes disponíveis\n\n### Componentes de navegação\n- **Navbar**: Barras de navegação responsivas com suporte a dropdown\n- **Breadcrumbs**: Navegação hierárquica com separadores personalizáveis\n- **Sidebar**: Navegação lateral colapsável com menus multinível\n- **Menu**: Menus dropdown com posicionamento e aninhamento\n- **Mega Menu**: Menus grandes para estruturas complexas\n- **Pagination**: Navegação de páginas com botões personalizáveis\n- **Tabs**: Sistemas de abas horizontais e verticais\n\n### Componentes de formulário\n- **Input**: Campos de texto com variantes (outlined, filled, standard)\n- **Input Number**: Campos numéricos com controles de incremento\n- **Input Phone**: Entrada de telefone com seleção de código de país\n- **Textarea**: Entrada de texto multilinha com redimensionamento automático\n- **Select**: Seleção dropdown com busca e multi-seleção\n- **Checkbox**: Caixas de seleção simples e agrupadas\n- **Radio Button**: Grupos de botões de rádio\n- **Switch**: Interruptores de alternância\n- **Form**: Layouts completos de formulário com validação\n\n### Componentes de exibição de dados\n- **Table**: Tabelas avançadas com ordenação, paginação e busca\n- **Card**: Componentes de cartão versáteis com cabeçalhos e rodapés\n- **List**: Listas simples e complexas com seleção\n- **Avatar**: Avatares de usuário com indicadores de status\n- **Badge**: Badges de notificação com contadores\n- **Chip**: Elementos compactos para tags e filtros\n- **Typography**: Componentes de texto com hierarquia Material Design\n- **Timeline**: Linhas do tempo verticais","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":1,"totalChunks":15,"timestamp":1749751085616}}],["7a273937-8f16-420b-99c1-09fca46f100d",{"pageContent":"## 8. Package.json Atualizado\n\n```json\n{\n  \"name\": \"rag-openai-frontend\",\n  \"version\": \"2.0.0\",\n  \"type\": \"module\",\n  \"scripts\": {\n    \"dev\": \"vite\",\n    \"build\": \"vite build\",\n    \"preview\": \"vite preview\"\n  },\n  \"dependencies\": {\n    \"react\": \"^18.2.0\",\n    \"react-dom\": \"^18.2.0\",\n    \"@material-tailwind/react\": \"^2.1.10\",\n    \"@heroicons/react\": \"^2.0.18\",\n    \"openai\": \"^4.38.0\",\n    \"@orama/orama\": \"^2.0.0\",\n    \"langchain\": \"^0.1.36\",\n    \"pdfjs-dist\": \"^3.11.174\"\n  },\n  \"devDependencies\": {\n    \"@vitejs/plugin-react\": \"^4.2.1\",\n    \"vite\": \"^5.2.0\",\n    \"tailwindcss\": \"^3.4.0\",\n    \"autoprefixer\": \"^10.4.18\",\n    \"postcss\": \"^8.4.35\"\n  }\n}\n```\n\n## Resumo das Funcionalidades Implementadas\n\n### 1. **Processamento Avançado**\n- ✅ Web Worker para processar PDFs sem travar a UI\n- ✅ Chunking inteligente com análise de importância\n- ✅ Progress tracking detalhado em duas fases\n- ✅ Cache de embeddings para evitar reprocessamento\n\n### 2. **Busca e Respostas**\n- ✅ Busca semântica com threshold configurável\n- ✅ Reranking opcional com GPT-3.5\n- ✅ Streaming de respostas em tempo real\n- ✅ Cache de respostas para queries idênticas\n\n### 3. **Gestão de Dados**\n- ✅ Export/Import de índices completos\n- ✅ Histórico de consultas persistente\n- ✅ Estatísticas detalhadas do documento\n- ✅ Estimativa de custos em tempo real\n\n### 4. **UX Profissional**\n- ✅ Interface responsiva e moderna\n- ✅ Feedback visual rico durante processamento\n- ✅ Tratamento robusto de erros\n- ✅ Atalhos de teclado (Ctrl+Enter)\n\n### 5. **Performance**\n- ✅ Rate limiting automático\n- ✅ Retry automático em falhas\n- ✅ Gerenciamento inteligente de memória\n- ✅ Bundle splitting para carregamento rápido\n\nEste sistema está pronto para produção e oferece a mais alta qualidade disponível com os modelos da OpenAI!","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":42,"totalChunks":43,"timestamp":1749751085616}}],["b0b141fe-63f2-4c45-b333-ba76212f8de6",{"pageContent":"## 4. Componente de Setup Inicial Melhorado\n\n```javascript\n// src/components/ApiKeySetup.jsx\nimport React, { useState, useEffect } from 'react';\nimport {\n  Card,\n  CardBody,\n  Typography,\n  Input,\n  Button,\n  Alert,\n  List,\n  ListItem,\n  Chip,\n} from '@material-tailwind/react';\nimport { \n  KeyIcon, \n  DocumentIcon, \n  CheckCircleIcon,\n  ExclamationTriangleIcon \n} from '@heroicons/react/24/outline';\nimport { ConfigService } from '../services/config.service';\n\nexport function ApiKeySetup({ onComplete }) {\n  const [apiKey, setApiKey] = useState('');\n  const [error, setError] = useState('');\n  const [isValidating, setIsValidating] = useState(false);\n  const [availableIndexes, setAvailableIndexes] = useState([]);\n  const [showIndexes, setShowIndexes] = useState(false);\n\n  useEffect(() => {\n    // Verificar se há índices salvos\n    const indexes = ConfigService.getAvailableIndexes();\n    setAvailableIndexes(indexes);\n    setShowIndexes(indexes.length > 0);\n  }, []);\n\n  const validateAndSaveKey = async () => {\n    if (!ConfigService.validateApiKey(apiKey)) {\n      setError('Formato de API key inválido. Deve começar com \"sk-\"');\n      return;\n    }\n\n    setIsValidating(true);\n    setError('');\n\n    try {\n      // Testar a API key\n      const response = await fetch('https://api.openai.com/v1/models', {\n        headers: {\n          'Authorization': `Bearer ${apiKey}`\n        }\n      });\n\n      if (!response.ok) {\n        if (response.status === 401) {\n          throw new Error('API key inválida ou expirada');\n        } else if (response.status === 429) {\n          throw new Error('Limite de requisições excedido. Tente novamente em alguns segundos.');\n        } else {\n          throw new Error('Erro ao validar API key');\n        }\n      }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":16,"totalChunks":43,"timestamp":1749751085616}}],["40aa6cde-65e8-4ab8-bd02-c6c50a6bd3c7",{"pageContent":"export const useAudioRecording = () => {\n  const [isRecording, setIsRecording] = useState(false);\n  const [isSupported, setIsSupported] = useState(false);\n  const [hasPermission, setHasPermission] = useState(false);\n  const [duration, setDuration] = useState(0);\n  const [error, setError] = useState(null);\n\n  const audioServiceRef = useRef(null);\n  const durationIntervalRef = useRef(null);\n\n  // Initialize audio service\n  useEffect(() => {\n    audioServiceRef.current = createAudioService();\n    setIsSupported(audioServiceRef.current.isSupported());\n\n    return () => {\n      // Cleanup on unmount\n      if (audioServiceRef.current) {\n        audioServiceRef.current.cleanup?.();\n      }\n      if (durationIntervalRef.current) {\n        clearInterval(durationIntervalRef.current);\n      }\n    };\n  }, []);\n\n  /**\n   * Request microphone permission\n   * @returns {Promise<boolean>} Whether permission was granted\n   */\n  const requestPermission = useCallback(async () => {\n    if (!audioServiceRef.current) return false;\n\n    try {\n      const granted = await audioServiceRef.current.requestPermission();\n      setHasPermission(granted);\n      setError(null);\n      return granted;\n    } catch (err) {\n      setError(err.message);\n      return false;\n    }\n  }, []);\n\n  /**\n   * Start recording audio\n   * @param {Object} options - Recording options\n   * @param {function} options.onComplete - Callback when recording completes\n   * @param {function} options.onError - Callback for errors\n   * @returns {Promise<void>}\n   */\n  const startRecording = useCallback(async ({ onComplete, onError } = {}) => {\n    if (!audioServiceRef.current || !isSupported) {\n      const errorMsg = 'Audio recording is not supported';\n      setError(errorMsg);\n      if (onError) onError(new Error(errorMsg));\n      return;\n    }","metadata":{"source":"src/hooks/useAudioRecording.js","section":"","chunkIndex":1,"totalChunks":4,"timestamp":1749751085616}}],["95d60022-ae4b-4f26-9dba-0e5af35a3f63",{"pageContent":"export const createAudioService = () => {\n  return new AudioService();\n};","metadata":{"source":"src/services/audio.service.js","section":"","chunkIndex":4,"totalChunks":5,"timestamp":1749751085616}}],["b161f6df-b4e1-494e-9631-fe5233f6a077",{"pageContent":"// Busca vetorial\n      let results;\n      try {\n        results = await search(this.db, {\n          mode: 'vector',\n          vector: {\n            value: queryEmbedding,\n            property: 'embedding'\n          },\n          limit: limit * 2,\n          threshold: threshold,\n          includeVectors: false\n        });\n      } catch (searchError) {\n        console.warn('Vector search failed, falling back to text search:', searchError);\n        // Fallback to simple text search\n        results = await search(this.db, {\n          term: query,\n          limit: limit\n        });\n      }\n\n      if (!results || !results.hits || results.hits.length === 0) {\n        return [];\n      }\n\n      let relevantDocs = results.hits.map(hit => ({\n        text: hit.document.text,\n        score: hit.score || 0.5,\n        metadata: {\n          pageNumber: hit.document.pageNumber,\n          source: hit.document.source,\n          chunkIndex: hit.document.chunkIndex,\n          totalTokens: hit.document.totalTokens,\n          importance: hit.document.importance,\n          hash: hit.document.hash\n        }\n      }));\n\n      // Reranking com GPT (with error handling)\n      if (useReranking && relevantDocs.length > 0) {\n        try {\n          relevantDocs = await this.rerankDocuments(query, relevantDocs, limit);\n        } catch (rerankError) {\n          console.warn('Reranking failed, using original order:', rerankError);\n        }\n      }\n\n      // Incluir contexto adjacente se solicitado\n      if (includeContext) {\n        try {\n          relevantDocs = await this.expandWithContext(relevantDocs);\n        } catch (contextError) {\n          console.warn('Context expansion failed:', contextError);\n        }\n      }\n\n      return relevantDocs.slice(0, limit);\n    } catch (error) {\n      console.error('Search failed:', error);\n      throw new Error(`Erro na busca: ${error.message}`);\n    }\n  }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":7,"totalChunks":18,"timestamp":1749751085616}}],["4c17d792-6fe0-4316-93dc-155240b01e72",{"pageContent":"## 3. Exemplos de código para cada componente\n\n### Implementação básica de Button\n```jsx\nimport { Button } from \"@material-tailwind/react\";\n\nexport function ButtonExample() {\n  return (\n    <div className=\"flex gap-4\">\n      <Button>Default</Button>\n      <Button variant=\"gradient\">Gradient</Button>\n      <Button variant=\"outlined\">Outlined</Button>\n      <Button variant=\"text\">Text</Button>\n    </div>\n  );\n}\n```\n\n### Card avançado com múltiplas seções\n```jsx\nimport {\n  Card,\n  CardHeader,\n  CardBody,\n  CardFooter,\n  Typography,\n  Button,\n} from \"@material-tailwind/react\";\n\nexport function CardExample() {\n  return (\n    <Card className=\"mt-6 w-96\">\n      <CardHeader color=\"blue-gray\" className=\"relative h-56\">\n        <img\n          src=\"https://images.unsplash.com/photo-1540553016722-983e48a2cd10\"\n          alt=\"card-image\"\n        />\n      </CardHeader>\n      <CardBody>\n        <Typography variant=\"h5\" color=\"blue-gray\" className=\"mb-2\">\n          UI/UX Review Check\n        </Typography>\n        <Typography>\n          The place is close to Barceloneta Beach and bus stop just 2 min by walk.\n        </Typography>\n      </CardBody>\n      <CardFooter className=\"pt-0\">\n        <Button>Read More</Button>\n      </CardFooter>\n    </Card>\n  );\n}\n```\n\n### Formulário com validação\n```jsx\nimport { Input, Button, Typography, Card } from \"@material-tailwind/react\";","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":3,"totalChunks":15,"timestamp":1749751085616}}],["d74bab06-748f-49bf-8302-047779739538",{"pageContent":"export class OpenAIService {\n  /**\n   * @param {string} apiKey - OpenAI API key\n   */\n  constructor(apiKey) {\n    this.apiKey = apiKey;\n    this.chatModel = null;\n  }\n\n  /**\n   * Initialize the chat model\n   * @private\n   */\n  _initChatModel() {\n    if (!this.chatModel) {\n      this.chatModel = new ChatOpenAI({\n        openAIApiKey: this.apiKey,\n        modelName: OPENAI_CONFIG.CHAT_MODEL,\n        temperature: OPENAI_CONFIG.TEMPERATURE,\n        streaming: true,\n      });\n    }\n    return this.chatModel;\n  }\n\n  /**\n   * Send a chat message with streaming response\n   * @param {string} prompt - User prompt\n   * @param {string} context - PDF context text\n   * @param {function} onChunk - Callback for each response chunk\n   * @returns {Promise<string>} Complete response text\n   * @throws {Error} When chat request fails\n   */\n  async sendChatMessage(prompt, context = '', onChunk) {\n    try {\n      const model = this._initChatModel();\n      \n      const messages = [\n        new SystemMessage(\n          context \n            ? `Você é um assistente útil. Aqui está o contexto do documento PDF extraído:\\n\\n${context.substring(0, OPENAI_CONFIG.MAX_CONTEXT_LENGTH)}...`\n            : 'Você é um assistente útil.'\n        ),\n        new HumanMessage(prompt)\n      ];\n\n      const stream = await model.stream(messages);\n      let fullResponse = '';\n      \n      for await (const chunk of stream) {\n        const content = chunk.content;\n        if (typeof content === 'string') {\n          fullResponse += content;\n          if (onChunk) {\n            onChunk(fullResponse);\n          }\n        }\n      }\n      \n      return fullResponse;\n    } catch (error) {\n      console.error('Chat request failed:', error);\n      throw new Error(ERROR_MESSAGES.CHAT_REQUEST_FAILED);\n    }\n  }","metadata":{"source":"src/services/openai.service.js","section":"","chunkIndex":1,"totalChunks":4,"timestamp":1749751085616}}],["0ea02a9c-a93a-43d1-883a-e6bbc04948fd",{"pageContent":"export function HighQualityRAG() {\n  const [ragService] = useState(() => new HighQualityRAGService());\n  const [isInitialized, setIsInitialized] = useState(false);\n  const [isProcessing, setIsProcessing] = useState(false);\n  const [uploadedFile, setUploadedFile] = useState(null);\n  const [progress, setProgress] = useState(null);\n  const [query, setQuery] = useState('');\n  const [response, setResponse] = useState(null);\n  const [error, setError] = useState(null);\n  const [stats, setStats] = useState(null);\n  const [showStats, setShowStats] = useState(false);\n  const [isStreaming, setIsStreaming] = useState(false);\n  const [savedIndexes, setSavedIndexes] = useState([]);\n  const [showSavedIndexes, setShowSavedIndexes] = useState(false);\n  const [queryHistory, setQueryHistory] = useState([]);\n  const fileInputRef = useRef(null);\n  const indexInputRef = useRef(null);\n\n  // Inicializar serviço\n  useEffect(() => {\n    const init = async () => {\n      try {\n        await ragService.initialize();\n        setIsInitialized(true);\n        \n        // Carregar histórico de queries\n        const history = JSON.parse(localStorage.getItem('query_history') || '[]');\n        setQueryHistory(history);\n      } catch (err) {\n        setError(err.message);\n      }\n    };\n    \n    if (ConfigService.hasApiKey()) {\n      init();\n    }\n  }, [ragService]);\n\n  // Cleanup ao desmontar\n  useEffect(() => {\n    return () => {\n      ragService.cleanup();\n    };\n  }, [ragService]);\n\n  // Processar PDF\n  const handleFileUpload = useCallback(async (event) => {\n    const file = event.target.files[0];\n    if (!file) return;\n\n    if (file.type !== 'application/pdf') {\n      setError('Por favor, selecione um arquivo PDF');\n      return;\n    }\n\n    if (file.size > 100 * 1024 * 1024) {\n      setError('Arquivo muito grande. Máximo: 100MB');\n      return;\n    }\n\n    setUploadedFile(file);\n    setIsProcessing(true);\n    setError(null);\n    setProgress(null);\n    setResponse(null);","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":21,"totalChunks":43,"timestamp":1749751085616}}],["c57fb741-d599-474b-ac41-5a2099a4a963",{"pageContent":"<CardBody className=\"text-center\">\n                  <Typography variant=\"h6\" color=\"purple\">Média/Chunk</Typography>\n                  <Typography variant=\"h3\" color=\"blue-gray\">\n                    {stats.averageChunkSize}\n                  </Typography>\n                </CardBody>\n              </Card>\n            </div>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":26,"totalChunks":43,"timestamp":1749751085616}}],["b14826fe-2bc5-431d-98fc-deac5e7ab360",{"pageContent":"### Recursos exclusivos da versão Pro\n- **Componentes avançados de dashboard**: Widgets de análise, cartões KPI, gráficos avançados\n- **Templates de e-commerce**: Catálogos de produtos, carrinhos de compras, fluxos de checkout\n- **Páginas de autenticação**: Login, signup, redefinição de senha com múltiplas variantes\n- **Templates de landing page**: Páginas de marketing, landing pages SaaS, portfólios\n- **Blocks premium**: Seções hero, depoimentos, tabelas de preços, seções de equipe\n\n## 8. Compatibilidade com Tailwind CSS v3\n\n### Status atual de compatibilidade\n- **Versão Tailwind suportada**: v3.0+ (até v3.4.x)\n- **Versão mínima requerida**: Tailwind CSS v3.0\n- **Modo JIT**: Totalmente suportado e recomendado\n\n### Suporte ao modo JIT\n```javascript\n// tailwind.config.js\nmodule.exports = withMT({\n  mode: 'jit', // Habilitado por padrão em v3+\n  content: [\n    './src/**/*.{js,jsx,ts,tsx}',\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n});\n```\n\n### Integração com novos recursos do Tailwind v3\n\n**Suporte a valores arbitrários**\n```javascript\nimport { Button } from \"@material-tailwind/react\";\n\nexport default function ArbitraryValues() {\n  return (\n    <Button className=\"bg-[#1da1f2] text-[14px] p-[10px] rounded-[8px]\">\n      Custom Values\n    </Button>\n  );\n}\n```\n\n**Paleta de cores aprimorada**\nMaterial Tailwind suporta automaticamente a paleta expandida do Tailwind v3, incluindo cores como cyan, rose, fuchsia e lime por padrão.\n\n**Container Queries (v3.2+)**\n```javascript\n<Card className=\"@container\">\n  <div className=\"@sm:flex @md:grid-cols-2\">\n    {/* Responsivo ao tamanho do container */}\n  </div>\n</Card>\n```\n\n### Considerações de migração\n\n**De Tailwind v2 para v3**\n```bash\nnpm install -D tailwindcss@latest postcss@latest autoprefixer@latest\nnpm install @material-tailwind/react@latest\n```","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":9,"totalChunks":15,"timestamp":1749751085616}}],["7a9a932c-10ac-41f4-9616-f76e3e8cca34",{"pageContent":"# React + Vite\n\nThis template provides a minimal setup to get React working in Vite with HMR and some ESLint rules.\n\nCurrently, two official plugins are available:\n\n- [@vitejs/plugin-react](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react) uses [Babel](https://babeljs.io/) for Fast Refresh\n- [@vitejs/plugin-react-swc](https://github.com/vitejs/vite-plugin-react/blob/main/packages/plugin-react-swc) uses [SWC](https://swc.rs/) for Fast Refresh\n\n## Expanding the ESLint configuration\n\nIf you are developing a production application, we recommend using TypeScript with type-aware lint rules enabled. Check out the [TS template](https://github.com/vitejs/vite/tree/main/packages/create-vite/template-react-ts) for information on how to integrate TypeScript and [`typescript-eslint`](https://typescript-eslint.io) in your project.","metadata":{"source":"README.md","section":"","chunkIndex":0,"totalChunks":1,"timestamp":1749751085616}}],["647d64bc-9236-4eb5-ad23-f8c4963ecbe9",{"pageContent":"# Sistema RAG Frontend com OpenAI - Implementação Completa de Alta Qualidade\n\n## 📋 Descrição Detalhada do Projeto para Claude Code\n\n### Visão Geral\nEste projeto implementa um sistema RAG (Retrieval-Augmented Generation) completo que roda inteiramente no navegador (frontend-only), projetado para processar documentos PDF de até 100MB e fornecer respostas contextualizadas usando os modelos mais avançados da OpenAI.\n\n### Arquitetura e Fluxo de Dados\n```\n1. Upload PDF → 2. Extração de Texto → 3. Chunking Inteligente → 4. Geração de Embeddings\n                                                                             ↓\n8. Resposta Contextualizada ← 7. Geração GPT-4 ← 6. Busca Semântica ← 5. Armazenamento Vetorial\n```\n\n### Características Técnicas Principais\n- **Stack**: React + Vite + Material Tailwind + OpenAI API + Orama (vector DB)\n- **Modelos**: text-embedding-3-large (3072 dims) + GPT-4 Turbo\n- **Processamento**: 100% no navegador com Web Workers\n- **Persistência**: Export/Import de índices vetoriais completos\n- **Segurança**: API key fornecida pelo usuário, armazenada apenas em sessionStorage\n\n### Funcionalidades Implementadas\n1. **Setup Inicial**: Interface para usuário inserir sua API key OpenAI\n2. **Processamento de PDF**: \n   - Extração de texto página por página\n   - Chunking semântico com overlap\n   - Progress tracking detalhado\n   - Análise de importância dos chunks\n3. **Sistema de Embeddings**:\n   - Geração com modelo de alta qualidade\n   - Processamento em batches com rate limiting\n   - Cache e reutilização de embeddings\n4. **Busca e Resposta**:\n   - Busca vetorial com threshold de similaridade\n   - Reranking opcional com GPT-3.5\n   - Streaming de respostas do GPT-4\n5. **Gestão de Dados**:\n   - Export de índice completo (embeddings + metadados)\n   - Import de índices salvos\n   - Estatísticas e análise de custos","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":0,"totalChunks":43,"timestamp":1749751085616}}],["ac4521e2-344e-4b83-822b-dd5c718e676d",{"pageContent":"### Considerações de migração\n\n**De Tailwind v2 para v3**\n```bash\nnpm install -D tailwindcss@latest postcss@latest autoprefixer@latest\nnpm install @material-tailwind/react@latest\n```\n\n**Atualizações de configuração**\n```javascript\n// Não usar mais 'purge', usar 'content' em vez disso\nmodule.exports = withMT({\n  content: ['./src/**/*.{js,jsx,ts,tsx}'], // era 'purge' em v2\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n});\n```\n\n## 9. Exemplos de layouts completos\n\n### Layout de Dashboard\n```jsx\nimport {\n  Card,\n  Typography,\n  List,\n  ListItem,\n  ListItemPrefix,\n  Accordion,\n  AccordionHeader,\n  AccordionBody,\n} from \"@material-tailwind/react\";\n\nexport function DashboardLayout() {\n  return (\n    <div className=\"flex\">\n      {/* Sidebar */}\n      <Card className=\"h-[calc(100vh-2rem)] w-full max-w-[20rem] p-4 shadow-xl shadow-blue-gray-900/5\">\n        <div className=\"mb-2 p-4\">\n          <Typography variant=\"h5\" color=\"blue-gray\">\n            Sidebar\n          </Typography>\n        </div>\n        <List>\n          <Accordion open={open === 1}>\n            <ListItem className=\"p-0\" selected={open === 1}>\n              <AccordionHeader onClick={() => handleOpen(1)} className=\"border-b-0 p-3\">\n                <ListItemPrefix>\n                  <PresentationChartBarIcon className=\"h-5 w-5\" />\n                </ListItemPrefix>\n                <Typography color=\"blue-gray\" className=\"mr-auto font-normal\">\n                  Dashboard\n                </Typography>\n              </AccordionHeader>\n            </ListItem>\n          </Accordion>\n        </List>\n      </Card>\n      \n      {/* Main Content */}\n      <div className=\"flex-1 p-4\">\n        <Typography variant=\"h4\" color=\"blue-gray\" className=\"mb-4\">\n          Dashboard\n        </Typography>\n        {/* Dashboard content */}\n      </div>\n    </div>\n  );\n}\n```","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":10,"totalChunks":15,"timestamp":1749751085616}}],["929cdb13-e4c4-40bb-a630-45d9deb47c02",{"pageContent":"---\n\n## 1. Serviço de Configuração e Gerenciamento de API Key\n\n```javascript\n// src/services/config.service.js\nexport class ConfigService {\n  static API_KEY_STORAGE = 'openai_api_key';\n  static INDEX_METADATA_STORAGE = 'rag_index_metadata';\n  \n  // API Key Management\n  static saveApiKey(apiKey) {\n    sessionStorage.setItem(this.API_KEY_STORAGE, apiKey);\n  }\n  \n  static getApiKey() {\n    return sessionStorage.getItem(this.API_KEY_STORAGE);\n  }\n  \n  static hasApiKey() {\n    return !!this.getApiKey();\n  }\n  \n  static clearApiKey() {\n    sessionStorage.removeItem(this.API_KEY_STORAGE);\n  }\n  \n  static validateApiKey(apiKey) {\n    return apiKey && apiKey.startsWith('sk-') && apiKey.length > 20;\n  }\n  \n  // Index Metadata Management\n  static saveIndexMetadata(metadata) {\n    localStorage.setItem(this.INDEX_METADATA_STORAGE, JSON.stringify(metadata));\n  }\n  \n  static getIndexMetadata() {\n    const data = localStorage.getItem(this.INDEX_METADATA_STORAGE);\n    return data ? JSON.parse(data) : null;\n  }\n  \n  static clearIndexMetadata() {\n    localStorage.removeItem(this.INDEX_METADATA_STORAGE);\n  }\n  \n  // Available Indexes\n  static getAvailableIndexes() {\n    const keys = Object.keys(localStorage).filter(key => key.startsWith('rag_index_'));\n    return keys.map(key => {\n      const data = localStorage.getItem(key);\n      try {\n        const parsed = JSON.parse(data);\n        return parsed.metadata;\n      } catch {\n        return null;\n      }\n    }).filter(Boolean);\n  }\n}\n```\n\n## 2. Web Worker para Processamento de PDF\n\n```javascript\n// src/workers/pdf.worker.js\nimport * as pdfjsLib from 'pdfjs-dist';\n\n// Configurar worker do PDF.js\npdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":2,"totalChunks":43,"timestamp":1749751085616}}],["09378cba-e3b8-4c18-84b8-f3a2282a78fa",{"pageContent":"// Request permission if not already granted\n    if (!hasPermission) {\n      const granted = await requestPermission();\n      if (!granted) {\n        const errorMsg = 'Microphone permission required';\n        setError(errorMsg);\n        if (onError) onError(new Error(errorMsg));\n        return;\n      }\n    }\n\n    try {\n      setError(null);\n      setDuration(0);\n      setIsRecording(true);\n\n      // Start duration counter\n      durationIntervalRef.current = setInterval(() => {\n        setDuration(prev => prev + 1);\n      }, 1000);\n\n      await audioServiceRef.current.startRecording({\n        onStop: (audioBlob) => {\n          setIsRecording(false);\n          if (durationIntervalRef.current) {\n            clearInterval(durationIntervalRef.current);\n          }\n          if (onComplete) {\n            onComplete(audioBlob);\n          }\n        },\n        onError: (err) => {\n          setIsRecording(false);\n          setError(err.message);\n          if (durationIntervalRef.current) {\n            clearInterval(durationIntervalRef.current);\n          }\n          if (onError) {\n            onError(err);\n          }\n        }\n      });\n    } catch (err) {\n      setIsRecording(false);\n      setError(err.message);\n      if (durationIntervalRef.current) {\n        clearInterval(durationIntervalRef.current);\n      }\n      if (onError) {\n        onError(err);\n      }\n    }\n  }, [isSupported, hasPermission, requestPermission]);\n\n  /**\n   * Stop recording audio\n   */\n  const stopRecording = useCallback(() => {\n    if (audioServiceRef.current && isRecording) {\n      audioServiceRef.current.stopRecording();\n    }\n  }, [isRecording]);\n\n  /**\n   * Clear error state\n   */\n  const clearError = useCallback(() => {\n    setError(null);\n  }, []);","metadata":{"source":"src/hooks/useAudioRecording.js","section":"","chunkIndex":2,"totalChunks":4,"timestamp":1749751085616}}],["a2a8d7a8-012d-40a2-8927-d3cbe1b04c3d",{"pageContent":"{/* Card de Índices Salvos */}\n        {showIndexes && (\n          <Card>\n            <CardBody>\n              <Typography variant=\"h6\" className=\"mb-3 flex items-center gap-2\">\n                <DocumentIcon className=\"h-5 w-5\" />\n                Índices Salvos Encontrados\n              </Typography>\n              \n              <Typography variant=\"small\" color=\"gray\" className=\"mb-3\">\n                Você pode usar índices salvos anteriormente sem precisar de API key\n              </Typography>\n\n              <List>\n                {availableIndexes.map((index, idx) => (\n                  <ListItem key={idx} className=\"p-2\">\n                    <div className=\"flex items-center justify-between w-full\">\n                      <div>\n                        <Typography variant=\"small\" className=\"font-medium\">\n                          {index.stats?.documentName || 'Documento'}\n                        </Typography>\n                        <Typography variant=\"small\" color=\"gray\">\n                          {index.stats?.totalChunks} chunks • {new Date(index.created).toLocaleDateString()}\n                        </Typography>\n                      </div>\n                      <Chip\n                        value={`${index.stats?.pagesProcessed} páginas`}\n                        size=\"sm\"\n                        color=\"blue\"\n                      />\n                    </div>\n                  </ListItem>\n                ))}\n              </List>\n\n              <Button\n                onClick={loadExistingIndex}\n                variant=\"outlined\"\n                className=\"w-full mt-3\"\n                color=\"green\"\n              >\n                <CheckCircleIcon className=\"h-4 w-4 mr-2\" />\n                Usar Apenas Índices Salvos\n              </Button>\n            </CardBody>\n          </Card>\n        )}","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":19,"totalChunks":43,"timestamp":1749751085616}}],["1b679255-797c-4c51-af2f-312943808807",{"pageContent":"import OpenAI from 'openai';\nimport { create, insert, search, save, load } from '@orama/orama';\nimport { SimpleTextSplitter } from '../utils/textSplitter.js';\nimport { ConfigService } from './config.service';","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":0,"totalChunks":18,"timestamp":1749751085616}}],["db8dd6cf-a994-4de8-90c6-2a7f0dbe63b2",{"pageContent":"### Layout de E-commerce\n```jsx\nexport function EcommerceLayout() {\n  return (\n    <div className=\"min-h-screen bg-gray-50\">\n      {/* Header */}\n      <Navbar className=\"sticky top-0 z-10 h-max max-w-full rounded-none px-4 py-2 lg:px-8 lg:py-4\">\n        <div className=\"flex items-center justify-between text-blue-gray-900\">\n          <Typography as=\"a\" href=\"#\" className=\"mr-4 cursor-pointer py-1.5 font-medium\">\n            E-Shop\n          </Typography>\n          <div className=\"flex items-center gap-4\">\n            <Input type=\"search\" placeholder=\"Search...\" />\n            <IconButton variant=\"text\">\n              <ShoppingCartIcon className=\"h-5 w-5\" />\n            </IconButton>\n          </div>\n        </div>\n      </Navbar>\n\n      {/* Product Grid */}\n      <div className=\"container mx-auto px-4 py-8\">\n        <div className=\"grid grid-cols-1 gap-6 sm:grid-cols-2 lg:grid-cols-4\">\n          {/* Product cards */}\n        </div>\n      </div>\n    </div>\n  );\n}\n```","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":11,"totalChunks":15,"timestamp":1749751085616}}],["364c8b6a-927e-4c97-8b31-dbbc076f4c43",{"pageContent":"export const usePDF = () => {\n  const [extractedText, setExtractedText] = useState(null);\n  const [isProcessing, setIsProcessing] = useState(false);\n  const [progress, setProgress] = useState(0);\n  const [error, setError] = useState(null);\n\n  // Memoize service instance\n  const pdfService = useMemo(() => createPDFService(), []);\n\n  /**\n   * Extract text from PDF file\n   * @param {File} file - PDF file to process\n   * @returns {Promise<Object>} Extracted text data\n   */\n  const extractText = useCallback(async (file) => {\n    setIsProcessing(true);\n    setProgress(0);\n    setError(null);\n\n    try {\n      const result = await pdfService.extractText(file, (progressData) => {\n        setProgress(progressData.progress);\n      });\n\n      setExtractedText(result);\n      return result;\n    } catch (err) {\n      setError(err.message);\n      throw err;\n    } finally {\n      setIsProcessing(false);\n      setProgress(0);\n    }\n  }, [pdfService]);\n\n  /**\n   * Get PDF metadata\n   * @param {File} file - PDF file\n   * @returns {Promise<Object>} PDF metadata\n   */\n  const getMetadata = useCallback(async (file) => {\n    try {\n      return await pdfService.getMetadata(file);\n    } catch (err) {\n      setError(err.message);\n      throw err;\n    }\n  }, [pdfService]);\n\n  /**\n   * Validate PDF file\n   * @param {File} file - PDF file to validate\n   * @returns {boolean} Whether file is valid\n   */\n  const validateFile = useCallback((file) => {\n    try {\n      pdfService.validateFile(file);\n      return true;\n    } catch (err) {\n      setError(err.message);\n      return false;\n    }\n  }, [pdfService]);\n\n  /**\n   * Clear extracted text and reset state\n   */\n  const clearText = useCallback(() => {\n    setExtractedText(null);\n    setProgress(0);\n    setError(null);\n  }, []);\n\n  /**\n   * Clear error state\n   */\n  const clearError = useCallback(() => {\n    setError(null);\n  }, []);","metadata":{"source":"src/hooks/usePDF.js","section":"","chunkIndex":1,"totalChunks":3,"timestamp":1749751085616}}],["9aeb6817-a779-4706-969f-56da167acfa4",{"pageContent":"function reconstructPageText(textContent) {\n  const lines = {};\n  \n  textContent.items.forEach(item => {\n    const y = Math.round(item.transform[5]);\n    if (!lines[y]) lines[y] = [];\n    lines[y].push(item);\n  });\n  \n  return Object.keys(lines)\n    .sort((a, b) => b - a)\n    .map(y => {\n      return lines[y]\n        .sort((a, b) => a.transform[4] - b.transform[4])\n        .map(item => item.str)\n        .join(' ')\n        .trim();\n    })\n    .filter(line => line.length > 0)\n    .join('\\n');\n}\n```\n\n## 3. Serviço RAG Principal com Processamento Otimizado\n\n```javascript\n// src/services/highQualityRAG.service.js\nimport OpenAI from 'openai';\nimport { create, insert, search, save, load } from '@orama/orama';\nimport { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';\nimport { ConfigService } from './config.service';\n\nexport class HighQualityRAGService {\n  constructor() {\n    this.openai = null;\n    this.db = null;\n    this.initialized = false;\n    this.pdfWorker = null;\n    \n    // Configurações otimizadas para qualidade máxima\n    this.config = {\n      embeddingModel: 'text-embedding-3-large',\n      embeddingDimensions: 3072,\n      chatModel: 'gpt-4-turbo-preview',\n      chunkSize: 800,\n      chunkOverlap: 200,\n      temperature: 0.2,\n      maxTokens: 4000,\n      topK: 10,\n      similarityThreshold: 0.7,\n      batchSize: 5,\n      maxRetries: 3,\n      retryDelay: 1000\n    };\n\n    this.splitter = new RecursiveCharacterTextSplitter({\n      chunkSize: this.config.chunkSize,\n      chunkOverlap: this.config.chunkOverlap,\n      separators: [\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \": \", \" \", \"\"],\n      lengthFunction: (text) => this.estimateTokens(text)\n    });\n    \n    // Cache de embeddings para evitar reprocessamento\n    this.embeddingCache = new Map();\n  }\n\n  async initialize() {\n    const apiKey = ConfigService.getApiKey();\n    if (!apiKey) {\n      throw new Error('API key não configurada');\n    }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":4,"totalChunks":43,"timestamp":1749751085616}}],["db9c28f0-d3d9-45cb-8744-30bcb640807c",{"pageContent":"### Configuração para monorepo\n```javascript\nconst withMT = require(\"@material-tailwind/react/utils/withMT\");\n\nmodule.exports = withMT({\n  content: [\n    \"./index.html\",\n    \"./src/**/*.{js,ts,jsx,tsx}\",\n    \"path-to-your-node_modules/@material-tailwind/react/components/**/*.{js,ts,jsx,tsx}\",\n    \"path-to-your-node_modules/@material-tailwind/react/theme/components/**/*.{js,ts,jsx,tsx}\",\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n});\n```\n\n## 5. Customização de temas e cores\n\n### Paletas de cores personalizadas\n```javascript\n// tailwind.config.js\nconst withMT = require(\"@material-tailwind/react/utils/withMT\");\n\nmodule.exports = withMT({\n  theme: {\n    colors: {\n      // Substituir cores padrão completamente\n      white: '#ffffff',\n      purple: '#3f3cbb',\n      midnight: '#121063',\n      tahiti: '#3ab7bf',\n      bermuda: '#78dcca',\n    },\n    extend: {\n      colors: {\n        // Adicionar cores personalizadas à paleta existente\n        'custom-blue': '#3252df',\n        sky: {\n          50: \"#f0f9ff\",\n          100: \"#e0f2fe\",\n          200: \"#bae6fd\",\n          300: \"#7dd3fc\",\n          400: \"#38bdf8\",\n          500: \"#0ea5e9\",\n          600: \"#0284c7\",\n          700: \"#0369a1\",\n          800: \"#075985\",\n          900: \"#0c4a6e\",\n        },\n      },\n    },\n  },\n  plugins: [],\n});\n```\n\n### Customização através do ThemeProvider\n```javascript\nimport { ThemeProvider } from \"@material-tailwind/react\";","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":5,"totalChunks":15,"timestamp":1749751085616}}],["a3a1c5a3-9531-4d5f-bd03-d9ad8ae21b67",{"pageContent":"// Importar índice\n  const handleImport = useCallback(async (event) => {\n    const file = event.target.files[0];\n    if (!file) return;\n\n    setIsProcessing(true);\n    setError(null);\n\n    try {\n      const metadata = await ragService.importIndex(file);\n      setStats(metadata.stats);\n      setUploadedFile({ name: metadata.stats?.documentName || 'Documento importado' });\n      \n      alert(`✅ Índice importado com sucesso!\\n\\n` +\n        `📄 Documento: ${metadata.stats?.documentName || 'Desconhecido'}\\n` +\n        `📦 ${metadata.stats?.totalChunks || 0} chunks\\n` +\n        `📅 Criado em: ${new Date(metadata.created).toLocaleDateString()}`);\n    } catch (err) {\n      setError(err.message);\n    } finally {\n      setIsProcessing(false);\n    }\n  }, [ragService]);\n\n  // Carregar query do histórico\n  const loadFromHistory = (item) => {\n    setQuery(item.query);\n  };\n\n  // Limpar tudo\n  const handleReset = () => {\n    if (confirm('Deseja limpar todos os dados e começar novamente?')) {\n      ragService.cleanup();\n      setUploadedFile(null);\n      setStats(null);\n      setResponse(null);\n      setQuery('');\n      setError(null);\n      window.location.reload();\n    }\n  };","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":24,"totalChunks":43,"timestamp":1749751085616}}],["9f762773-b7e7-40a4-80b3-017bdcd306c5",{"pageContent":"/**\n   * Clear error state\n   */\n  const clearError = useCallback(() => {\n    setError(null);\n  }, []);\n\n  /**\n   * Format duration as MM:SS\n   * @param {number} seconds - Duration in seconds\n   * @returns {string} Formatted duration\n   */\n  const formatDuration = useCallback((seconds) => {\n    const minutes = Math.floor(seconds / 60);\n    const remainingSeconds = seconds % 60;\n    return `${minutes.toString().padStart(2, '0')}:${remainingSeconds.toString().padStart(2, '0')}`;\n  }, []);\n\n  return {\n    startRecording,\n    stopRecording,\n    requestPermission,\n    clearError,\n    formatDuration,\n    isRecording,\n    isSupported,\n    hasPermission,\n    duration,\n    error,\n  };\n};","metadata":{"source":"src/hooks/useAudioRecording.js","section":"","chunkIndex":3,"totalChunks":4,"timestamp":1749751085616}}],["dd6f4dad-44a9-4e74-a3cc-d7b4159b9610",{"pageContent":"### Componentes de feedback\n- **Alert**: Alertas de notificação com opções de descarte\n- **Progress Bar**: Indicadores de progresso lineares\n- **Spinner**: Spinners de carregamento em vários tamanhos\n- **Skeleton**: Placeholders de conteúdo para estados de carregamento\n- **Toast**: Mensagens de notificação temporárias\n- **Tooltip**: Informações ao passar o mouse\n\n### Componentes overlay e modal\n- **Dialog**: Diálogos modais com backdrop\n- **Drawer**: Painéis deslizantes de qualquer direção\n- **Popover**: Overlays de conteúdo posicionados\n- **Modal**: Implementações modais avançadas\n\n### Elementos interativos\n- **Button**: Botões primários, secundários, texto e ícone\n- **Button Group**: Coleções de botões agrupados\n- **Icon Button**: Botões circulares apenas com ícone\n- **Slider**: Sliders de intervalo com alças simples/duplas\n- **Rating Bar**: Componentes de classificação por estrelas\n- **Speed Dial**: Botão de ação flutuante com opções expansíveis\n\n### Componentes de layout\n- **Collapse**: Seções de conteúdo colapsáveis\n- **Accordion**: Painéis de conteúdo expansíveis\n- **Carousel**: Sliders de imagem/conteúdo\n- **Gallery**: Galerias de imagens com lightbox\n- **Stepper**: Indicadores de progresso passo a passo\n- **Footer**: Rodapés de site com várias opções de layout\n\n### Componentes Web 3.0\n- **Crypto Login**: Interfaces de conexão de carteira blockchain\n- **Crypto Card**: Cartões de exibição de criptomoedas\n- **Crypto Chart**: Visualização de dados de criptomoedas\n- **Crypto Modal**: Diálogos modais específicos para Web3\n- **Crypto Table**: Tabelas de transações blockchain\n\n## 3. Exemplos de código para cada componente\n\n### Implementação básica de Button\n```jsx\nimport { Button } from \"@material-tailwind/react\";","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":2,"totalChunks":15,"timestamp":1749751085616}}],["0f739760-7f2a-4d09-a59f-dc0b378b0863",{"pageContent":"/**\n * @fileoverview Custom hook for OpenAI integration\n */\n\nimport { useState, useCallback, useMemo } from 'react';\nimport { createOpenAIService } from '../services/openai.service';\n\n/**\n * Custom hook for OpenAI chat and transcription\n * @param {string} apiKey - OpenAI API key\n * @returns {Object} OpenAI utilities and state\n */","metadata":{"source":"src/hooks/useOpenAI.js","section":"","chunkIndex":0,"totalChunks":2,"timestamp":1749751085616}}],["31449d1d-7b33-4561-bd58-6cf03b1f1b16",{"pageContent":"/**\n * @fileoverview LRU cache implementation for embeddings\n */\n\n/**\n * LRU (Least Recently Used) cache for storing embeddings\n */","metadata":{"source":"src/utils/embeddingCache.js","section":"","chunkIndex":0,"totalChunks":3,"timestamp":1749751085616}}],["618fc63e-d607-4993-ad83-6543ea7139fa",{"pageContent":"/**\n * @fileoverview PDF processing service\n */\n\nimport * as pdfjsLib from 'pdfjs-dist';\nimport { PDF_CONFIG, ERROR_MESSAGES } from '../constants';\n\n// Configure PDF.js to use CDN worker\npdfjsLib.GlobalWorkerOptions.workerSrc = 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.31/build/pdf.worker.min.mjs';\n\n// Dynamic import for pdf-parse to avoid build issues\nlet pdfParse = null;\nconst loadPdfParse = async () => {\n  if (!pdfParse) {\n    try {\n      pdfParse = (await import('pdf-parse')).default;\n    } catch (error) {\n      console.warn('pdf-parse not available, using fallback method');\n    }\n  }\n  return pdfParse;\n};\n\n/**\n * PDF processing service class\n */","metadata":{"source":"src/services/pdf.service.js","section":"","chunkIndex":0,"totalChunks":6,"timestamp":1749751085616}}],["75577bbe-73a0-487c-a7dd-cb05884cf51c",{"pageContent":"// Limpar recursos\n  cleanup() {\n    this.embeddingCache.clear();\n    this.responseCache.clear();\n  }\n}","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":17,"totalChunks":18,"timestamp":1749751085616}}],["03bc23b0-1924-4659-8a21-a9fd6bacf3c1",{"pageContent":"/**\n * @fileoverview OpenAI API service layer\n */\n\nimport { ChatOpenAI } from '@langchain/openai';\nimport { HumanMessage, SystemMessage } from '@langchain/core/messages';\nimport { OPENAI_CONFIG, API_ENDPOINTS, ERROR_MESSAGES } from '../constants';\n\n/**\n * OpenAI service class for handling chat and transcription requests\n */","metadata":{"source":"src/services/openai.service.js","section":"","chunkIndex":0,"totalChunks":4,"timestamp":1749751085616}}],["a96fb1a8-e75c-443c-9fd6-496d2223c90b",{"pageContent":"/**\n * @fileoverview Utility functions for validation\n */\n\nimport { PDF_CONFIG } from '../constants';\n\n/**\n * Validate PDF file\n * @param {File} file - File to validate\n * @returns {{isValid: boolean, error: string|null}} Validation result\n * \n * @example\n * const result = validatePDFFile(file);\n * if (!result.isValid) console.error(result.error);\n */\nexport function validatePDFFile(file) {\n  if (!file) {\n    return { isValid: false, error: 'No file provided' };\n  }\n\n  if (file.size > PDF_CONFIG.MAX_FILE_SIZE) {\n    return { \n      isValid: false, \n      error: `File too large. Maximum size: ${formatFileSize(PDF_CONFIG.MAX_FILE_SIZE)}` \n    };\n  }\n\n  if (!PDF_CONFIG.ACCEPTED_TYPES.includes(file.type)) {\n    return { \n      isValid: false, \n      error: 'Invalid file type. Only PDF files are accepted.' \n    };\n  }\n\n  return { isValid: true, error: null };\n}\n\n// Helper function to format file size\nfunction formatFileSize(bytes) {\n  const sizes = ['Bytes', 'KB', 'MB', 'GB'];\n  if (bytes === 0) return '0 Bytes';\n  const i = Math.floor(Math.log(bytes) / Math.log(1024));\n  return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];\n}","metadata":{"source":"src/utils/validation.js","section":"","chunkIndex":0,"totalChunks":1,"timestamp":1749751085616}}],["41072492-b391-4cd6-9fbf-e21bb57bc6b2",{"pageContent":"### Customização através do ThemeProvider\n```javascript\nimport { ThemeProvider } from \"@material-tailwind/react\";\n\nconst customTheme = {\n  button: {\n    defaultProps: {\n      variant: \"filled\",\n      size: \"md\",\n      color: \"blue\",\n    },\n    styles: {\n      base: {\n        initial: {\n          verticalAlign: \"align-middle\",\n          fontFamily: \"font-sans\",\n          fontWeight: \"font-bold\",\n          textAlign: \"text-center\",\n          textTransform: \"uppercase\",\n          transition: \"transition-all\",\n          userSelect: \"select-none\",\n        },\n      },\n      variants: {\n        filled: {\n          blue: {\n            backgroud: \"bg-blue-500\",\n            color: \"text-white\",\n            shadow: \"shadow-md shadow-blue-500/10\",\n          },\n        },\n      },\n    },\n  },\n};\n\nexport default function App() {\n  return (\n    <ThemeProvider value={customTheme}>\n      {/* Your app components */}\n    </ThemeProvider>\n  );\n}\n```\n\n### Implementação de Dark Mode\n```javascript\n// tailwind.config.js\nmodule.exports = withMT({\n  darkMode: 'class', // ou 'media' para preferência automática do sistema\n  theme: {\n    extend: {\n      colors: {\n        // Definir cores para dark mode\n      },\n    },\n  },\n});\n```\n\n```javascript\n// Componente DarkModeToggle\nimport { useState, useEffect } from 'react';\nimport { Button } from \"@material-tailwind/react\";\n\nexport default function DarkModeToggle() {\n  const [darkMode, setDarkMode] = useState(false);\n\n  useEffect(() => {\n    if (darkMode) {\n      document.documentElement.classList.add('dark');\n    } else {\n      document.documentElement.classList.remove('dark');\n    }\n  }, [darkMode]);\n\n  return (\n    <div className=\"bg-white dark:bg-gray-800 min-h-screen p-8\">\n      <Button \n        onClick={() => setDarkMode(!darkMode)}\n        className=\"dark:bg-blue-600 dark:text-white\"\n      >\n        Toggle Dark Mode\n      </Button>\n    </div>\n  );\n}\n```\n\n## 6. Melhores práticas de uso\n\n### Otimização de performance","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":6,"totalChunks":15,"timestamp":1749751085616}}],["81d6419e-7e47-40fe-afc1-634c3810675f",{"pageContent":"const lines = {};\n    \n    // Group text items by Y position (line)\n    textContent.items.forEach(item => {\n      if (!item.transform || !item.str) return;\n      \n      const y = Math.round(item.transform[5]); // Y position\n      const x = Math.round(item.transform[4]); // X position\n      \n      if (!lines[y]) lines[y] = [];\n      lines[y].push({\n        x: x,\n        text: item.str,\n        width: item.width || 0\n      });\n    });\n    \n    // Sort lines by Y position (top to bottom)\n    const sortedLines = Object.keys(lines)\n      .sort((a, b) => b - a) // Descending (top to bottom)\n      .map(y => {\n        // Sort items in each line by X position (left to right)\n        const lineItems = lines[y].sort((a, b) => a.x - b.x);\n        \n        // Reconstruct line with proper spacing\n        let lineText = '';\n        let lastX = -1;\n        \n        lineItems.forEach((item, index) => {\n          if (index === 0) {\n            lineText = item.text;\n          } else {\n            // Calculate spacing based on X position difference\n            const spacing = item.x - (lastX + (lineItems[index - 1].width || 0));\n            \n            if (spacing > 10) { // Significant gap - add space\n              lineText += ' ' + item.text;\n            } else if (spacing > 2) { // Small gap - check if space needed\n              // Add space if previous text doesn't end with space and current doesn't start with punctuation\n              const needsSpace = !lineText.endsWith(' ') && \n                               !item.text.match(/^[.,;:!?]/);\n              lineText += (needsSpace ? ' ' : '') + item.text;\n            } else {\n              // No significant gap - concatenate directly\n              lineText += item.text;\n            }\n          }\n          lastX = item.x;\n        });\n        \n        return lineText.trim();\n      })\n      .filter(line => line.length > 0);\n    \n    // Join lines and clean up\n    let fullText = sortedLines.join('\\n');","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":15,"totalChunks":18,"timestamp":1749751085616}}],["4b71ed08-9585-4a77-8160-e0d26bc36dab",{"pageContent":"// Carregar índice\n    this.db = await load(data.index);\n    \n    // Restaurar cache de embeddings\n    if (data.embeddingCache) {\n      this.embeddingCache = new Map(data.embeddingCache);\n    }\n\n    this.initialized = true;\n\n    return data.metadata;\n  }\n\n  // Análise detalhada do documento\n  async analyzeDocument() {\n    if (!this.db) return null;\n\n    const allDocs = await search(this.db, {\n      term: '',\n      limit: 10000\n    });\n\n    const stats = {\n      totalChunks: allDocs.hits.length,\n      totalTokens: 0,\n      averageChunkSize: 0,\n      pagesProcessed: new Set(),\n      tokenDistribution: [],\n      importanceDistribution: [],\n      uniqueHashes: new Set()\n    };\n\n    allDocs.hits.forEach(hit => {\n      const metadata = hit.document.metadata;\n      stats.totalTokens += metadata.totalTokens;\n      stats.pagesProcessed.add(metadata.pageNumber);\n      stats.tokenDistribution.push(metadata.totalTokens);\n      stats.importanceDistribution.push(metadata.importance);\n      stats.uniqueHashes.add(metadata.hash);\n    });\n\n    stats.averageChunkSize = Math.round(stats.totalTokens / stats.totalChunks);\n    stats.pagesProcessed = stats.pagesProcessed.size;\n    stats.duplicateChunks = stats.totalChunks - stats.uniqueHashes.size;\n\n    // Calcular percentis\n    stats.tokenPercentiles = this.calculatePercentiles(stats.tokenDistribution);\n    stats.importancePercentiles = this.calculatePercentiles(stats.importanceDistribution);\n\n    return stats;\n  }\n\n  // Utilitários\n  estimateTokens(text) {\n    // Aproximação: ~3 caracteres por token em português\n    return Math.ceil(text.length / 3);\n  }\n\n  generateHash(text) {\n    // Hash simples para cache\n    let hash = 0;\n    for (let i = 0; i < text.length; i++) {\n      const char = text.charCodeAt(i);\n      hash = ((hash << 5) - hash) + char;\n      hash = hash & hash;\n    }\n    return hash.toString(36);\n  }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":14,"totalChunks":43,"timestamp":1749751085616}}],["c8323c21-53e7-4fd3-aec9-90a70c867a02",{"pageContent":"/**\n * @fileoverview Custom hook for audio recording\n */\n\nimport { useState, useCallback, useRef, useEffect } from 'react';\nimport { createAudioService } from '../services/audio.service';\n\n/**\n * Custom hook for audio recording functionality\n * @returns {Object} Audio recording utilities and state\n */","metadata":{"source":"src/hooks/useAudioRecording.js","section":"","chunkIndex":0,"totalChunks":4,"timestamp":1749751085616}}],["60923483-6718-4e99-ab22-3074969b255c",{"pageContent":"### Formulário com validação\n```jsx\nimport { Input, Button, Typography, Card } from \"@material-tailwind/react\";\n\nexport function FormExample() {\n  return (\n    <Card color=\"transparent\" shadow={false}>\n      <Typography variant=\"h4\" color=\"blue-gray\">\n        Sign Up\n      </Typography>\n      <form className=\"mt-8 mb-2 w-80 max-w-screen-lg sm:w-96\">\n        <div className=\"mb-1 flex flex-col gap-6\">\n          <Typography variant=\"h6\" color=\"blue-gray\" className=\"-mb-3\">\n            Your Name\n          </Typography>\n          <Input\n            size=\"lg\"\n            placeholder=\"name@mail.com\"\n            className=\" !border-t-blue-gray-200 focus:!border-t-gray-900\"\n            labelProps={{\n              className: \"before:content-none after:content-none\",\n            }}\n          />\n        </div>\n        <Button className=\"mt-6\" fullWidth>\n          sign up\n        </Button>\n      </form>\n    </Card>\n  );\n}\n```\n\n## 4. Configuração específica para Vite e React\n\n### Criação de projeto Vite com React\n```bash\nnpm create vite@latest my-project -- --template react\ncd my-project\nnpm install\n```\n\n### Configuração do Vite (vite.config.js)\n```javascript\nimport { defineConfig } from 'vite'\nimport react from '@vitejs/plugin-react'\n\nexport default defineConfig({\n  plugins: [react()],\n  css: {\n    postcss: {\n      plugins: [\n        require('tailwindcss'),\n        require('autoprefixer'),\n      ],\n    }\n  }\n})\n```\n\n### Configuração Tailwind específica para Vite\n```javascript\n// tailwind.config.js\nconst withMT = require(\"@material-tailwind/react/utils/withMT\");\n\nmodule.exports = withMT({\n  content: [\n    \"./index.html\",\n    \"./src/**/*.{vue,js,ts,jsx,tsx}\"\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n});\n```\n\n### Configuração para monorepo\n```javascript\nconst withMT = require(\"@material-tailwind/react/utils/withMT\");","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":4,"totalChunks":15,"timestamp":1749751085616}}],["352ae44f-bf2b-4635-b851-505a2cd9f74c",{"pageContent":"export class ConfigService {\n  static API_KEY_STORAGE = 'openai_api_key';\n  static INDEX_METADATA_STORAGE = 'rag_index_metadata';\n  \n  // API Key Management\n  static saveApiKey(apiKey) {\n    sessionStorage.setItem(this.API_KEY_STORAGE, apiKey);\n  }\n  \n  static getApiKey() {\n    return sessionStorage.getItem(this.API_KEY_STORAGE);\n  }\n  \n  static hasApiKey() {\n    return !!this.getApiKey();\n  }\n  \n  static clearApiKey() {\n    sessionStorage.removeItem(this.API_KEY_STORAGE);\n  }\n  \n  static validateApiKey(apiKey) {\n    return apiKey && apiKey.startsWith('sk-') && apiKey.length > 20;\n  }\n  \n  // Index Metadata Management\n  static saveIndexMetadata(metadata) {\n    localStorage.setItem(this.INDEX_METADATA_STORAGE, JSON.stringify(metadata));\n  }\n  \n  static getIndexMetadata() {\n    const data = localStorage.getItem(this.INDEX_METADATA_STORAGE);\n    return data ? JSON.parse(data) : null;\n  }\n  \n  static clearIndexMetadata() {\n    localStorage.removeItem(this.INDEX_METADATA_STORAGE);\n  }\n  \n  // Available Indexes\n  static getAvailableIndexes() {\n    const keys = Object.keys(localStorage).filter(key => key.startsWith('rag_index_'));\n    return keys.map(key => {\n      const data = localStorage.getItem(key);\n      try {\n        const parsed = JSON.parse(data);\n        return parsed.metadata;\n      } catch {\n        return null;\n      }\n    }).filter(Boolean);\n  }\n}","metadata":{"source":"src/services/config.service.js","section":"","chunkIndex":0,"totalChunks":1,"timestamp":1749751085616}}],["ad9f1762-eec1-4a9c-9c51-cbe1a437f773",{"pageContent":"// Gerar embedding com retry automático\n  async generateEmbeddingWithRetry(text, retries = 0) {\n    try {\n      const response = await this.openai.embeddings.create({\n        model: this.config.embeddingModel,\n        input: text,\n        dimensions: this.config.embeddingDimensions\n      });\n      \n      return response.data[0].embedding;\n    } catch (error) {\n      if (retries < this.config.maxRetries) {\n        await new Promise(resolve => setTimeout(resolve, this.config.retryDelay * (retries + 1)));\n        return this.generateEmbeddingWithRetry(text, retries + 1);\n      }\n      throw error;\n    }\n  }\n\n  // Busca semântica otimizada\n  async searchSemantic(query, options = {}) {\n    if (!this.initialized || !this.db) {\n      throw new Error('Sistema não inicializado');\n    }\n\n    const {\n      limit = this.config.topK,\n      threshold = this.config.similarityThreshold,\n      useReranking = true,\n      includeContext = true\n    } = options;\n\n    // Gerar embedding da query com cache\n    const queryHash = this.generateHash(query);\n    let queryEmbedding;\n    \n    if (this.embeddingCache.has(queryHash)) {\n      queryEmbedding = this.embeddingCache.get(queryHash);\n    } else {\n      queryEmbedding = await this.generateEmbeddingWithRetry(query);\n      this.embeddingCache.set(queryHash, queryEmbedding);\n    }\n\n    // Busca vetorial\n    const results = await search(this.db, {\n      mode: 'vector',\n      vector: {\n        value: queryEmbedding,\n        property: 'embedding'\n      },\n      limit: limit * 2,\n      threshold: threshold,\n      includeVectors: false\n    });\n\n    let relevantDocs = results.hits.map(hit => ({\n      text: hit.document.text,\n      score: hit.score,\n      metadata: hit.document.metadata\n    }));\n\n    // Reranking com GPT\n    if (useReranking && relevantDocs.length > 0) {\n      relevantDocs = await this.rerankDocuments(query, relevantDocs, limit);\n    }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":9,"totalChunks":43,"timestamp":1749751085616}}],["7220c553-87b0-4564-ab43-5e02f38986fa",{"pageContent":"// Construir contexto otimizado com metadados\n  buildOptimizedContext(documents, maxTokens) {\n    let context = '';\n    let currentTokens = 0;\n\n    // Ordenar por score * importance\n    const sorted = documents.sort((a, b) => \n      (b.score * b.metadata.importance) - (a.score * a.metadata.importance)\n    );\n\n    for (const doc of sorted) {\n      const docTokens = doc.metadata.totalTokens;\n      \n      if (currentTokens + docTokens > maxTokens) {\n        break;\n      }\n\n      const pageInfo = ` [Página ${doc.metadata.pageNumber}]`;\n      const relevanceInfo = ` [Relevância: ${(doc.score * 100).toFixed(1)}%]`;\n      \n      context += `${doc.text}${pageInfo}${relevanceInfo}\\n\\n---\\n\\n`;\n      currentTokens += docTokens;\n    }\n\n    return context.trim();\n  }\n\n  // Exportar índice completo com metadados\n  async exportIndex() {\n    const indexData = await save(this.db);\n    const stats = await this.analyzeDocument();\n    \n    const exportData = {\n      version: '2.0',\n      metadata: {\n        model: this.config.embeddingModel,\n        dimensions: this.config.embeddingDimensions,\n        created: new Date().toISOString(),\n        stats: stats,\n        config: this.config\n      },\n      index: indexData,\n      embeddingCache: Array.from(this.embeddingCache.entries()).slice(0, 100) // Limitar tamanho\n    };\n\n    const blob = new Blob([JSON.stringify(exportData)], { type: 'application/json' });\n    return blob;\n  }\n\n  // Importar índice com validação\n  async importIndex(file) {\n    const text = await file.text();\n    const data = JSON.parse(text);\n\n    // Validações\n    if (!data.version || data.version !== '2.0') {\n      throw new Error('Versão de índice incompatível');\n    }\n\n    if (data.metadata?.dimensions !== this.config.embeddingDimensions) {\n      throw new Error(`Índice usa ${data.metadata.dimensions} dimensões, mas o sistema está configurado para ${this.config.embeddingDimensions}`);\n    }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":11,"totalChunks":18,"timestamp":1749751085616}}],["d03996cb-2d1f-4767-ac4c-af252a97eba9",{"pageContent":"// Construir contexto otimizado com metadados\n  buildOptimizedContext(documents, maxTokens) {\n    let context = '';\n    let currentTokens = 0;\n\n    // Ordenar por score * importance\n    const sorted = documents.sort((a, b) => \n      (b.score * b.metadata.importance) - (a.score * a.metadata.importance)\n    );\n\n    for (const doc of sorted) {\n      const docTokens = doc.metadata.totalTokens;\n      \n      if (currentTokens + docTokens > maxTokens) {\n        break;\n      }\n\n      const pageInfo = ` [Página ${doc.metadata.pageNumber}]`;\n      const relevanceInfo = ` [Relevância: ${(doc.score * 100).toFixed(1)}%]`;\n      \n      context += `${doc.text}${pageInfo}${relevanceInfo}\\n\\n---\\n\\n`;\n      currentTokens += docTokens;\n    }\n\n    return context.trim();\n  }\n\n  // Exportar índice completo com metadados\n  async exportIndex() {\n    const indexData = await save(this.db);\n    const stats = await this.analyzeDocument();\n    \n    const exportData = {\n      version: '2.0',\n      metadata: {\n        model: this.config.embeddingModel,\n        dimensions: this.config.embeddingDimensions,\n        created: new Date().toISOString(),\n        stats: stats,\n        config: this.config\n      },\n      index: indexData,\n      embeddingCache: Array.from(this.embeddingCache.entries()).slice(0, 100) // Limitar tamanho\n    };\n\n    const blob = new Blob([JSON.stringify(exportData)], { type: 'application/json' });\n    return blob;\n  }\n\n  // Importar índice com validação\n  async importIndex(file) {\n    const text = await file.text();\n    const data = JSON.parse(text);\n\n    // Validações\n    if (!data.version || data.version !== '2.0') {\n      throw new Error('Versão de índice incompatível');\n    }\n\n    if (data.metadata?.dimensions !== this.config.embeddingDimensions) {\n      throw new Error(`Índice usa ${data.metadata.dimensions} dimensões, mas o sistema está configurado para ${this.config.embeddingDimensions}`);\n    }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":13,"totalChunks":43,"timestamp":1749751085616}}],["18524e5a-0fbd-4648-9454-e8878d8cd9e9",{"pageContent":"// UI do histórico\n  const HistoryDialog = () => (\n    <Dialog open={showSavedIndexes} handler={() => setShowSavedIndexes(false)} size=\"lg\">\n      <DialogHeader>\n        <div className=\"flex items-center gap-2\">\n          <ClockIcon className=\"h-6 w-6\" />\n          Histórico de Consultas\n        </div>\n      </DialogHeader>\n      <DialogBody className=\"overflow-y-auto max-h-[60vh]\">\n        {queryHistory.length > 0 ? (\n          <List>\n            {queryHistory.map((item, idx) => (\n              <ListItem \n                key={idx} \n                className=\"p-3 hover:bg-gray-50 cursor-pointer\"\n                onClick={() => {\n                  loadFromHistory(item);\n                  setShowSavedIndexes(false);\n                }}\n              >\n                <div className=\"w-full\">\n                  <div className=\"flex justify-between items-start mb-1\">\n                    <Typography variant=\"small\" className=\"font-medium\">\n                      {item.query}\n                    </Typography>\n                    <Chip\n                      value={`${item.sources} fontes`}\n                      size=\"sm\"\n                      color=\"blue\"\n                    />\n                  </div>\n                  <Typography variant=\"small\" color=\"gray\">\n                    {item.answer}\n                  </Typography>\n                  <Typography variant=\"small\" color=\"blue-gray\" className=\"mt-1\">\n                    {new Date(item.timestamp).toLocaleString()}\n                  </Typography>\n                </div>\n              </ListItem>\n            ))}\n          </List>\n        ) : (\n          <Typography color=\"gray\" className=\"text-center py-8\">\n            Nenhuma consulta realizada ainda\n          </Typography>\n        )}\n      </DialogBody>\n      <DialogFooter>\n        <Button\n          variant=\"text\"\n          color=\"red\"\n          onClick={() => {\n            if (confirm('Deseja limpar todo o histórico?')) {\n              setQueryHistory([]);","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":31,"totalChunks":43,"timestamp":1749751085616}}],["b5cd5118-baeb-461e-8865-88c13f54bb57",{"pageContent":"export const createPDFService = () => {\n  return new PDFService();\n};","metadata":{"source":"src/services/pdf.service.js","section":"","chunkIndex":5,"totalChunks":6,"timestamp":1749751085616}}],["429756af-ce7e-44a5-a268-336b32f49b7c",{"pageContent":"### Estrutura de Arquivos Necessária\n```\nsrc/\n├── services/\n│   ├── config.service.js         # Gerenciamento de API key\n│   └── highQualityRAG.service.js # Lógica principal do RAG\n├── components/\n│   ├── ApiKeySetup.jsx          # Tela de configuração inicial\n│   └── HighQualityRAG.jsx       # Interface principal do sistema\n├── workers/\n│   └── pdf.worker.js            # Web Worker para processamento\n└── App.jsx                      # Componente raiz\n```\n\n### Dependências Necessárias\n```json\n{\n  \"dependencies\": {\n    \"openai\": \"^4.0.0\",\n    \"@orama/orama\": \"^2.0.0\",\n    \"langchain\": \"^0.1.0\",\n    \"pdfjs-dist\": \"^3.0.0\",\n    \"@material-tailwind/react\": \"^2.0.0\",\n    \"@heroicons/react\": \"^2.0.0\"\n  }\n}\n```\n\n### Instruções de Implementação\n1. Instale as dependências: `npm install openai @orama/orama langchain pdfjs-dist`\n2. Copie os arquivos de serviço e componentes para as pastas correspondentes\n3. Configure o Web Worker para processamento assíncrono\n4. Ajuste as configurações de chunking e embedding conforme necessário\n5. Teste com PDFs pequenos antes de processar arquivos grandes\n\n---\n\n## 1. Serviço de Configuração e Gerenciamento de API Key","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":1,"totalChunks":43,"timestamp":1749751085616}}],["cf5e84c2-fe02-4b83-89bf-5aeae8789292",{"pageContent":"// Carregar índice\n    this.db = await load(data.index);\n    \n    // Restaurar cache de embeddings\n    if (data.embeddingCache) {\n      this.embeddingCache = new Map(data.embeddingCache);\n    }\n\n    this.initialized = true;\n\n    return data.metadata;\n  }\n\n  // Análise detalhada do documento\n  async analyzeDocument() {\n    if (!this.db) return null;\n\n    try {\n      const allDocs = await search(this.db, {\n        term: '*',\n        limit: 10000\n      });\n\n      const stats = {\n        totalChunks: allDocs.hits.length,\n        totalTokens: 0,\n        averageChunkSize: 0,\n        pagesProcessed: new Set(),\n        tokenDistribution: [],\n        importanceDistribution: [],\n        uniqueHashes: new Set()\n      };\n\n      allDocs.hits.forEach(hit => {\n        const doc = hit.document;\n        stats.totalTokens += doc.totalTokens || 0;\n        stats.pagesProcessed.add(doc.pageNumber);\n        stats.tokenDistribution.push(doc.totalTokens || 0);\n        stats.importanceDistribution.push(doc.importance || 1);\n        stats.uniqueHashes.add(doc.hash);\n      });\n\n      stats.averageChunkSize = stats.totalChunks > 0 ? Math.round(stats.totalTokens / stats.totalChunks) : 0;\n      stats.pagesProcessed = stats.pagesProcessed.size;\n      stats.duplicateChunks = stats.totalChunks - stats.uniqueHashes.size;\n\n      // Calcular percentis\n      stats.tokenPercentiles = this.calculatePercentiles(stats.tokenDistribution);\n      stats.importancePercentiles = this.calculatePercentiles(stats.importanceDistribution);\n\n      return stats;\n    } catch (error) {\n      console.error('Error analyzing document:', error);\n      return null;\n    }\n  }\n\n  // Utilitários\n  estimateTokens(text) {\n    if (!text || typeof text !== 'string') {\n      return 0;\n    }\n    // Aproximação: ~3 caracteres por token em português\n    return Math.ceil(text.length / 3);\n  }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":12,"totalChunks":18,"timestamp":1749751085616}}],["6dd0c2d0-ec3c-4178-8216-08f800c7862f",{"pageContent":"onProgress?.({\n            phase: 'extraction',\n            current: data.progress.current,\n            total: data.progress.total,\n            percentage: data.progress.percentage * 0.4, // 40% para extração\n            message: `Extraindo texto: ${data.progress.current}/${data.progress.total} páginas`\n          });\n        } \n        else if (type === 'processing-complete') {\n          // Iniciar fase de embeddings\n          this.pdfWorker.removeEventListener('message', handleWorkerMessage);\n          \n          try {\n            await this.generateAndStoreEmbeddings(allChunks, file.name, onProgress);\n            \n            const processingTime = Date.now() - startTime;\n            const result = {\n              success: true,\n              documentName: file.name,\n              totalPages: processedPages,\n              totalChunks: allChunks.length,\n              processingTime: processingTime,\n              estimatedCost: this.estimateCost(allChunks.length)\n            };\n            \n            resolve(result);\n          } catch (err) {\n            reject(err);\n          }\n        }\n        else if (type === 'error') {\n          this.pdfWorker.removeEventListener('message', handleWorkerMessage);\n          reject(new Error(error));\n        }\n      };\n\n      this.pdfWorker.addEventListener('message', handleWorkerMessage);\n      \n      // Iniciar processamento\n      this.pdfWorker.postMessage({\n        type: 'process-pdf',\n        data: { arrayBuffer, chunkSize: this.config.chunkSize }\n      });\n    });\n  }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":7,"totalChunks":43,"timestamp":1749751085616}}],["aaccf6f1-e1e3-4976-a47c-8273cc7e073c",{"pageContent":"this.audioChunks = [];\n      this.onDataAvailable = onDataAvailable;\n      this.onStop = onStop;\n\n      this.mediaRecorder.ondataavailable = (event) => {\n        if (event.data.size > 0) {\n          this.audioChunks.push(event.data);\n          if (this.onDataAvailable) {\n            this.onDataAvailable(event.data);\n          }\n        }\n      };\n\n      this.mediaRecorder.onstop = () => {\n        const audioBlob = new Blob(this.audioChunks, { type: AUDIO_CONFIG.AUDIO_TYPE });\n        if (this.onStop) {\n          this.onStop(audioBlob);\n        }\n        this.cleanup();\n      };\n\n      this.mediaRecorder.onerror = (event) => {\n        console.error('MediaRecorder error:', event.error);\n        if (onError) {\n          onError(event.error);\n        } else {\n          throw new Error(ERROR_MESSAGES.MICROPHONE_ACCESS);\n        }\n      };\n\n      this.mediaRecorder.start();\n\n      // Auto-stop after max recording time\n      setTimeout(() => {\n        if (this.isRecording()) {\n          this.stopRecording();\n        }\n      }, AUDIO_CONFIG.MAX_RECORDING_TIME);\n\n    } catch (error) {\n      console.error('Failed to start recording:', error);\n      this.cleanup();\n      throw new Error(ERROR_MESSAGES.MICROPHONE_ACCESS);\n    }\n  }\n\n  /**\n   * Stop audio recording\n   * @returns {void}\n   */\n  stopRecording() {\n    if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {\n      this.mediaRecorder.stop();\n    }\n  }\n\n  /**\n   * Check if currently recording\n   * @returns {boolean} Whether recording is active\n   */\n  isRecording() {\n    return this.mediaRecorder && this.mediaRecorder.state === 'recording';\n  }\n\n  /**\n   * Clean up resources\n   * @private\n   */\n  cleanup() {\n    if (this.stream) {\n      this.stream.getTracks().forEach(track => track.stop());\n      this.stream = null;\n    }\n    this.mediaRecorder = null;\n    this.audioChunks = [];\n  }","metadata":{"source":"src/services/audio.service.js","section":"","chunkIndex":2,"totalChunks":5,"timestamp":1749751085616}}],["77baae0f-aeb4-43f0-b378-762b65847762",{"pageContent":"{/* Upload e Gerenciamento */}\n      <Card>\n        <CardBody>\n          <div className=\"flex items-center justify-between mb-4\">\n            <Typography variant=\"h5\">Gerenciamento de Documentos</Typography>\n            {uploadedFile && (\n              <Chip \n                value={uploadedFile.name} \n                color=\"green\" \n                icon={<DocumentIcon className=\"h-4 w-4\" />}\n                onClose={() => {\n                  if (confirm('Deseja remover este documento?')) {\n                    handleReset();\n                  }\n                }}\n              />\n            )}\n          </div>\n          \n          <div className=\"grid grid-cols-1 md:grid-cols-4 gap-3\">\n            {/* Upload PDF */}\n            <input\n              ref={fileInputRef}\n              type=\"file\"\n              accept=\".pdf\"\n              onChange={handleFileUpload}\n              disabled={isProcessing}\n              className=\"hidden\"\n            />\n            <Button\n              onClick={() => fileInputRef.current?.click()}\n              disabled={isProcessing}\n              color=\"blue\"\n              className=\"flex items-center justify-center gap-2\"\n            >\n              <DocumentIcon className=\"h-5 w-5\" />\n              Novo PDF\n            </Button>\n\n            {/* Importar Índice */}\n            <input\n              ref={indexInputRef}\n              type=\"file\"\n              accept=\".json\"\n              onChange={handleImport}\n              disabled={isProcessing}\n              className=\"hidden\"\n            />\n            <Button\n              onClick={() => indexInputRef.current?.click()}\n              variant=\"outlined\"\n              disabled={isProcessing}\n              className=\"flex items-center justify-center gap-2\"\n            >\n              <ArrowUpTrayIcon className=\"h-5 w-5\" />\n              Importar\n            </Button>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":34,"totalChunks":43,"timestamp":1749751085616}}],["9e84913b-b087-41d1-8b13-f0962c960515",{"pageContent":"this.initialized = true;\n    } catch (error) {\n      this.initialized = false;\n      throw new Error(`Falha na inicialização: ${error.message}`);\n    }\n  }\n\n  // Processar PDF\n  async processPDF(file, onProgress) {\n    if (!this.initialized) {\n      await this.initialize();\n    }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":2,"totalChunks":18,"timestamp":1749751085616}}],["bc0b1b5f-b7b2-4943-9fcc-c0ec569b91cb",{"pageContent":"setUploadedFile(file);\n    setIsProcessing(true);\n    setError(null);\n    setProgress(null);\n    setResponse(null);\n\n    try {\n      const result = await ragService.processPDF(file, (prog) => {\n        setProgress(prog);\n      });\n\n      // Obter estatísticas\n      const docStats = await ragService.analyzeDocument();\n      setStats(docStats);\n\n      // Salvar metadados\n      const metadata = {\n        fileName: file.name,\n        processedAt: new Date().toISOString(),\n        stats: docStats,\n        result: result\n      };\n      \n      ConfigService.saveIndexMetadata(metadata);\n\n      setProgress(null);\n      \n      // Notificação de sucesso\n      const notification = `✅ PDF processado com sucesso!\\n\\n` +\n        `📄 ${result.totalPages} páginas\\n` +\n        `📦 ${result.totalChunks} chunks\\n` +\n        `⏱️ ${(result.processingTime / 1000).toFixed(1)}s\\n` +\n        `💰 Custo estimado: $${result.estimatedCost.total.toFixed(4)}`;\n      \n      alert(notification);\n      \n    } catch (err) {\n      setError(err.message);\n    } finally {\n      setIsProcessing(false);\n    }\n  }, [ragService]);\n\n  // Fazer pergunta\n  const handleQuery = useCallback(async () => {\n    if (!query.trim()) return;\n\n    setIsProcessing(true);\n    setError(null);\n    setResponse(null);\n    setIsStreaming(true);\n\n    try {\n      const startTime = Date.now();\n      const result = await ragService.generateResponse(query, {\n        streamResponse: true\n      });\n\n      // Processar stream\n      let fullResponse = '';\n      setResponse({ \n        answer: '', \n        sources: result.sources, \n        isStreaming: true,\n        startTime: startTime \n      });\n\n      for await (const chunk of result.stream) {\n        const content = chunk.choices[0]?.delta?.content || '';\n        fullResponse += content;\n        \n        setResponse(prev => ({\n          ...prev,\n          answer: fullResponse\n        }));\n      }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":22,"totalChunks":43,"timestamp":1749751085616}}],["505e6d68-7fe0-467b-bd28-93c3b9acc9a8",{"pageContent":"// Verificar se tem acesso aos modelos necessários\n      const models = await response.json();\n      const hasEmbedding = models.data.some(m => m.id.includes('embedding'));\n      const hasGPT4 = models.data.some(m => m.id.includes('gpt-4'));\n\n      if (!hasEmbedding || !hasGPT4) {\n        throw new Error('Sua API key precisa ter acesso aos modelos de embedding e GPT-4');\n      }\n\n      // Salvar e continuar\n      ConfigService.saveApiKey(apiKey);\n      onComplete();\n    } catch (err) {\n      setError(err.message);\n    } finally {\n      setIsValidating(false);\n    }\n  };\n\n  const loadExistingIndex = () => {\n    // Permitir uso sem API key se há índices salvos\n    ConfigService.saveApiKey('sk-dummy-for-search-only');\n    onComplete();\n  };\n\n  return (\n    <div className=\"min-h-screen flex items-center justify-center bg-gray-50 p-4\">\n      <div className=\"w-full max-w-lg space-y-4\">\n        {/* Card Principal */}\n        <Card>\n          <CardBody className=\"space-y-6\">\n            <div className=\"text-center\">\n              <KeyIcon className=\"h-12 w-12 mx-auto text-blue-500 mb-4\" />\n              <Typography variant=\"h4\" color=\"blue-gray\">\n                Sistema RAG com OpenAI\n              </Typography>\n              <Typography color=\"gray\" className=\"mt-2\">\n                Para processar novos documentos, você precisa fornecer sua API key da OpenAI\n              </Typography>\n            </div>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":17,"totalChunks":43,"timestamp":1749751085616}}],["c545f59b-6ffc-4841-bfb9-8955dcf42d83",{"pageContent":"/**\n * @fileoverview Custom hook for PDF processing\n */\n\nimport { useState, useCallback, useMemo } from 'react';\nimport { createPDFService } from '../services/pdf.service';\n\n/**\n * Custom hook for PDF processing\n * @returns {Object} PDF utilities and state\n */","metadata":{"source":"src/hooks/usePDF.js","section":"","chunkIndex":0,"totalChunks":3,"timestamp":1749751085616}}],["251ab9ab-3527-44f0-a7be-ea7748e439cb",{"pageContent":"/**\n   * Transcribe audio using OpenAI Whisper\n   * @param {Blob} audioBlob - Audio data to transcribe\n   * @param {string} language - Language code (default: 'pt')\n   * @returns {Promise<string>} Transcribed text\n   * @throws {Error} When transcription fails\n   */\n  async transcribeAudio(audioBlob, language = 'pt') {\n    try {\n      const formData = new FormData();\n      formData.append('file', audioBlob, 'audio.webm');\n      formData.append('model', OPENAI_CONFIG.WHISPER_MODEL);\n      formData.append('language', language);\n\n      const response = await fetch(API_ENDPOINTS.OPENAI_TRANSCRIPTIONS, {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${this.apiKey}`,\n        },\n        body: formData\n      });\n\n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n      }\n\n      const data = await response.json();\n      return data.text || '';\n    } catch (error) {\n      console.error('Transcription failed:', error);\n      throw new Error(ERROR_MESSAGES.TRANSCRIPTION_FAILED);\n    }\n  }\n}\n\n/**\n * Create OpenAI service instance\n * @param {string} apiKey - OpenAI API key\n * @returns {OpenAIService} Service instance\n */","metadata":{"source":"src/services/openai.service.js","section":"","chunkIndex":2,"totalChunks":4,"timestamp":1749751085616}}],["82d36ffe-dd58-47a5-949b-f19c97d43b71",{"pageContent":"allChunks.push({\n                text: chunkText,\n                metadata: {\n                  pageNumber: pageNum,\n                  chunkIndex: index,\n                  source: file.name,\n                  totalTokens: this.estimateTokens(chunkText),\n                  importance: this.calculateImportance(chunkText, pageNum, totalPages),\n                  hash: hash\n                }\n              });\n            });\n          }\n          \n          page.cleanup();\n        }\n        \n        onProgress?.({\n          phase: 'extraction',\n          current: endPage,\n          total: totalPages,\n          percentage: (endPage / totalPages) * 40,\n          message: `Extraindo texto: ${endPage}/${totalPages} páginas`\n        });\n        \n        // Yield to main thread\n        await new Promise(resolve => setTimeout(resolve, 0));\n      }\n      \n      // Log do texto completo extraído\n      const fullText = allChunks.map(chunk => chunk.text).join('\\n\\n');\n      console.log('📄 TEXTO COMPLETO DO PDF EXTRAÍDO:');\n      console.log('=====================================');\n      console.log(fullText);\n      console.log('=====================================');\n      console.log(`Total de chunks: ${allChunks.length}`);\n      \n      // Process embeddings\n      await this.generateAndStoreEmbeddings(allChunks, file.name, onProgress);\n      \n      const processingTime = Date.now() - startTime;\n      return {\n        success: true,\n        documentName: file.name,\n        totalPages: totalPages,\n        totalChunks: allChunks.length,\n        processingTime: processingTime,\n        estimatedCost: this.estimateCost(allChunks.length)\n      };\n    } catch (error) {\n      throw new Error(`Erro no processamento do PDF: ${error.message}`);\n    }\n  }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":4,"totalChunks":18,"timestamp":1749751085616}}],["ece906dd-6296-4f35-bcb7-9d5f150560f6",{"pageContent":"// Configurar worker do PDF.js\npdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;\n\nself.onmessage = async function(event) {\n  const { type, data } = event.data;\n  \n  if (type === 'process-pdf') {\n    try {\n      const { arrayBuffer, chunkSize } = data;\n      const pdf = await pdfjsLib.getDocument({ data: arrayBuffer }).promise;\n      const totalPages = pdf.numPages;\n      const chunks = [];\n      \n      // Processar em batches de páginas\n      const PAGES_PER_BATCH = 5;\n      \n      for (let startPage = 1; startPage <= totalPages; startPage += PAGES_PER_BATCH) {\n        const endPage = Math.min(startPage + PAGES_PER_BATCH - 1, totalPages);\n        const batchChunks = [];\n        \n        for (let pageNum = startPage; pageNum <= endPage; pageNum++) {\n          const page = await pdf.getPage(pageNum);\n          const textContent = await page.getTextContent();\n          \n          // Reconstruir texto preservando estrutura\n          const pageText = reconstructPageText(textContent);\n          \n          batchChunks.push({\n            pageNumber: pageNum,\n            text: pageText,\n            estimatedTokens: Math.ceil(pageText.length / 3)\n          });\n          \n          page.cleanup();\n        }\n        \n        // Enviar batch processado\n        self.postMessage({\n          type: 'batch-complete',\n          data: {\n            chunks: batchChunks,\n            progress: {\n              current: endPage,\n              total: totalPages,\n              percentage: (endPage / totalPages) * 100\n            }\n          }\n        });\n        \n        // Pequena pausa para não sobrecarregar\n        await new Promise(resolve => setTimeout(resolve, 10));\n      }\n      \n      self.postMessage({ type: 'processing-complete' });\n      \n    } catch (error) {\n      self.postMessage({\n        type: 'error',\n        error: error.message\n      });\n    }\n  }\n};","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":3,"totalChunks":43,"timestamp":1749751085616}}],["d2ed30b7-5880-4fe4-a202-a4f877e0eee6",{"pageContent":"export class AudioService {\n  constructor() {\n    this.mediaRecorder = null;\n    this.stream = null;\n    this.audioChunks = [];\n    this.onDataAvailable = null;\n    this.onStop = null;\n  }\n\n  /**\n   * Check if audio recording is supported\n   * @returns {boolean} Whether audio recording is supported\n   */\n  isSupported() {\n    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia && window.MediaRecorder);\n  }\n\n  /**\n   * Request microphone permission\n   * @returns {Promise<boolean>} Whether permission was granted\n   */\n  async requestPermission() {\n    if (!this.isSupported()) {\n      throw new Error('Audio recording is not supported in this browser');\n    }\n\n    try {\n      const stream = await navigator.mediaDevices.getUserMedia(AUDIO_CONFIG.CONSTRAINTS);\n      // Stop the stream immediately after getting permission\n      stream.getTracks().forEach(track => track.stop());\n      return true;\n    } catch (error) {\n      console.error('Microphone permission denied:', error);\n      return false;\n    }\n  }\n\n  /**\n   * Start audio recording\n   * @param {Object} options - Recording options\n   * @param {function} options.onDataAvailable - Callback for data chunks\n   * @param {function} options.onStop - Callback when recording stops\n   * @param {function} options.onError - Callback for errors\n   * @returns {Promise<void>}\n   * @throws {Error} When recording fails to start\n   */\n  async startRecording({ onDataAvailable, onStop, onError } = {}) {\n    if (!this.isSupported()) {\n      throw new Error('Audio recording is not supported');\n    }\n\n    try {\n      this.stream = await navigator.mediaDevices.getUserMedia(AUDIO_CONFIG.CONSTRAINTS);\n      this.mediaRecorder = new MediaRecorder(this.stream, {\n        mimeType: AUDIO_CONFIG.AUDIO_TYPE\n      });\n\n      this.audioChunks = [];\n      this.onDataAvailable = onDataAvailable;\n      this.onStop = onStop;","metadata":{"source":"src/services/audio.service.js","section":"","chunkIndex":1,"totalChunks":5,"timestamp":1749751085616}}],["1e3e6870-85a0-4078-9b66-988fbe623272",{"pageContent":"generateHash(text) {\n    if (!text || typeof text !== 'string') {\n      return 'invalid-hash';\n    }\n    // Hash simples para cache\n    let hash = 0;\n    for (let i = 0; i < text.length; i++) {\n      const char = text.charCodeAt(i);\n      hash = ((hash << 5) - hash) + char;\n      hash = hash & hash;\n    }\n    return hash.toString(36);\n  }\n\n  calculateImportance(text, pageNum, totalPages) {\n    // Validação de entrada\n    if (!text || typeof text !== 'string') {\n      console.warn('calculateImportance received invalid text:', typeof text, text);\n      return 1.0; // Retorna importância padrão\n    }\n    \n    const textStr = String(text).trim();\n    if (!textStr) {\n      return 0.5;\n    }\n    \n    let score = 1.0;\n    \n    try {\n      // Primeiras páginas (resumo, introdução)\n      if (pageNum <= 3) score *= 1.3;\n      \n      // Últimas páginas (conclusão)\n      if (pageNum >= totalPages - 2) score *= 1.2;\n      \n      // Títulos e subtítulos (heurística)\n      if (textStr.match(/^[A-Z\\s\\d.]{3,50}$/m)) score *= 1.4;\n      \n      // Parágrafos com números e dados\n      const numbers = textStr.match(/\\d+\\.?\\d*/g) || [];\n      if (numbers.length > 5) score *= 1.2;\n      \n      // Listas e enumerações\n      if (textStr.match(/^\\s*[\\d\\-*•]\\s+/m)) score *= 1.1;\n      \n      // Chunks mais longos (mais contexto)\n      if (textStr.length > 600) score *= 1.1;\n    } catch (error) {\n      console.error('Error in calculateImportance:', error);\n      return 1.0;\n    }\n    \n    return Math.min(score, 2.0);\n  }\n\n  calculatePercentiles(values) {\n    if (!values.length) return {};\n    \n    const sorted = values.sort((a, b) => a - b);\n    const percentiles = {};\n    \n    [25, 50, 75, 90, 95].forEach(p => {\n      const index = Math.floor((p / 100) * sorted.length);\n      percentiles[`p${p}`] = sorted[index];\n    });\n    \n    return percentiles;\n  }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":13,"totalChunks":18,"timestamp":1749751085616}}],["32dc6479-c0e3-4474-909a-189b774a23df",{"pageContent":"estimateCost(totalChunks) {\n    const tokensPerChunk = this.config.chunkSize;\n    const totalTokens = totalChunks * tokensPerChunk;\n    \n    return {\n      embedding: {\n        model: this.config.embeddingModel,\n        tokens: totalTokens,\n        cost: (totalTokens / 1000) * 0.00013 // $0.00013 per 1K tokens\n      },\n      total: (totalTokens / 1000) * 0.00013\n    };\n  }\n\n  // Reconstituir texto da página com melhor qualidade\n  reconstructPageText(textContent) {\n    if (!textContent || !textContent.items || !Array.isArray(textContent.items)) {\n      return '';\n    }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":14,"totalChunks":18,"timestamp":1749751085616}}],["100a0a06-1404-4144-8cdd-893e48f67e86",{"pageContent":"export class HighQualityRAGService {\n  constructor() {\n    this.openai = null;\n    this.db = null;\n    this.initialized = false;\n    \n    // Configurações otimizadas para qualidade máxima\n    this.config = {\n      embeddingModel: 'text-embedding-3-large',\n      embeddingDimensions: 3072,\n      chatModel: 'gpt-4o-mini', // Updated to current available model\n      chunkSize: 800,\n      chunkOverlap: 200,\n      temperature: 0.2,\n      maxTokens: 4000,\n      topK: 10,\n      similarityThreshold: 0.7,\n      batchSize: 5,\n      maxRetries: 3,\n      retryDelay: 1000\n    };\n\n    this.splitter = new SimpleTextSplitter({\n      chunkSize: this.config.chunkSize,\n      chunkOverlap: this.config.chunkOverlap,\n      separators: [\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \": \", \" \"]\n    });\n    \n    // Cache de embeddings para evitar reprocessamento\n    this.embeddingCache = new Map();\n  }\n\n  async initialize() {\n    try {\n      const apiKey = ConfigService.getApiKey();\n      if (!apiKey) {\n        throw new Error('API key não configurada');\n      }\n\n      // Test API key before proceeding\n      this.openai = new OpenAI({\n        apiKey: apiKey,\n        dangerouslyAllowBrowser: true\n      });\n\n      // Test OpenAI connection\n      try {\n        await this.openai.models.list();\n      } catch (apiError) {\n        throw new Error(`Erro na API OpenAI: ${apiError.message}`);\n      }\n\n      // Criar banco vetorial otimizado\n      this.db = await create({\n        schema: {\n          id: 'string',\n          text: 'string',\n          embedding: `vector[${this.config.embeddingDimensions}]`,\n          pageNumber: 'number',\n          source: 'string', \n          chunkIndex: 'number',\n          totalTokens: 'number',\n          importance: 'number',\n          hash: 'string'\n        }\n      });\n\n      this.initialized = true;\n    } catch (error) {\n      this.initialized = false;\n      throw new Error(`Falha na inicialização: ${error.message}`);\n    }\n  }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":1,"totalChunks":18,"timestamp":1749751085616}}],["1cc12c1a-585e-4b7e-b975-2aba5dfc3f61",{"pageContent":"### Layout de Landing Page\n```jsx\nexport function LandingPageLayout() {\n  return (\n    <>\n      {/* Hero Section */}\n      <div className=\"relative flex h-screen content-center items-center justify-center pt-16 pb-32\">\n        <div className=\"absolute top-0 h-full w-full bg-[url('/img/background-1.jpg')] bg-cover bg-center\" />\n        <div className=\"absolute top-0 h-full w-full bg-black/75 bg-cover bg-center\" />\n        <div className=\"max-w-8xl container relative mx-auto\">\n          <div className=\"flex flex-wrap items-center\">\n            <div className=\"ml-auto mr-auto w-full px-4 text-center lg:w-8/12\">\n              <Typography variant=\"h1\" color=\"white\" className=\"mb-6 font-black\">\n                Your story starts with us.\n              </Typography>\n              <Typography variant=\"lead\" color=\"white\" className=\"opacity-80\">\n                This is a simple example of a Landing Page you can build using\n                Material Tailwind. It features multiple CSS components\n              </Typography>\n            </div>\n          </div>\n        </div>\n      </div>\n\n      {/* Features Section */}\n      <section className=\"-mt-32 bg-white px-4 pb-20 pt-4\">\n        <div className=\"container mx-auto\">\n          <div className=\"grid grid-cols-1 gap-6 md:grid-cols-2 lg:grid-cols-3\">\n            {/* Feature cards */}\n          </div>\n        </div>\n      </section>\n    </>\n  );\n}\n```\n\n## 10. Troubleshooting comum e soluções\n\n### Problema 1: Erros de configuração TypeScript\n\n**Solução para conflito de versão @types/react**\n```bash\n# Deletar node_modules e package-lock.json\nrm -rf node_modules package-lock.json\n\n# Downgrade da versão @types/react no package.json\n\"@types/react\": \"18.2.42\"  # Nota: Sem o símbolo ^\n\nnpm install\n```\n\n**Resolução de módulo em tsconfig.json**\n```json\n{\n  \"compilerOptions\": {\n    \"moduleResolution\": \"node\"\n  }\n}\n```\n\n### Problema 2: Conflitos com Tailwind CSS","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":12,"totalChunks":15,"timestamp":1749751085616}}],["c5cb191e-7c2d-4cc8-83b3-125e471000ad",{"pageContent":"async initialize() {\n    const apiKey = ConfigService.getApiKey();\n    if (!apiKey) {\n      throw new Error('API key não configurada');\n    }\n\n    this.openai = new OpenAI({\n      apiKey: apiKey,\n      dangerouslyAllowBrowser: true\n    });\n\n    // Criar banco vetorial otimizado\n    this.db = await create({\n      schema: {\n        id: 'string',\n        text: 'string',\n        embedding: `vector[${this.config.embeddingDimensions}]`,\n        metadata: {\n          pageNumber: 'number',\n          source: 'string',\n          chunkIndex: 'number',\n          totalTokens: 'number',\n          importance: 'number',\n          hash: 'string' // Para cache\n        }\n      },\n      components: {\n        tokenizer: {\n          language: 'portuguese' // Otimizado para português\n        }\n      }\n    });\n\n    // Inicializar Web Worker\n    this.pdfWorker = new Worker(new URL('../workers/pdf.worker.js', import.meta.url), {\n      type: 'module'\n    });\n\n    this.initialized = true;\n  }\n\n  // Processar PDF com Web Worker\n  async processPDF(file, onProgress) {\n    if (!this.initialized) {\n      await this.initialize();\n    }\n\n    return new Promise(async (resolve, reject) => {\n      const startTime = Date.now();\n      const arrayBuffer = await file.arrayBuffer();\n      const allChunks = [];\n      let processedPages = 0;\n\n      // Listener para mensagens do worker\n      const handleWorkerMessage = async (event) => {\n        const { type, data, error } = event.data;","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":5,"totalChunks":43,"timestamp":1749751085616}}],["d2005a0a-a110-496e-ad9c-22b6c57c5c92",{"pageContent":"try {\n      const startTime = Date.now();\n      const arrayBuffer = await file.arrayBuffer();\n      \n      // Dynamic import para evitar erro se pdfjs não estiver disponível\n      const pdfjsLib = await import('pdfjs-dist');\n      pdfjsLib.GlobalWorkerOptions.workerSrc = 'https://cdn.jsdelivr.net/npm/pdfjs-dist@5.3.31/build/pdf.worker.min.mjs';\n      \n      const pdf = await pdfjsLib.getDocument({ data: arrayBuffer }).promise;\n      const totalPages = pdf.numPages;\n      const allChunks = [];\n      \n      // Process pages in batches\n      const PAGES_PER_BATCH = 3; // Menor para evitar travar a UI\n      \n      for (let startPage = 1; startPage <= totalPages; startPage += PAGES_PER_BATCH) {\n        const endPage = Math.min(startPage + PAGES_PER_BATCH - 1, totalPages);\n        \n        for (let pageNum = startPage; pageNum <= endPage; pageNum++) {\n          const page = await pdf.getPage(pageNum);\n          const textContent = await page.getTextContent();\n          \n          // Enhanced text reconstruction with proper spacing\n          const pageText = this.reconstructPageText(textContent);\n          \n          // Log do texto extraído da página\n          console.log(`📄 Texto extraído da página ${pageNum}:`, pageText);\n          \n          if (pageText) {\n            const chunks = await this.splitter.splitText(pageText);\n            \n            chunks.forEach((chunkData, index) => {\n              // Handle both string and object chunks\n              const chunkText = typeof chunkData === 'string' ? chunkData : chunkData.text;\n              \n              if (!chunkText || typeof chunkText !== 'string') {\n                console.warn('Invalid chunk detected:', chunkData);\n                return;\n              }\n              \n              const hash = this.generateHash(chunkText);\n              \n              allChunks.push({\n                text: chunkText,\n                metadata: {\n                  pageNumber: pageNum,","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":3,"totalChunks":18,"timestamp":1749751085616}}],["56c33526-59ac-498c-a913-eb0cd09ff1e7",{"pageContent":"export const createOpenAIService = (apiKey) => {\n  if (!apiKey) {\n    throw new Error('OpenAI API key is required');\n  }\n  return new OpenAIService(apiKey);\n};","metadata":{"source":"src/services/openai.service.js","section":"","chunkIndex":3,"totalChunks":4,"timestamp":1749751085616}}],["2ffca510-9030-458a-aa5b-e35cd4f61b4e",{"pageContent":"// Extract text from each page\n      for (let i = 1; i <= pdf.numPages; i++) {\n        const page = await pdf.getPage(i);\n        const textContent = await page.getTextContent();\n        \n        // Group items by line based on Y position\n        const lines = {};\n        textContent.items.forEach(item => {\n          const y = Math.round(item.transform[5]);\n          if (!lines[y]) lines[y] = [];\n          lines[y].push(item);\n        });\n        \n        // Sort lines and concatenate text with improved spacing\n        const sortedLines = Object.keys(lines).sort((a, b) => b - a);\n        const pageText = sortedLines.map(y => {\n          const lineItems = lines[y].sort((a, b) => a.transform[4] - b.transform[4]);\n          let lineText = '';\n          \n          for (let i = 0; i < lineItems.length; i++) {\n            const item = lineItems[i];\n            if (i > 0) {\n              const prevItem = lineItems[i - 1];\n              const prevEnd = prevItem.transform[4] + prevItem.width;\n              const currentStart = item.transform[4];\n              \n              // Add space if there's a gap between items\n              if (currentStart - prevEnd > 1) {\n                lineText += ' ';\n              }\n            }\n            lineText += item.str;\n          }\n          \n          return lineText.trim();\n        }).filter(line => line).join('\\n');\n        \n        fullText += pageText + '\\n\\n';\n\n        // Report progress if callback provided\n        if (onProgress) {\n          onProgress({\n            currentPage: i,\n            totalPages: pdf.numPages,\n            progress: (i / pdf.numPages) * 100\n          });\n        }\n      }","metadata":{"source":"src/services/pdf.service.js","section":"","chunkIndex":2,"totalChunks":6,"timestamp":1749751085616}}],["6c1ff4a6-8b9f-4683-9864-8a36df1ac8a9",{"pageContent":"// Gerar e armazenar embeddings com retry e cache\n  async generateAndStoreEmbeddings(chunks, sourceName, onProgress) {\n    const totalChunks = chunks.length;\n    let processedChunks = 0;\n    \n    // Processar em batches\n    for (let i = 0; i < totalChunks; i += this.config.batchSize) {\n      const batch = chunks.slice(i, i + this.config.batchSize);\n      \n      // Verificar cache primeiro\n      const embeddings = await Promise.all(\n        batch.map(async (chunk) => {\n          if (this.embeddingCache.has(chunk.metadata.hash)) {\n            return this.embeddingCache.get(chunk.metadata.hash);\n          }\n          \n          // Gerar com retry\n          const embedding = await this.generateEmbeddingWithRetry(chunk.text);\n          this.embeddingCache.set(chunk.metadata.hash, embedding);\n          return embedding;\n        })\n      );\n      \n      // Inserir no banco vetorial\n      for (let j = 0; j < batch.length; j++) {\n        const chunk = batch[j];\n        const embedding = embeddings[j];\n        \n        await insert(this.db, {\n          id: `${sourceName}_p${chunk.metadata.pageNumber}_c${chunk.metadata.chunkIndex}`,\n          text: chunk.text,\n          embedding: embedding,\n          pageNumber: chunk.metadata.pageNumber,\n          source: chunk.metadata.source,\n          chunkIndex: chunk.metadata.chunkIndex,\n          totalTokens: chunk.metadata.totalTokens,\n          importance: chunk.metadata.importance,\n          hash: chunk.metadata.hash\n        });\n      }\n      \n      processedChunks += batch.length;\n      \n      onProgress?.({\n        phase: 'embedding',\n        current: processedChunks,\n        total: totalChunks,\n        percentage: 40 + (processedChunks / totalChunks) * 60, // 40-100%\n        message: `Gerando embeddings: ${processedChunks}/${totalChunks} chunks`\n      });\n      \n      // Rate limiting\n      if (i + this.config.batchSize < totalChunks) {\n        await new Promise(resolve => setTimeout(resolve, 200));\n      }\n    }\n  }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":5,"totalChunks":18,"timestamp":1749751085616}}],["6337f583-d87c-48d9-9aa7-6c691ed7ab4a",{"pageContent":"/**\n * @fileoverview Utility functions for formatting data\n */\n\n/**\n * Format file size in human-readable format\n * @param {number} bytes - File size in bytes\n * @param {number} [decimals=2] - Number of decimal places\n * @returns {string} Formatted file size\n * \n * @example\n * formatFileSize(1024) // \"1.00 KB\"\n * formatFileSize(1048576) // \"1.00 MB\"\n */\nexport function formatFileSize(bytes, decimals = 2) {\n  if (bytes === 0) return '0 Bytes';\n\n  const k = 1024;\n  const dm = decimals < 0 ? 0 : decimals;\n  const sizes = ['Bytes', 'KB', 'MB', 'GB', 'TB'];\n\n  const i = Math.floor(Math.log(bytes) / Math.log(k));\n\n  return parseFloat((bytes / Math.pow(k, i)).toFixed(dm)) + ' ' + sizes[i];\n}\n\n/**\n * Format duration in MM:SS format\n * @param {number} seconds - Duration in seconds\n * @returns {string} Formatted duration\n * \n * @example\n * formatDuration(65) // \"01:05\"\n * formatDuration(3661) // \"61:01\"\n */\nexport function formatDuration(seconds) {\n  const minutes = Math.floor(seconds / 60);\n  const remainingSeconds = seconds % 60;\n  return `${minutes.toString().padStart(2, '0')}:${remainingSeconds.toString().padStart(2, '0')}`;\n}","metadata":{"source":"src/utils/format.js","section":"","chunkIndex":0,"totalChunks":1,"timestamp":1749751085616}}],["32f7431d-81e7-476f-953b-7f3480d36aec",{"pageContent":"{/* Distribuição de tokens */}\n            {stats.tokenPercentiles && (\n              <Card>\n                <CardBody>\n                  <Typography variant=\"h6\" className=\"mb-3\">\n                    Distribuição de Tokens por Chunk\n                  </Typography>\n                  <div className=\"space-y-2\">\n                    <div className=\"flex justify-between\">\n                      <Typography variant=\"small\">Percentil 25:</Typography>\n                      <Typography variant=\"small\" className=\"font-medium\">\n                        {stats.tokenPercentiles.p25} tokens\n                      </Typography>\n                    </div>\n                    <div className=\"flex justify-between\">\n                      <Typography variant=\"small\">Mediana (P50):</Typography>\n                      <Typography variant=\"small\" className=\"font-medium\">\n                        {stats.tokenPercentiles.p50} tokens\n                      </Typography>\n                    </div>\n                    <div className=\"flex justify-between\">\n                      <Typography variant=\"small\">Percentil 75:</Typography>\n                      <Typography variant=\"small\" className=\"font-medium\">\n                        {stats.tokenPercentiles.p75} tokens\n                      </Typography>\n                    </div>\n                    <div className=\"flex justify-between\">\n                      <Typography variant=\"small\">Percentil 95:</Typography>\n                      <Typography variant=\"small\" className=\"font-medium\">\n                        {stats.tokenPercentiles.p95} tokens\n                      </Typography>\n                    </div>\n                  </div>\n                </CardBody>\n              </Card>\n            )}","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":27,"totalChunks":43,"timestamp":1749751085616}}],["e548cb7f-deb8-462f-abe5-f81778c79728",{"pageContent":"// Gerar e armazenar embeddings com retry e cache\n  async generateAndStoreEmbeddings(chunks, sourceName, onProgress) {\n    const totalChunks = chunks.length;\n    let processedChunks = 0;\n    \n    // Processar em batches\n    for (let i = 0; i < totalChunks; i += this.config.batchSize) {\n      const batch = chunks.slice(i, i + this.config.batchSize);\n      \n      // Verificar cache primeiro\n      const embeddings = await Promise.all(\n        batch.map(async (chunk) => {\n          if (this.embeddingCache.has(chunk.metadata.hash)) {\n            return this.embeddingCache.get(chunk.metadata.hash);\n          }\n          \n          // Gerar com retry\n          const embedding = await this.generateEmbeddingWithRetry(chunk.text);\n          this.embeddingCache.set(chunk.metadata.hash, embedding);\n          return embedding;\n        })\n      );\n      \n      // Inserir no banco vetorial\n      for (let j = 0; j < batch.length; j++) {\n        const chunk = batch[j];\n        const embedding = embeddings[j];\n        \n        await insert(this.db, {\n          id: `${sourceName}_p${chunk.metadata.pageNumber}_c${chunk.metadata.chunkIndex}`,\n          text: chunk.text,\n          embedding: embedding,\n          metadata: chunk.metadata\n        });\n      }\n      \n      processedChunks += batch.length;\n      \n      onProgress?.({\n        phase: 'embedding',\n        current: processedChunks,\n        total: totalChunks,\n        percentage: 40 + (processedChunks / totalChunks) * 60, // 40-100%\n        message: `Gerando embeddings: ${processedChunks}/${totalChunks} chunks`\n      });\n      \n      // Rate limiting\n      if (i + this.config.batchSize < totalChunks) {\n        await new Promise(resolve => setTimeout(resolve, 200));\n      }\n    }\n  }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":8,"totalChunks":43,"timestamp":1749751085616}}],["1e5a29d8-c4fb-4dd8-ac31-c5c889c5acd2",{"pageContent":"<div className=\"space-y-4\">\n              <div>\n                <Input\n                  type=\"password\"\n                  label=\"API Key da OpenAI\"\n                  value={apiKey}\n                  onChange={(e) => setApiKey(e.target.value)}\n                  placeholder=\"sk-...\"\n                  error={!!error}\n                  icon={<KeyIcon className=\"h-5 w-5\" />}\n                />\n                {error && (\n                  <Typography variant=\"small\" color=\"red\" className=\"mt-1 flex items-center gap-1\">\n                    <ExclamationTriangleIcon className=\"h-4 w-4\" />\n                    {error}\n                  </Typography>\n                )}\n              </div>\n\n              <Button\n                onClick={validateAndSaveKey}\n                disabled={!apiKey || isValidating}\n                className=\"w-full\"\n                color=\"blue\"\n              >\n                {isValidating ? 'Validando...' : 'Continuar com API Key'}\n              </Button>\n\n              <div className=\"text-center space-y-2\">\n                <Typography variant=\"small\" color=\"gray\">\n                  Sua API key será armazenada apenas nesta sessão\n                </Typography>\n                <Typography variant=\"small\">\n                  <a \n                    href=\"https://platform.openai.com/api-keys\" \n                    target=\"_blank\" \n                    rel=\"noopener noreferrer\"\n                    className=\"text-blue-500 hover:underline\"\n                  >\n                    Obter API key →\n                  </a>\n                </Typography>\n              </div>\n            </div>\n          </CardBody>\n        </Card>","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":18,"totalChunks":43,"timestamp":1749751085616}}],["ffd95a73-afdc-403a-a8f5-fb981ed39009",{"pageContent":"});\n        \n        return lineText.trim();\n      })\n      .filter(line => line.length > 0);\n    \n    // Join lines and clean up\n    let fullText = sortedLines.join('\\n');\n    \n    // Clean up common PDF extraction issues\n    fullText = fullText\n      // Fix broken words\n      .replace(/(\\w)\\s+(\\w)/g, (match, p1, p2) => {\n        // If both are single characters, likely a broken word\n        if (p1.length === 1 && p2.length === 1) {\n          return p1 + p2;\n        }\n        return match;\n      })\n      // Fix multiple spaces\n      .replace(/\\s{2,}/g, ' ')\n      // Fix line breaks in middle of words\n      .replace(/(\\w)-\\s*\\n\\s*(\\w)/g, '$1$2')\n      // Fix sentence boundaries\n      .replace(/([.!?])\\s*\\n\\s*([A-Z])/g, '$1 $2')\n      // Clean up extra whitespace\n      .trim();\n    \n    return fullText;\n  }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":16,"totalChunks":18,"timestamp":1749751085616}}],["0d38726f-687b-46ca-93a4-faef9adb79ec",{"pageContent":"# Material Tailwind: Pesquisa Extensiva e Guia Completo\n\nMaterial Tailwind é uma biblioteca de componentes que une o poder do Tailwind CSS com os princípios do Material Design, oferecendo **mais de 460 componentes** prontos para uso com mais de **3.1 milhões de instalações** no NPM e **53.000+ projetos ativos**. A biblioteca está disponível para React e HTML, mantendo total compatibilidade com Tailwind CSS v3 e oferecendo versões gratuita e Pro.\n\n## 1. Guia completo de instalação e configuração em projetos React\n\n### Pré-requisitos essenciais\nPara instalar Material Tailwind com sucesso, você precisa de Node.js v16.14.0 ou superior e um projeto React configurado. A biblioteca requer Tailwind CSS como dependência peer.\n\n### Processo de instalação passo a passo\n\n**Etapa 1: Instalar Tailwind CSS**\n```bash\n# Para Create React App ou projetos React existentes\nnpm install -D tailwindcss postcss autoprefixer\nnpx tailwindcss init -p\n```\n\n**Etapa 2: Instalar Material Tailwind React**\n```bash\nnpm install @material-tailwind/react\n# ou usando Yarn\nyarn add @material-tailwind/react\n```\n\n**Etapa 3: Configurar Tailwind com Material Tailwind**\n```javascript\n// tailwind.config.js\nconst withMT = require(\"@material-tailwind/react/utils/withMT\");\n\nmodule.exports = withMT({\n  content: [\"./src/**/*.{js,jsx,ts,tsx}\"],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n});\n```\n\n**Etapa 4: Adicionar diretivas Tailwind ao CSS**\n```css\n/* src/index.css */\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n```\n\n**Etapa 5: Configurar ThemeProvider**\n```javascript\n// src/index.js ou src/main.jsx\nimport React from \"react\";\nimport ReactDOM from \"react-dom/client\";\nimport App from \"./App\";\nimport \"./index.css\";\nimport { ThemeProvider } from \"@material-tailwind/react\";\n\nconst root = ReactDOM.createRoot(document.getElementById(\"root\"));\nroot.render(\n  <React.StrictMode>\n    <ThemeProvider>\n      <App />\n    </ThemeProvider>\n  </React.StrictMode>\n);\n```","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":0,"totalChunks":15,"timestamp":1749751085616}}],["75b37b91-e7ee-46e3-a4ef-8fe329745174",{"pageContent":"// Gerar embedding com retry automático\n  async generateEmbeddingWithRetry(text, retries = 0) {\n    try {\n      const response = await this.openai.embeddings.create({\n        model: this.config.embeddingModel,\n        input: text,\n        dimensions: this.config.embeddingDimensions\n      });\n      \n      return response.data[0].embedding;\n    } catch (error) {\n      if (retries < this.config.maxRetries) {\n        await new Promise(resolve => setTimeout(resolve, this.config.retryDelay * (retries + 1)));\n        return this.generateEmbeddingWithRetry(text, retries + 1);\n      }\n      throw error;\n    }\n  }\n\n  // Busca semântica otimizada\n  async searchSemantic(query, options = {}) {\n    try {\n      if (!this.initialized || !this.db) {\n        throw new Error('Sistema não inicializado');\n      }\n\n      if (!query || query.trim().length === 0) {\n        return [];\n      }\n\n      const {\n        limit = this.config.topK,\n        threshold = this.config.similarityThreshold,\n        useReranking = true,\n        includeContext = true\n      } = options;\n\n      // Gerar embedding da query com cache\n      const queryHash = this.generateHash(query);\n      let queryEmbedding;\n      \n      try {\n        if (this.embeddingCache.has(queryHash)) {\n          queryEmbedding = this.embeddingCache.get(queryHash);\n        } else {\n          queryEmbedding = await this.generateEmbeddingWithRetry(query);\n          this.embeddingCache.set(queryHash, queryEmbedding);\n        }\n      } catch (embedError) {\n        throw new Error(`Erro ao gerar embedding: ${embedError.message}`);\n      }","metadata":{"source":"src/services/highQualityRAG.service.js","section":"","chunkIndex":6,"totalChunks":18,"timestamp":1749751085616}}],["fb95c701-83e3-4455-971b-14c0b0fcff0c",{"pageContent":"// UI de estatísticas\n  const StatsDialog = () => (\n    <Dialog open={showStats} handler={() => setShowStats(false)} size=\"xl\">\n      <DialogHeader>\n        <div className=\"flex items-center gap-2\">\n          <ChartBarIcon className=\"h-6 w-6\" />\n          Estatísticas do Documento\n        </div>\n      </DialogHeader>\n      <DialogBody className=\"overflow-y-auto max-h-[70vh]\">\n        {stats && (\n          <div className=\"space-y-6\">\n            {/* Cards de estatísticas principais */}\n            <div className=\"grid grid-cols-2 md:grid-cols-4 gap-4\">\n              <Card className=\"bg-blue-50\">\n                <CardBody className=\"text-center\">\n                  <Typography variant=\"h6\" color=\"blue\">Total de Chunks</Typography>\n                  <Typography variant=\"h3\" color=\"blue-gray\">\n                    {stats.totalChunks}\n                  </Typography>\n                </CardBody>\n              </Card>\n              \n              <Card className=\"bg-green-50\">\n                <CardBody className=\"text-center\">\n                  <Typography variant=\"h6\" color=\"green\">Total de Tokens</Typography>\n                  <Typography variant=\"h3\" color=\"blue-gray\">\n                    {(stats.totalTokens / 1000).toFixed(1)}k\n                  </Typography>\n                </CardBody>\n              </Card>\n              \n              <Card className=\"bg-orange-50\">\n                <CardBody className=\"text-center\">\n                  <Typography variant=\"h6\" color=\"orange\">Páginas</Typography>\n                  <Typography variant=\"h3\" color=\"blue-gray\">\n                    {stats.pagesProcessed}\n                  </Typography>\n                </CardBody>\n              </Card>\n              \n              <Card className=\"bg-purple-50\">\n                <CardBody className=\"text-center\">\n                  <Typography variant=\"h6\" color=\"purple\">Média/Chunk</Typography>\n                  <Typography variant=\"h3\" color=\"blue-gray\">","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":25,"totalChunks":43,"timestamp":1749751085616}}],["722ce725-dc76-458f-96cf-8707a35c853c",{"pageContent":"export const getEmbeddingCache = (maxSize = 1000) => {\n  if (!cacheInstance) {\n    cacheInstance = new EmbeddingCache(maxSize);\n  }\n  return cacheInstance;\n};","metadata":{"source":"src/utils/embeddingCache.js","section":"","chunkIndex":2,"totalChunks":3,"timestamp":1749751085616}}],["8b39f3f9-cdc6-46ee-8aac-d82911968a7c",{"pageContent":"const finalText = fullText.trim();\n      console.log('📄 Texto extraído do PDF:', finalText);\n      \n      return {\n        content: finalText,\n        pageCount: pdf.numPages,\n        extractedAt: new Date()\n      };\n    } catch (error) {\n      console.error('PDF text extraction failed:', error);\n      throw new Error(ERROR_MESSAGES.PDF_PROCESSING);\n    }\n  }\n\n  /**\n   * Extract text using pdf-parse (alternative method)\n   * @param {File} file - PDF file to process\n   * @param {Function} onProgress - Progress callback function\n   * @returns {Promise<Object>} Extracted text data\n   */\n  async extractTextWithPdfParse(file, onProgress) {\n    const parser = await loadPdfParse();\n    if (!parser) {\n      // Fallback to original method if pdf-parse is not available\n      return this.extractText(file, onProgress);\n    }\n\n    try {\n      const arrayBuffer = await file.arrayBuffer();\n      const buffer = Buffer.from(arrayBuffer);\n      \n      const data = await parser(buffer, {\n        // Options for better text extraction\n        normalizeWhitespace: true,\n        disableCombineTextItems: false\n      });\n\n      // Call progress callback with completion\n      if (onProgress) {\n        onProgress({\n          currentPage: data.numpages,\n          totalPages: data.numpages,\n          progress: 100\n        });\n      }\n\n      console.log('📄 Texto extraído do PDF (pdf-parse):', data.text);\n      \n      return {\n        content: data.text,\n        pageCount: data.numpages,\n        extractedAt: new Date()\n      };\n    } catch (error) {\n      console.warn('pdf-parse extraction failed, falling back to pdfjs:', error);\n      return this.extractText(file, onProgress);\n    }\n  }\n\n  /**\n   * Get PDF metadata\n   * @param {File} file - PDF file\n   * @returns {Promise<Object>} PDF metadata\n   */\n  async getMetadata(file) {\n    this.validateFile(file);","metadata":{"source":"src/services/pdf.service.js","section":"","chunkIndex":3,"totalChunks":6,"timestamp":1749751085616}}],["7425f259-73ec-4ab8-bd5a-deab073d4c29",{"pageContent":"export class EmbeddingCache {\n  /**\n   * @param {number} maxSize - Maximum number of items to store\n   */\n  constructor(maxSize = 1000) {\n    this.cache = new Map();\n    this.maxSize = maxSize;\n  }\n\n  /**\n   * Get value from cache\n   * @param {string} key - Cache key\n   * @returns {any} Cached value or undefined\n   */\n  get(key) {\n    const value = this.cache.get(key);\n    if (value) {\n      // Move to end (most recent)\n      this.cache.delete(key);\n      this.cache.set(key, value);\n    }\n    return value;\n  }\n\n  /**\n   * Set value in cache\n   * @param {string} key - Cache key\n   * @param {any} value - Value to cache\n   */\n  set(key, value) {\n    // Remove existing entry\n    if (this.cache.has(key)) {\n      this.cache.delete(key);\n    }\n    \n    // Remove oldest if at capacity\n    if (this.cache.size >= this.maxSize) {\n      const firstKey = this.cache.keys().next().value;\n      this.cache.delete(firstKey);\n    }\n    \n    this.cache.set(key, value);\n  }\n\n  /**\n   * Check if key exists\n   * @param {string} key - Cache key\n   * @returns {boolean}\n   */\n  has(key) {\n    return this.cache.has(key);\n  }\n\n  /**\n   * Clear cache\n   */\n  clear() {\n    this.cache.clear();\n  }\n\n  /**\n   * Get cache size\n   * @returns {number}\n   */\n  get size() {\n    return this.cache.size;\n  }\n\n  /**\n   * Generate cache key from text\n   * @param {string} text - Text to hash\n   * @returns {string} Cache key\n   */\n  static generateKey(text) {\n    // Simple hash function for cache key\n    let hash = 0;\n    for (let i = 0; i < text.length; i++) {\n      const char = text.charCodeAt(i);\n      hash = ((hash << 5) - hash) + char;\n      hash = hash & hash; // Convert to 32bit integer\n    }\n    return hash.toString(36);\n  }\n}\n\n// Singleton instance\nlet cacheInstance = null;\n\n/**\n * Get or create cache instance\n * @param {number} maxSize - Maximum cache size\n * @returns {EmbeddingCache}\n */","metadata":{"source":"src/utils/embeddingCache.js","section":"","chunkIndex":1,"totalChunks":3,"timestamp":1749751085616}}],["2a2ecc01-a4fe-454d-a255-1927afaa5e70",{"pageContent":"// Listener para mensagens do worker\n      const handleWorkerMessage = async (event) => {\n        const { type, data, error } = event.data;\n\n        if (type === 'batch-complete') {\n          processedPages += data.chunks.length;\n          \n          // Processar chunks do batch\n          for (const pageData of data.chunks) {\n            const chunks = await this.splitter.splitText(pageData.text);\n            \n            chunks.forEach((chunk, index) => {\n              const hash = this.generateHash(chunk);\n              \n              allChunks.push({\n                text: chunk,\n                metadata: {\n                  pageNumber: pageData.pageNumber,\n                  chunkIndex: index,\n                  source: file.name,\n                  totalTokens: this.estimateTokens(chunk),\n                  importance: this.calculateImportance(chunk, pageData.pageNumber, 100), // Assumindo 100 páginas max\n                  hash: hash\n                }\n              });\n            });\n          }","metadata":{"source":"docs/rag-openai-high-quality.md","section":"","chunkIndex":6,"totalChunks":43,"timestamp":1749751085616}}],["fe75bf23-d031-47a7-b68f-bed522519abb",{"pageContent":"/**\n   * Get overlap text with token count\n   * @param {string} text - Text to get overlap from\n   * @param {number} overlapTokens - Number of tokens to overlap\n   * @returns {Object} Overlap text and token count\n   */\n  getOverlapWithTokens(text, overlapTokens) {\n    if (!text || overlapTokens <= 0) return { text: '', tokens: 0 };\n    \n    // Get last words that fit within overlap token count\n    const words = text.split(' ');\n    let overlap = '';\n    let currentTokens = 0;\n    \n    for (let i = words.length - 1; i >= 0; i--) {\n      const word = words[i];\n      const wordTokens = this.tokenizer(word);\n      \n      if (currentTokens + wordTokens + (overlap ? 1 : 0) <= overlapTokens) {\n        overlap = word + (overlap ? ' ' + overlap : '');\n        currentTokens += wordTokens + (overlap ? 1 : 0);\n      } else {\n        break;\n      }\n    }\n    \n    return { text: overlap, tokens: currentTokens };\n  }\n\n  /**\n   * Split text for PDF processing with page context\n   * @param {string} text - PDF text content\n   * @param {Object} options - Additional options\n   * @returns {Promise<Array>} Chunks with metadata\n   */\n  async splitPDFText(text, options = {}) {\n    const { pageBreaks = [], sourceFile = 'unknown' } = options;\n    const chunks = await this.splitText(text);\n    \n    // Enhance chunks with PDF metadata\n    return chunks.map(chunk => ({\n      ...chunk,\n      metadata: {\n        ...chunk.metadata,\n        sourceFile,\n        pageNumber: this.findPageNumber(chunk.index, pageBreaks)\n      }\n    }));\n  }\n\n  /**\n   * Find page number for chunk\n   * @private\n   */\n  findPageNumber(chunkIndex, pageBreaks) {\n    // Implementation would map chunk indices to page numbers\n    return 1; // Placeholder\n  }\n}","metadata":{"source":"src/utils/textSplitter.js","section":"","chunkIndex":3,"totalChunks":4,"timestamp":1749751085616}}],["2e299e95-3c4a-4dd5-9443-8fba327b1422",{"pageContent":"export class PDFService {\n  /**\n   * Validate PDF file before processing\n   * @param {File} file - PDF file to validate\n   * @throws {Error} When file is invalid\n   */\n  validateFile(file) {\n    if (!file) {\n      throw new Error('No file provided');\n    }\n\n    if (file.size > PDF_CONFIG.MAX_FILE_SIZE) {\n      throw new Error(ERROR_MESSAGES.FILE_TOO_LARGE);\n    }\n\n    if (!PDF_CONFIG.ACCEPTED_TYPES.includes(file.type)) {\n      throw new Error(ERROR_MESSAGES.INVALID_FILE_TYPE);\n    }\n  }\n\n  /**\n   * Extract text from PDF file\n   * @param {File} file - PDF file to process\n   * @param {function} onProgress - Progress callback (optional)\n   * @returns {Promise<{content: string, pageCount: number, extractedAt: Date}>}\n   * @throws {Error} When extraction fails\n   */\n  async extractText(file, onProgress) {\n    this.validateFile(file);\n\n    try {\n      // Convert file to ArrayBuffer\n      const arrayBuffer = await file.arrayBuffer();\n      \n      // Load PDF document\n      const pdf = await pdfjsLib.getDocument({ data: arrayBuffer }).promise;\n      let fullText = '';","metadata":{"source":"src/services/pdf.service.js","section":"","chunkIndex":1,"totalChunks":6,"timestamp":1749751085616}}],["589773be-6c80-453c-925b-e7c125d5c582",{"pageContent":"export class SimpleTextSplitter {\n  constructor({ \n    chunkSize = 512, \n    chunkOverlap = 50, \n    separators = [\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \": \", \" \"],\n    preserveContext = true,\n    tokenizer = null \n  }) {\n    this.chunkSize = chunkSize;\n    this.chunkOverlap = chunkOverlap;\n    this.separators = separators;\n    this.preserveContext = preserveContext;\n    this.tokenizer = tokenizer || this.defaultTokenizer;\n  }\n\n  /**\n   * Simple token counter (approximation)\n   * @param {string} text - Text to count tokens\n   * @returns {number} Approximate token count\n   */\n  defaultTokenizer(text) {\n    // Approximate: 1 token ≈ 4 characters\n    return Math.ceil(text.length / 4);\n  }\n\n  async splitText(text) {\n    if (!text || typeof text !== 'string') {\n      return [];\n    }\n\n    // Normalize text\n    text = text.trim();\n    const textTokens = this.tokenizer(text);\n    \n    if (textTokens <= this.chunkSize) {\n      return [text];\n    }\n\n    const chunks = [];\n    let currentChunk = '';\n    let currentTokenCount = 0;\n    const sentences = this.splitBySeparators(text);\n\n    for (const sentence of sentences) {\n      const sentenceTokens = this.tokenizer(sentence);\n      \n      // If adding this sentence would exceed chunk size\n      if (currentChunk && (currentTokenCount + sentenceTokens + 1) > this.chunkSize) {\n        // Add current chunk if it has content\n        if (currentChunk.trim()) {\n          chunks.push(this.formatChunk(currentChunk.trim(), chunks.length));\n        }","metadata":{"source":"src/utils/textSplitter.js","section":"","chunkIndex":1,"totalChunks":4,"timestamp":1749751085616}}],["3def05ab-a9aa-4924-9765-5256b5cb31ee",{"pageContent":"### Problema 6: Ícones Material não exibindo\n\n**Adicionar fonte Material Icons ao HTML**\n```html\n<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" />\n```\n\n## Recursos adicionais e links importantes\n\n### Documentação oficial\n- **React Docs**: https://www.material-tailwind.com/docs/react/installation\n- **HTML Docs**: https://www.material-tailwind.com/docs/html/installation\n- **GitHub Repository**: https://github.com/creativetimofficial/material-tailwind\n\n### Exemplos e templates\n- **Material Tailwind Dashboard React**: Template admin gratuito com 40+ componentes\n- **Material Tailwind Kit React**: UI kit com seções pré-construídas\n- **CodeSandbox Playground**: Editor online para Material Tailwind React/HTML\n- **Material Tailwind Blocks**: Coleção premium com 30+ novos blocks\n\nMaterial Tailwind oferece uma solução robusta para desenvolvedores que buscam combinar a estética do Material Design com a abordagem utility-first do Tailwind CSS. A versão gratuita fornece funcionalidade substancial para a maioria dos projetos, enquanto a versão Pro atende a necessidades comerciais avançadas com componentes e templates premium. A compatibilidade total com Tailwind CSS v3.x garante capacidades modernas de desenvolvimento web, embora os desenvolvedores devam monitorar as atualizações de compatibilidade com v4 para preparar seus projetos para o futuro.","metadata":{"source":"docs/Material-Tailwind-Guia-Completo.md","section":"","chunkIndex":14,"totalChunks":15,"timestamp":1749751085616}}],["7b1aa003-2614-42c3-9888-7be63c8c0aa3",{"pageContent":"/**\n   * Get PDF metadata\n   * @param {File} file - PDF file\n   * @returns {Promise<Object>} PDF metadata\n   */\n  async getMetadata(file) {\n    this.validateFile(file);\n\n    try {\n      const arrayBuffer = await file.arrayBuffer();\n      const pdf = await pdfjsLib.getDocument({ data: arrayBuffer }).promise;\n      const metadata = await pdf.getMetadata();\n\n      return {\n        pageCount: pdf.numPages,\n        title: metadata.info?.Title || 'Untitled',\n        author: metadata.info?.Author || 'Unknown',\n        subject: metadata.info?.Subject || '',\n        creator: metadata.info?.Creator || '',\n        producer: metadata.info?.Producer || '',\n        creationDate: metadata.info?.CreationDate || null,\n        modificationDate: metadata.info?.ModDate || null,\n      };\n    } catch (error) {\n      console.error('Failed to get PDF metadata:', error);\n      throw new Error('Failed to extract PDF metadata');\n    }\n  }\n}\n\n/**\n * Create PDF service instance\n * @returns {PDFService} Service instance\n */","metadata":{"source":"src/services/pdf.service.js","section":"","chunkIndex":4,"totalChunks":6,"timestamp":1749751085616}}],["1c5e5b81-20d3-4c93-a6c4-77c27f1e0550",{"pageContent":"// Start new chunk with overlap from previous chunk\n        if (chunks.length > 0 && this.chunkOverlap > 0) {\n          const overlapData = this.getOverlapWithTokens(chunks[chunks.length - 1].text, this.chunkOverlap);\n          currentChunk = overlapData.text ? overlapData.text + ' ' + sentence : sentence;\n          currentTokenCount = overlapData.tokens + sentenceTokens + 1;\n        } else {\n          currentChunk = sentence;\n          currentTokenCount = sentenceTokens;\n        }\n      } else {\n        // Add sentence to current chunk\n        currentChunk = currentChunk ? currentChunk + ' ' + sentence : sentence;\n        currentTokenCount += sentenceTokens + (currentChunk ? 1 : 0);\n      }\n    }\n\n    // Add the last chunk if it has content\n    if (currentChunk.trim()) {\n      chunks.push(this.formatChunk(currentChunk.trim(), chunks.length));\n    }\n\n    return chunks.filter(chunk => chunk.text && chunk.text.length > 0);\n  }\n\n  /**\n   * Format chunk with metadata\n   * @param {string} text - Chunk text\n   * @param {number} index - Chunk index\n   * @returns {Object} Formatted chunk\n   */\n  formatChunk(text, index) {\n    return {\n      text,\n      index,\n      tokens: this.tokenizer(text),\n      metadata: {\n        chunkIndex: index,\n        hasOverlap: index > 0 && this.chunkOverlap > 0\n      }\n    };\n  }\n\n  splitBySeparators(text) {\n    let parts = [text];\n    \n    for (const separator of this.separators) {\n      const newParts = [];\n      for (const part of parts) {\n        if (part.includes(separator)) {\n          const split = part.split(separator);\n          for (let i = 0; i < split.length; i++) {\n            if (split[i].trim()) {\n              newParts.push(split[i].trim());\n            }\n          }\n        } else {\n          newParts.push(part);\n        }\n      }\n      parts = newParts;\n    }\n    \n    return parts;\n  }","metadata":{"source":"src/utils/textSplitter.js","section":"","chunkIndex":2,"totalChunks":4,"timestamp":1749751085616}}],["fb7c17c4-e1e1-4582-8a54-52a62fc794cf",{"pageContent":"/**\n   * Get recording duration in seconds\n   * @returns {number} Recording duration\n   */\n  getDuration() {\n    if (!this.mediaRecorder) return 0;\n    \n    // Estimate duration based on chunks (approximate)\n    return this.audioChunks.length * 0.1; // Rough estimate\n  }\n}\n\n/**\n * Create audio service instance\n * @returns {AudioService} Service instance\n */","metadata":{"source":"src/services/audio.service.js","section":"","chunkIndex":3,"totalChunks":5,"timestamp":1749751085616}}],["1895f762-389e-4b07-8475-ed1bb92da68c",{"pageContent":"# Template Abrangente para CLAUDE CODE\n\n\n## Visão Geral do Projeto\n\n<project-info>\nNome do Projeto: Podcast POC\nRepositório: podcast-poc\nDescrição: CLI Node.js para gravação de voz, transcrição automática, síntese de voz (TTS) e processamento com IA. Aplicação focada em criar um fluxo completo de produção de conteúdo em áudio através de interfaces de linha de comando.\nLinguagens: JavaScript/Node.js\nFrameworks: React (para interface web), Vite (build tool)\nBibliotecas Principais: OpenAI API, pdfjs-dist (extração de PDF), Material Tailwind (UI), TailwindCSS\nPlataforma: CLI/Web - Aplicação híbrida com foco em terminal e interface web complementar\n</project-info>\n\n## Comandos Essenciais\n\n```bash\n# Instalação e setup\nnpm install                # Instalar todas as dependências do projeto\nnpm fund                   # Ver informações de financiamento dos pacotes (opcional)\n\n# Build\nnpm run build             # Build para produção usando Vite\nnpm run preview           # Visualizar build de produção localmente\n\n# Testes\n# Nota: Este projeto não possui testes configurados ainda\n# TODO: Implementar testes com Vitest ou Jest\n\n# Lint e Verificação de Tipos\nnpm run lint              # Verificar código com ESLint\n# Nota: Não há script de formatação automática configurado\n# Recomenda-se configurar Prettier no futuro\n\n# Execução\nnpm run dev               # Iniciar servidor de desenvolvimento Vite (port 5173)\n# Acesso: http://localhost:5173\n\n# CI/CD\n# Não configurado - projeto em fase POC\n\n# Git\ngit checkout development  # Branch de desenvolvimento principal\ngit checkout -b feature/nome-da-feature  # Criar nova feature\ngit commit -m \"tipo: descrição\"  # Formato de commit recomendado\n```\n\n## Arquitetura do Sistema\n\n<architecture>\nO sistema utiliza uma arquitetura funcional e modular organizada em 7 módulos principais, que fornecem funcionalidades específicas e se integram de forma coesa:\n\n### Core Modules","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":0,"totalChunks":9,"timestamp":1749751088966}}],["4c80953b-9b4e-4241-9b62-ff31368816e2",{"pageContent":"<architecture>\nO sistema utiliza uma arquitetura funcional e modular organizada em 7 módulos principais, que fornecem funcionalidades específicas e se integram de forma coesa:\n\n### Core Modules\n\n1. **Voice Recorder (PvRecorder)**\n   - Captura de áudio em tempo real usando Picovoice Recorder\n   - Configuração de dispositivos de entrada de áudio\n   - Buffer de áudio otimizado para processamento\n   - Suporte multiplataforma (Windows, macOS, Linux)\n\n2. **Transcriber (Whisper)**\n   - Transcrição de áudio para texto usando OpenAI Whisper\n   - Suporte para múltiplos idiomas\n   - Processamento local ou via API\n   - Alta precisão em transcrição\n\n3. **TTS (Text-to-Speech)**\n   - Conversão de texto em fala natural\n   - Múltiplas vozes e idiomas disponíveis\n   - Controle de velocidade e tom\n   - Integração com OpenAI TTS API\n\n4. **AI Processor**\n   - Processamento inteligente de comandos e contexto\n   - Integração com GPT-4 para respostas avançadas\n   - Gerenciamento de histórico de conversação\n   - Otimização de prompts e respostas\n\n5. **Audio Player**\n   - Reprodução de áudio sintetizado\n   - Controle de volume e playback\n   - Suporte para múltiplos formatos de áudio\n   - Queue de reprodução para respostas longas\n\n6. **Key Capture**\n   - Captura de teclas de atalho globais\n   - Ativação por voz ou teclado\n   - Configuração customizável de hotkeys\n   - Integração com sistema operacional\n\n7. **Handlers Manager**\n   - Gerenciamento centralizado de eventos\n   - Coordenação entre módulos\n   - Tratamento de erros e exceções\n   - Sistema de callbacks e promises\n\n### Fluxo de Dados","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":1,"totalChunks":9,"timestamp":1749751088966}}],["cb5d70fa-6757-419e-89ae-13cca98ebf3c",{"pageContent":"7. **Handlers Manager**\n   - Gerenciamento centralizado de eventos\n   - Coordenação entre módulos\n   - Tratamento de erros e exceções\n   - Sistema de callbacks e promises\n\n### Fluxo de Dados\n\n1. **Configuração Inicial**: O usuário configura a API key através do ApiKeySetup\n2. **Upload de Documento**: PDFs são processados e texto é extraído\n3. **Processamento RAG**: Texto é dividido em chunks e embeddings são gerados\n4. **Interação por Chat**: Usuário faz perguntas via texto ou áudio\n5. **Busca Semântica**: Sistema encontra chunks relevantes\n6. **Geração de Resposta**: OpenAI processa contexto e gera resposta\n7. **Síntese de Voz**: Resposta pode ser convertida em áudio\n\n### Integrações Externas\n\n- **OpenAI API**: Core da funcionalidade de IA (GPT-4, Whisper, TTS)\n- **PDF.js**: Biblioteca Mozilla para processamento de PDFs\n- **Web Audio API**: Captura nativa de áudio do navegador\n- **localStorage**: Persistência de configurações e cache\n\n### Padrões de Design\n\n- **Separação de Responsabilidades**: Cada módulo tem uma função específica\n- **Facade Pattern**: Services encapsulam complexidade das APIs\n- **Observer Pattern**: Hooks React para gerenciamento de estado\n- **Factory Pattern**: Criação dinâmica de clientes OpenAI\n- **Singleton Pattern**: Instâncias únicas de serviços\n\n### Comunicação entre Módulos\n\n- Módulos se comunicam através de interfaces bem definidas\n- Services retornam Promises para operações assíncronas\n- Hooks React abstraem a complexidade dos services\n- Componentes consomem hooks para funcionalidades\n- Sistema de eventos para atualizações em tempo real\n</architecture>\n\n## Estrutura do Repositório","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":2,"totalChunks":9,"timestamp":1749751088966}}],["8bdfc8cd-cedf-44fd-a289-9a2fb2377fd7",{"pageContent":"## Estrutura do Repositório\n\n```\npodcast-poc/\n├── src/                     # Código fonte da aplicação\n│   ├── components/          # Componentes React reutilizáveis\n│   │   ├── features/        # Componentes de funcionalidades específicas\n│   │   └── ui/              # Componentes de interface genéricos\n│   ├── services/            # Serviços e lógica de negócio\n│   ├── hooks/               # Custom React hooks\n│   ├── constants/           # Constantes e configurações\n│   ├── utils/               # Funções utilitárias\n│   ├── assets/              # Recursos estáticos (imagens, etc)\n│   ├── App.jsx              # Componente principal da aplicação\n│   ├── main.jsx             # Ponto de entrada da aplicação\n│   └── index.css            # Estilos globais\n├── public/                  # Arquivos públicos servidos diretamente\n├── docs/                    # Documentação do projeto\n├── .ondokai/                # Configurações e documentação do Ondokai\n├── package.json             # Dependências e scripts npm\n├── vite.config.js           # Configuração do Vite\n├── tailwind.config.js       # Configuração do Tailwind CSS\n├── postcss.config.js        # Configuração do PostCSS\n└── eslint.config.js         # Configuração do ESLint\n```\n\n## Convenções de Código\n\n<code-conventions>\n### Estilo de Código\n- Indentação: 2 espaços\n- Comprimento máximo de linha: 80-100 caracteres (recomendado)\n- Quotes: simples para imports, duplas para strings JSX\n- Ponto-e-vírgula: obrigatório (aplicado pelo ESLint)\n- Chaves: mesma linha (estilo K&R)\n- Trailing comma: sempre em objetos e arrays multilinhas","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":3,"totalChunks":9,"timestamp":1749751088966}}],["e4f1e7d4-50ce-42f8-b11a-b6e7b80935aa",{"pageContent":"### Nomenclatura\n- Variáveis: camelCase (ex: `apiKey`, `isLoading`)\n- Funções: camelCase (ex: `validateApiKey`, `extractText`)\n- Classes: PascalCase (ex: `ConfigService`, `OpenAIService`)\n- Constantes: UPPER_SNAKE_CASE (ex: `API_KEY_STORAGE`, `MAX_TOKENS`)\n- Arquivos de componentes: PascalCase.jsx (ex: `ApiKeySetup.jsx`)\n- Arquivos de serviços: camelCase.service.js (ex: `config.service.js`)\n- Hooks customizados: use + PascalCase (ex: `useOpenAI`, `usePDF`)\n\n### Documentação\n- Estilo de comentários: JSDoc para funções públicas complexas\n- Comentários inline: evitar, preferir código autoexplicativo\n- TODOs: formato `// TODO: descrição da tarefa`\n- Documentar propósito não óbvio, não o que o código faz\n\n### Padrões de Projeto\n- **Service Pattern**: Toda lógica de negócio em classes de serviço estáticas\n- **Custom Hooks**: Encapsular lógica de estado e side-effects\n- **Composição**: Preferir composição sobre herança\n- **Single Responsibility**: Cada módulo/componente com uma única responsabilidade\n- **Error Boundaries**: Usar para capturar erros em componentes React\n</code-conventions>\n\n## Fluxo de Trabalho Git\n\n<git-workflow>\n### Branches\n- Branch principal: `main`\n- Branch de desenvolvimento: `development`\n- Branches de feature: `feature/nome-da-feature`\n- Branches de fix: `fix/nome-do-bug`\n- Branches de hotfix: `hotfix/nome-critico`\n\n### Commits\n- Formato de mensagem: `tipo: descrição breve (máx 50 chars)`\n- Tipos aceitos:\n  - `feat`: nova funcionalidade\n  - `fix`: correção de bug\n  - `docs`: apenas documentação\n  - `style`: formatação, sem mudança de código\n  - `refactor`: refatoração sem mudança de funcionalidade\n  - `test`: adição ou correção de testes\n  - `chore`: manutenção, atualização de dependências\n- Corpo do commit: explicar o \"porquê\", não o \"o quê\"","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":4,"totalChunks":9,"timestamp":1749751088966}}],["ed660ee6-6eef-43e5-b2cf-d075f9938010",{"pageContent":"### Pull Requests\n- Template básico:\n  - Descrição da mudança\n  - Tipo de mudança (feature/fix/breaking change)\n  - Como testar\n  - Checklist de revisão\n- Processo de revisão: auto-merge permitido em POC\n- Critérios de merge: \n  - ESLint passando\n  - Build bem-sucedido\n  - Sem conflitos\n\n### CI/CD\n- Gatilhos: não configurado (projeto POC)\n- Verificações manuais recomendadas antes do merge:\n  - `npm run lint`\n  - `npm run build`\n  - Testar funcionalidades afetadas\n</git-workflow>\n\n## Gerenciamento de Dependências\n\n<dependencies>\n### Dependências Críticas\n- **openai**: v5.3.0 - Cliente oficial OpenAI para GPT-4, Whisper e embeddings\n- **react**: v19.1.0 - Framework UI principal\n- **@material-tailwind/react**: v2.1.10 - Componentes UI com Material Design\n- **pdfjs-dist**: v5.3.31 - Extração de texto de PDFs (Mozilla)\n- **@orama/orama**: v3.1.7 - Busca vetorial e full-text search\n- **wavesurfer.js**: v7.9.5 - Visualização e reprodução de áudio\n\n### Dependências de Desenvolvimento\n- **vite**: v6.3.5 - Build tool e dev server ultrarrápido\n- **eslint**: v9.25.0 - Linting e qualidade de código\n- **tailwindcss**: v3.4.17 - Framework CSS utility-first\n- **@vitejs/plugin-react**: v4.4.1 - Suporte React para Vite\n\n### Política de Atualização\n- Frequência: Mensal para patches, trimestral para minor versions\n- Breaking changes: Criar branch específica, testar todas funcionalidades\n- Monitoramento: \n  - `npm outdated` - verificar pacotes desatualizados\n  - `npm audit` - verificar vulnerabilidades de segurança\n  - Renovate Bot recomendado para automação futura\n</dependencies>\n\n\n\n\n\n## Configuração do Ambiente de Desenvolvimento\n\n<dev-environment>\n### Requisitos\n- Node.js: v18.0.0+ (recomendado v20+)\n- npm: v9.0.0+\n- Git: v2.0+\n- VSCode ou editor com suporte a JSX\n- Chrome/Firefox/Safari (desenvolvimento web)","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":5,"totalChunks":9,"timestamp":1749751088966}}],["417e1f39-f3bf-42bd-b4ba-6008af83fdea",{"pageContent":"<dev-environment>\n### Requisitos\n- Node.js: v18.0.0+ (recomendado v20+)\n- npm: v9.0.0+\n- Git: v2.0+\n- VSCode ou editor com suporte a JSX\n- Chrome/Firefox/Safari (desenvolvimento web)\n\n### Configuração Passo a Passo\n1. Clone o repositório:\n   ```bash\n   git clone [url-do-repo]\n   cd podcast-poc\n   ```\n2. Instale as dependências:\n   ```bash\n   npm install\n   ```\n3. Configure variáveis de ambiente (se necessário):\n   ```bash\n   cp .env.example .env.local\n   ```\n4. Inicie o servidor de desenvolvimento:\n   ```bash\n   npm run dev\n   ```\n5. Acesse http://localhost:5173\n\n### Configurações Específicas\n- **VSCode**: Instalar extensões ESLint, Tailwind CSS IntelliSense\n- **Git**: Configurar email e nome para commits\n- **Node**: Usar nvm para gerenciar versões\n- **Browser**: Instalar React Developer Tools\n\n### Containers e Virtualização\n- Docker: Não configurado (futuro: Dockerfile para produção)\n- Dev Containers: Configuração VSCode recomendada para consistência\n</dev-environment>\n\n## Best Practices de Desenvolvimento\n\n<development-best-practices>\n### Estrutura de Componentes React\n- Um componente por arquivo\n- Props destructuring no início\n- Hooks no topo do componente\n- Handlers antes do return\n- JSX limpo e legível\n\n```jsx\n// Exemplo de estrutura ideal\nexport function ComponentName({ prop1, prop2 }) {\n  // Estados\n  const [state, setState] = useState();\n  \n  // Hooks customizados\n  const { data, loading } = useCustomHook();\n  \n  // Effects\n  useEffect(() => {\n    // lógica\n  }, [dependências]);\n  \n  // Handlers\n  const handleClick = () => {\n    // lógica\n  };\n  \n  // Render condicional early return\n  if (loading) return <Spinner />;\n  \n  // JSX principal\n  return (\n    <div>\n      {/* conteúdo */}\n    </div>\n  );\n}\n```\n\n### Organização de Serviços\n- Classes estáticas para serviços stateless\n- Métodos puros sempre que possível\n- Tratamento de erros consistente\n- Retornar Promises para operações assíncronas","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":6,"totalChunks":9,"timestamp":1749751088966}}],["0545e60a-4421-4f84-991c-229bc9e52746",{"pageContent":"### Organização de Serviços\n- Classes estáticas para serviços stateless\n- Métodos puros sempre que possível\n- Tratamento de erros consistente\n- Retornar Promises para operações assíncronas\n\n```javascript\nexport class ServiceName {\n  static async methodName(params) {\n    try {\n      // validação de entrada\n      if (!params) throw new Error('Params required');\n      \n      // lógica principal\n      const result = await someOperation();\n      \n      // retorno padronizado\n      return { success: true, data: result };\n    } catch (error) {\n      console.error('ServiceName.methodName error:', error);\n      return { success: false, error: error.message };\n    }\n  }\n}\n```\n\n### Estado e Performance\n- useState para estado local simples\n- useReducer para estado complexo\n- Context API para estado global (evitar prop drilling)\n- Memoização com useMemo/useCallback quando necessário\n- Lazy loading de componentes pesados\n\n### Segurança\n- Nunca commitar API keys ou secrets\n- Usar sessionStorage para dados sensíveis temporários\n- Validar inputs do usuário\n- Sanitizar conteúdo antes de renderizar\n- CORS configurado apropriadamente\n\n### Qualidade de Código\n- Sempre rodar `npm run lint` antes de commitar\n- Resolver warnings do ESLint\n- Evitar any em TypeScript (quando migrar)\n- Testes para funcionalidades críticas\n- Code review mesmo em POC\n\n### Debugging\n- Console.log com prefixos descritivos\n- React DevTools para estado e props\n- Network tab para requisições API\n- Breakpoints no código\n- Error boundaries para capturar erros\n\n### Performance\n- Otimizar bundle size (analisar com vite-bundle-visualizer)\n- Lazy loading de rotas e componentes pesados\n- Debounce/throttle para inputs frequentes\n- Virtualização para listas longas\n- Cache de requisições quando apropriado\n</development-best-practices>\n\n\n\n## Instruções para Mode de Pensamento Estendido\n\n<thinking-mode>\nUse as seguintes técnicas para ativar o modo de pensamento estendido do Claude CODE:","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":7,"totalChunks":9,"timestamp":1749751088966}}],["e653e2de-f267-4cb9-b439-983ac65ec692",{"pageContent":"## Instruções para Mode de Pensamento Estendido\n\n<thinking-mode>\nUse as seguintes técnicas para ativar o modo de pensamento estendido do Claude CODE:\n\n- \"think\" - Ativa 4.000 tokens de pensamento\n- \"think hard\" ou \"think a lot\" ou \"think deeply\" - Ativa 10.000 tokens de pensamento \n- \"think harder\" ou \"think very hard\" - Ativa 31.999 tokens de pensamento\n- \"ultrathink\" ou \"megathink\" - Ativa o nível máximo de pensamento (31.999 tokens)\n\nUse essas técnicas para análises complexas de arquitetura, planejamento de features ou debugging de problemas difíceis.\n</thinking-mode>\n\n\n\n\n\n\n---\n\n**Notas Adicionais para o CLAUDE CODE:**\n\nEste template deve ser adaptado conforme as necessidades específicas do seu projeto. Seções podem ser removidas ou adicionadas de acordo com a complexidade e requisitos. Mantenha este arquivo atualizado conforme o projeto evolui para garantir que o CLAUDE CODE sempre tenha informações precisas sobre o contexto técnico.\n\nPara referências a diretórios ou arquivos específicos, use caminhos relativos à raiz do projeto para facilitar a navegação do CLAUDE CODE.\n\nUse o comando \"think hard\" quando precisar de análises mais profundas sobre arquitetura ou refatoração, e \"ultrathink\" para problemas particularmente complexos ou decisões críticas.","metadata":{"source":".ondokai/onboarding.md","section":"","chunkIndex":8,"totalChunks":9,"timestamp":1749751088966}}]],{"0":"f6163dfe-27fc-4de4-9423-e1e1f1aef6ef","1":"2f8be819-32d0-449d-a6fc-f3e7bb6b6277","2":"e7eec84b-b180-4984-9e13-07270375d616","3":"99f49edc-dfaa-42c2-9e0e-1c11bcb8b4b5","4":"7cea2645-9f4d-4a1e-a0ab-3c9f5b170996","5":"ffe097f5-8d92-475d-9a10-89c22c7a80d4","6":"e01f80b3-6ea4-4553-9ac5-e644d0dfe86a","7":"d46a1cbe-e87b-47e3-9437-2bc5e53b1f87","8":"db263f02-57b7-4be4-8daa-6c20bbe6a13a","9":"b1272569-bfb1-4e6d-bc90-92fb2c4bcbe8","10":"bca7231e-5af3-4baf-bf51-5de083f6edf0","11":"2055625f-1256-4efd-bcca-02755156e83e","12":"e70b1ebf-a796-4a28-a92e-3beff6574c1e","13":"62907203-2a8c-4aef-b2de-6c47816173cd","14":"6d6ce899-daa0-4d1a-b038-4acfcabd79b5","15":"9441a1ef-6651-4c29-acd2-fa538753c5f6","16":"e5d30ecc-601d-4b93-9809-c30bd88522a2","17":"fc944953-5f6e-4cc4-bea9-9f679e5a3e04","18":"6e0db5f7-d8d8-49f5-9ac5-12d10f89ed7d","19":"fdea8ca4-2e30-4465-b669-6827e84ef8d2","20":"6648d688-bc9d-4614-9417-897fc9f2c58e","21":"d8fac1e3-7212-4f86-afab-db01f1d5e4f5","22":"6299ba7e-b4e8-45f7-86a7-e5e57baac349","23":"172f6ed5-e45a-4ffe-b93d-499e43ea71b2","24":"475bba3f-97d5-4b9c-8045-54c1285dc7f7","25":"dccd13b6-366e-46ae-9d96-b27436464c72","26":"b081e4e5-aa7d-439b-9030-1fa00e9817ea","27":"e8115602-7c95-4a30-ab19-e721d02756e7","28":"85687305-0288-4749-8ff3-57f810c74355","29":"a38decfb-a2e3-4029-a0da-26ec022c3cf3","30":"77c98b8f-2bf3-4e64-9805-e1b80825004d","31":"7a273937-8f16-420b-99c1-09fca46f100d","32":"b0b141fe-63f2-4c45-b333-ba76212f8de6","33":"40aa6cde-65e8-4ab8-bd02-c6c50a6bd3c7","34":"95d60022-ae4b-4f26-9dba-0e5af35a3f63","35":"b161f6df-b4e1-494e-9631-fe5233f6a077","36":"4c17d792-6fe0-4316-93dc-155240b01e72","37":"d74bab06-748f-49bf-8302-047779739538","38":"0ea02a9c-a93a-43d1-883a-e6bbc04948fd","39":"c57fb741-d599-474b-ac41-5a2099a4a963","40":"b14826fe-2bc5-431d-98fc-deac5e7ab360","41":"7a9a932c-10ac-41f4-9616-f76e3e8cca34","42":"647d64bc-9236-4eb5-ad23-f8c4963ecbe9","43":"ac4521e2-344e-4b83-822b-dd5c718e676d","44":"929cdb13-e4c4-40bb-a630-45d9deb47c02","45":"09378cba-e3b8-4c18-84b8-f3a2282a78fa","46":"a2a8d7a8-012d-40a2-8927-d3cbe1b04c3d","47":"1b679255-797c-4c51-af2f-312943808807","48":"db8dd6cf-a994-4de8-90c6-2a7f0dbe63b2","49":"364c8b6a-927e-4c97-8b31-dbbc076f4c43","50":"9aeb6817-a779-4706-969f-56da167acfa4","51":"db9c28f0-d3d9-45cb-8744-30bcb640807c","52":"a3a1c5a3-9531-4d5f-bd03-d9ad8ae21b67","53":"9f762773-b7e7-40a4-80b3-017bdcd306c5","54":"dd6f4dad-44a9-4e74-a3cc-d7b4159b9610","55":"0f739760-7f2a-4d09-a59f-dc0b378b0863","56":"31449d1d-7b33-4561-bd58-6cf03b1f1b16","57":"618fc63e-d607-4993-ad83-6543ea7139fa","58":"75577bbe-73a0-487c-a7dd-cb05884cf51c","59":"03bc23b0-1924-4659-8a21-a9fd6bacf3c1","60":"a96fb1a8-e75c-443c-9fd6-496d2223c90b","61":"41072492-b391-4cd6-9fbf-e21bb57bc6b2","62":"81d6419e-7e47-40fe-afc1-634c3810675f","63":"4b71ed08-9585-4a77-8160-e0d26bc36dab","64":"c8323c21-53e7-4fd3-aec9-90a70c867a02","65":"60923483-6718-4e99-ab22-3074969b255c","66":"352ae44f-bf2b-4635-b851-505a2cd9f74c","67":"ad9f1762-eec1-4a9c-9c51-cbe1a437f773","68":"7220c553-87b0-4564-ab43-5e02f38986fa","69":"d03996cb-2d1f-4767-ac4c-af252a97eba9","70":"18524e5a-0fbd-4648-9454-e8878d8cd9e9","71":"b5cd5118-baeb-461e-8865-88c13f54bb57","72":"429756af-ce7e-44a5-a268-336b32f49b7c","73":"cf5e84c2-fe02-4b83-89bf-5aeae8789292","74":"6dd0c2d0-ec3c-4178-8216-08f800c7862f","75":"aaccf6f1-e1e3-4976-a47c-8273cc7e073c","76":"77baae0f-aeb4-43f0-b378-762b65847762","77":"9e84913b-b087-41d1-8b13-f0962c960515","78":"bc0b1b5f-b7b2-4943-9fcc-c0ec569b91cb","79":"505e6d68-7fe0-467b-bd28-93c3b9acc9a8","80":"c545f59b-6ffc-4841-bfb9-8955dcf42d83","81":"251ab9ab-3527-44f0-a7be-ea7748e439cb","82":"82d36ffe-dd58-47a5-949b-f19c97d43b71","83":"ece906dd-6296-4f35-bcb7-9d5f150560f6","84":"d2ed30b7-5880-4fe4-a202-a4f877e0eee6","85":"1e3e6870-85a0-4078-9b66-988fbe623272","86":"32dc6479-c0e3-4474-909a-189b774a23df","87":"100a0a06-1404-4144-8cdd-893e48f67e86","88":"1cc12c1a-585e-4b7e-b975-2aba5dfc3f61","89":"c5cb191e-7c2d-4cc8-83b3-125e471000ad","90":"d2005a0a-a110-496e-ad9c-22b6c57c5c92","91":"56c33526-59ac-498c-a913-eb0cd09ff1e7","92":"2ffca510-9030-458a-aa5b-e35cd4f61b4e","93":"6c1ff4a6-8b9f-4683-9864-8a36df1ac8a9","94":"6337f583-d87c-48d9-9aa7-6c691ed7ab4a","95":"32f7431d-81e7-476f-953b-7f3480d36aec","96":"e548cb7f-deb8-462f-abe5-f81778c79728","97":"1e5a29d8-c4fb-4dd8-ac31-c5c889c5acd2","98":"ffd95a73-afdc-403a-a8f5-fb981ed39009","99":"0d38726f-687b-46ca-93a4-faef9adb79ec","100":"75b37b91-e7ee-46e3-a4ef-8fe329745174","101":"fb95c701-83e3-4455-971b-14c0b0fcff0c","102":"722ce725-dc76-458f-96cf-8707a35c853c","103":"8b39f3f9-cdc6-46ee-8aac-d82911968a7c","104":"7425f259-73ec-4ab8-bd5a-deab073d4c29","105":"2a2ecc01-a4fe-454d-a255-1927afaa5e70","106":"fe75bf23-d031-47a7-b68f-bed522519abb","107":"2e299e95-3c4a-4dd5-9443-8fba327b1422","108":"589773be-6c80-453c-925b-e7c125d5c582","109":"3def05ab-a9aa-4924-9765-5256b5cb31ee","110":"7b1aa003-2614-42c3-9888-7be63c8c0aa3","111":"1c5e5b81-20d3-4c93-a6c4-77c27f1e0550","112":"fb7c17c4-e1e1-4582-8a54-52a62fc794cf","113":"1895f762-389e-4b07-8475-ed1bb92da68c","114":"4c80953b-9b4e-4241-9b62-ff31368816e2","115":"cb5d70fa-6757-419e-89ae-13cca98ebf3c","116":"8bdfc8cd-cedf-44fd-a289-9a2fb2377fd7","117":"e4f1e7d4-50ce-42f8-b11a-b6e7b80935aa","118":"ed660ee6-6eef-43e5-b2cf-d075f9938010","119":"417e1f39-f3bf-42bd-b4ba-6008af83fdea","120":"0545e60a-4421-4f84-991c-229bc9e52746","121":"e653e2de-f267-4cb9-b439-983ac65ec692"}]